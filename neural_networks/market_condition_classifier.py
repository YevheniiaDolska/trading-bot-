import numpy as np
import pandas as pd
import tensorflow as tf
import os
from datetime import datetime, timedelta
from binance.client import Client
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.preprocessing import RobustScaler
from sklearn.utils.class_weight import compute_class_weight
import joblib
import logging
import pandas_ta as ta
import requests
from scipy.stats import zscore
from tensorflow.keras.metrics import Precision, Recall, AUC
from tensorflow.keras.regularizers import l2
import time
from binance.exceptions import BinanceAPIException
import zipfile
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor
import tensorflow.keras.backend as K
import xgboost as xgb
from tensorflow.keras.layers import Layer, GRU, Input, Concatenate
from sklearn.ensemble import VotingClassifier
import matplotlib.pyplot as plt
from tensorflow.keras.backend import clear_session
import glob
import requests
import zipfile
from io import BytesIO
from threading import Lock
from ta.trend import SMAIndicator
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Input, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split, KFold
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import tensorflow.keras.backend as K
from sklearn.model_selection import StratifiedKFold
from sklearn.dummy import DummyClassifier
from scipy.stats import zscore
from scipy.stats import mode



# ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
def initialize_strategy():
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            logging.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU!")
            return tf.distribute.MirroredStrategy()
        except RuntimeError as e:
            logging.warning(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU: {e}")
            return tf.distribute.get_strategy()
    else:
        logging.info("‚ùå GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
        return tf.distribute.get_strategy()

    

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def cleanup_training_files():
    """
    –£–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª—ã —Å –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
    """
    files_to_delete = glob.glob("binance_data*.csv")  # –ò—â–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö
    for file_path in files_to_delete:
        try:
            os.remove(file_path)
            logging.info(f"üóë –£–¥–∞–ª—ë–Ω —Ñ–∞–π–ª: {file_path}")
        except Exception as e:
            logging.error(f"‚ö† –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {file_path}: {e}")
            
            

class Attention(Layer):
    """Attention-–º–µ—Ö–∞–Ω–∏–∑–º –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫"""
    def __init__(self):
        super(Attention, self).__init__()

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1), initializer="normal")
        self.b = self.add_weight(name="att_bias", shape=(1,), initializer="zeros")
        super(Attention, self).build(input_shape)

    def call(self, x):
        e = K.tanh(K.dot(x, self.W) + self.b)
        a = K.softmax(e, axis=1)
        return x * a
    
class MarketClassifier:
    def __init__(self, model_path="market_condition_classifier.h5", scaler_path="scaler.pkl"):
        self.model_path = model_path
        self.scaler_path = scaler_path
        self.base_url = "https://data.binance.vision/data/spot/monthly/klines/{symbol}/1m/"
        
        
    def apply_in_chunks(df, func, chunk_size=100000):
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é func –∫ DataFrame –ø–æ —á–∞–Ω–∫–∞–º –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.
        –ï—Å–ª–∏ df –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DataFrame, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç func(df).
        """
        if not isinstance(df, pd.DataFrame):
            return func(df)
        if len(df) <= chunk_size:
            return func(df)
        # –†–∞–∑–±–∏–≤–∞–µ–º DataFrame –Ω–∞ —á–∞–Ω–∫–∏
        chunks = [df.iloc[i:i+chunk_size] for i in range(0, len(df), chunk_size)]
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É
        processed_chunks = [func(chunk) for chunk in chunks]
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –≤ –æ–¥–∏–Ω DataFrame
        return pd.concat(processed_chunks)
    

    def fetch_binance_data(self, symbol, interval, start_date, end_date):
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å Binance –±–µ–∑ API-–∫–ª—é—á–∞ (–∞—Ä—Ö–∏–≤ Binance) –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: timestamp, open, high, low, close, volume.
        """
        base_url_monthly = "https://data.binance.vision/data/spot/monthly/klines"
        logging.info(f"üì° –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å Binance –¥–ª—è {symbol} ({interval}) c {start_date} –ø–æ {end_date}...")

        start_date = pd.to_datetime(start_date)
        end_date = pd.to_datetime(end_date)
        all_data = []
        downloaded_files = set()
        download_lock = Lock()  # –ì–ª–æ–±–∞–ª—å–Ω–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è

        def download_and_process(date):
            year, month = date.year, date.month
            month_str = f"{month:02d}"
            file_name = f"{symbol}-{interval}-{year}-{month_str}.zip"
            file_url = f"{base_url_monthly}/{symbol}/{interval}/{file_name}"

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            with download_lock:
                if file_name in downloaded_files:
                    logging.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫ {file_name}, —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")
                    return None

                logging.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è {file_url}...")
                response = requests.head(file_url, timeout=5)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_url}")
                    return None

                logging.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ {file_url}...")
                response = requests.get(file_url, timeout=15)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_url}: –ö–æ–¥ {response.status_code}")
                    return None

                logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω {file_name}")
                downloaded_files.add(file_name)

            try:
                with zipfile.ZipFile(BytesIO(response.content)) as zip_file:
                    csv_file = file_name.replace('.zip', '.csv')
                    with zip_file.open(csv_file) as file:
                        df = pd.read_csv(
                            file, header=None, 
                            names=[
                                "timestamp", "open", "high", "low", "close", "volume",
                                "close_time", "quote_asset_volume", "number_of_trades",
                                "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", "ignore"
                            ],
                            dtype={
                                "timestamp": "int64",
                                "open": "float32",
                                "high": "float32",
                                "low": "float32",
                                "close": "float32",
                                "volume": "float32",
                                "quote_asset_volume": "float32",
                                "number_of_trades": "int32",
                                "taker_buy_base_asset_volume": "float32",
                                "taker_buy_quote_asset_volume": "float32"
                            },
                            low_memory=False
                        )
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime
                        df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
                        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]
                        # –ü—Ä–∏–≤–æ–¥–∏–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∫ —Ç–∏–ø—É float, –Ω–µ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞—è timestamp
                        df[['open', 'high', 'low', 'close', 'volume']] = df[['open', 'high', 'low', 'close', 'volume']].astype(float)
                        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º timestamp –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
                        df.set_index("timestamp", inplace=True)
                        return df
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {symbol} –∑–∞ {date.strftime('%Y-%m')}: {e}")
                return None

        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–º —Ä–µ–∂–∏–º–µ
        with ThreadPoolExecutor(max_workers=8) as executor:
            results = list(executor.map(download_and_process, pd.date_range(start=start_date, end=end_date, freq='MS')))

        # –°–æ–±–∏—Ä–∞–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        all_data = [df for df in results if df is not None]

        if not all_data:
            raise ValueError(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –º–µ—Å—è—Ü–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}.")

        df = pd.concat(all_data)
        logging.info(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {df.shape}")

        # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –∫–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–∞–∫ —Å—Ç–æ–ª–±–µ—Ü, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å
        if "timestamp" not in df.columns:
            df.reset_index(inplace=True)

        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ timestamp –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–∏–ø
        df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
        df.set_index("timestamp", inplace=True)

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–æ 1-–º–∏–Ω—É—Ç–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        df = df.resample('1min').ffill()

        # –°–±—Ä–æ—Å –∏–Ω–¥–µ–∫—Å–∞, —á—Ç–æ–±—ã timestamp —Å—Ç–∞–ª –æ–±—ã—á–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π
        df.reset_index(inplace=True)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        num_nans = df.isna().sum().sum()
        if num_nans > 0:
            if num_nans / len(df) > 0.05:  # –ï—Å–ª–∏ –±–æ–ª–µ–µ 5% –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–ø—É—â–µ–Ω—ã
                logging.warning("‚ö† –ü—Ä–æ–ø—É—â–µ–Ω–æ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö! –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–∏ —Å–≤–µ—á–∏.")
                df.dropna(inplace=True)
            else:
                df.fillna(method='ffill', inplace=True)

        logging.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(df)} –∑–∞–ø–∏—Å–µ–π")
        return df

        

    def add_indicators(self, data):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pandas_ta"""
        # –ë–∞–∑–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        data['atr'] = ta.atr(data['high'], data['low'], data['close'], length=7)
        data['adx'] = ta.adx(data['high'], data['low'], data['close'], length=7)['ADX_7']
        
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ MA –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞
        for period in [5, 10, 15, 20, 50]:
            data[f'sma_{period}'] = ta.sma(data['close'], length=period)
            data[f'ema_{period}'] = ta.ema(data['close'], length=period)
        
        # –ò–º–ø—É–ª—å—Å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        data['rsi'] = ta.rsi(data['close'], length=7)
        macd = ta.macd(data['close'], fast=12, slow=26, signal=9)
        data['macd'] = macd['MACD_12_26_9']
        data['macd_signal'] = macd['MACDs_12_26_9']
        data['macd_hist'] = macd['MACDh_12_26_9']
        data['willr'] = ta.willr(data['high'], data['low'], data['close'], length=14)
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: –≥—Ä—É–ø–ø–∏—Ä—É–µ–º rolling –¥–ª—è close —Å –æ–∫–Ω–æ–º 10 –¥–ª—è Bollinger Bands
        bb = ta.bbands(data['close'], length=10, std=2)
        data['bb_upper']  = bb[f'BBU_10_2.0']
        data['bb_middle'] = bb[f'BBM_10_2.0']
        data['bb_lower']  = bb[f'BBL_10_2.0']
        data['bb_width']  = (data['bb_upper'] - data['bb_lower']) / data['bb_middle']

        
        # –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: –≥—Ä—É–ø–ø–∏—Ä—É–µ–º rolling –¥–ª—è volume —Å –æ–∫–Ω–æ–º 20
        vs = data['volume'].rolling(window=20)
        data['volume_sma'] = vs.mean()
        data['volume_std'] = vs.std()
        data['volume_ratio'] = data['volume'] / data['volume_sma']
        
        # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        data['trend_strength'] = abs(data['close'].pct_change().rolling(window=20).sum())
        data['price_momentum'] = data['close'].diff(periods=10) / data['close'].shift(10)
        
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —É—Ä–æ–≤–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏/—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
        data['support_level'] = data['low'].rolling(window=20).min()
        data['resistance_level'] = data['high'].rolling(window=20).max()
        
        return data


    
    def compute_scaler(self, data_path, sample_size=100000, chunk_size=10000):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –ø–æ CSV-—Ñ–∞–π–ª—É, —á–∏—Ç–∞—è –¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞–º–∏, 
        –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã RobustScaler –Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å–Ω–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–π –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫ –∏ —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
        """
        samples = []
        total_samples = 0
        features = None
        for chunk in pd.read_csv(data_path, chunksize=chunk_size):
            chunk.dropna(inplace=True)
            # –ï—Å–ª–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –µ—â—ë –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã, –≤—ã–∑—ã–≤–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ñ—É–Ω–∫—Ü–∏—é
            if 'atr' not in chunk.columns:
                chunk = self.add_indicators(chunk)
            chunk = self.remove_outliers(chunk)
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ —Ä—ã–Ω–∫–∞
            label_mapping = {'bullish': 0, 'bearish': 1, 'flat': 2}
            chunk['market_type'] = chunk['market_type'].map(label_mapping)
            chunk.dropna(subset=['market_type'], inplace=True)
            if features is None:
                features = [col for col in chunk.columns if col not in ['market_type', 'symbol', 'timestamp']]
            X_chunk = chunk[features].values
            samples.append(X_chunk)
            total_samples += X_chunk.shape[0]
            if total_samples >= sample_size:
                break
        X_sample = np.concatenate(samples, axis=0)
        scaler = RobustScaler().fit(X_sample)
        logging.info(f"–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫ –æ–±—É—á–µ–Ω –Ω–∞ {X_sample.shape[0]} –≤—ã–±–æ—Ä–∫–∞—Ö.")
        return scaler, features

    def data_generator_split(self, data_path, scaler, features, batch_size, chunk_size=10000, 
                             split='train', train_fraction=0.8, random_seed=42):
        """
        –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —á–∏—Ç–∞–µ—Ç CSV-—Ñ–∞–π–ª —á–∞–Ω–∫–∞–º–∏, –≤—ã–ø–æ–ª–Ω—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ –¥–µ–ª–∏—Ç –∏—Ö 
        –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ train_fraction.
        """
        chunk_counter = 0
        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥—á–∞–Ω–∫–∞
        def process_subchunk(sub_df):
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏ —É–¥–∞–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã –¥–ª—è –ø–æ–¥—á–∞–Ω–∫–∞
            sub_df = self.add_indicators(sub_df)
            sub_df = self.remove_outliers(sub_df)
            return sub_df

        for chunk in pd.read_csv(data_path, chunksize=chunk_size):
            chunk.dropna(inplace=True)
            # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ —á–∞–Ω–∫–µ –≤—Å–µ –µ—â—ë –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–µ, –¥–µ–ª–∏–º –∏—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ
            # –ó–¥–µ—Å—å –≤—ã–∑—ã–≤–∞–µ–º apply_in_chunks –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            chunk = apply_in_chunks(chunk, process_subchunk, chunk_size=5000)
            
            label_mapping = {'bullish': 0, 'bearish': 1, 'flat': 2}
            chunk['market_type'] = chunk['market_type'].map(label_mapping)
            chunk.dropna(subset=['market_type'], inplace=True)
            
            X_chunk = chunk[features].values
            y_chunk = chunk['market_type'].values.astype(int)
            n_samples = X_chunk.shape[0]
            
            r = np.random.RandomState(random_seed + chunk_counter)
            random_vals = r.rand(n_samples)
            mask = random_vals < train_fraction if split == 'train' else random_vals >= train_fraction
            X_selected = X_chunk[mask]
            y_selected = y_chunk[mask]
            if X_selected.shape[0] == 0:
                chunk_counter += 1
                continue
            
            X_scaled = scaler.transform(X_selected)
            indices = np.arange(X_scaled.shape[0])
            r_shuffle = np.random.RandomState(random_seed + chunk_counter + 1000)
            r_shuffle.shuffle(indices)
            X_scaled = X_scaled[indices]
            y_selected = y_selected[indices]
            
            n_sel = X_scaled.shape[0]
            for i in range(0, n_sel, batch_size):
                batch_idx = slice(i, i + batch_size)
                X_batch = X_scaled[batch_idx]
                y_batch = y_selected[batch_idx]
                X_batch = np.expand_dims(X_batch, axis=1)
                yield X_batch.astype(np.float32), y_batch.astype(np.int32)
            
            chunk_counter += 1


    def prepare_training_dataset(self, data_path, scaler, features, batch_size, chunk_size=10000, 
                                 split='train', train_fraction=0.8, random_seed=42):
        """
        –°–æ–∑–¥–∞—ë—Ç tf.data.Dataset –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ data_generator_split.
        """
        gen = lambda: self.data_generator_split(
            data_path, scaler, features, batch_size, chunk_size, split, train_fraction, random_seed
        )
        # –ü–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω –±–∞—Ç—á –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–æ—Ä–º—ã –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        sample_gen = self.data_generator_split(
            data_path, scaler, features, batch_size, chunk_size, split, train_fraction, random_seed
        )
        sample = next(sample_gen)
        output_shapes = (tf.TensorShape([None, 1, sample[0].shape[-1]]), tf.TensorShape([None]))
        output_types = (tf.float32, tf.int32)
        dataset = tf.data.Dataset.from_generator(gen, output_types=output_types, output_shapes=output_shapes)
        dataset = dataset.shuffle(buffer_size=1000)
        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
        return dataset
    
    def validate_predictions(self, data, prediction, window=5):
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Å –ø–æ–º–æ—â—å—é –º—É–ª—å—Ç–∏–ø–µ—Ä–∏–æ–¥–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–≤–µ—á–µ–π –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —Å–∏–≥–Ω–∞–ª–∞
        recent_data = data.tail(window)
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç—Ä–µ–Ω–¥–∞
        price_direction = recent_data['close'].diff().apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)
        trend_consistency = abs(price_direction.sum()) / window
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ä–µ–º–Ω–æ–≥–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
        volume_trend = recent_data['volume'] > recent_data['volume_sma']
        volume_confirmation = volume_trend.mean()
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø—É–ª—å—Å–∞
        momentum_confirmation = (
            (recent_data['rsi'] > 60).all() or  # –°–∏–ª—å–Ω—ã–π –±—ã—á–∏–π –∏–º–ø—É–ª—å—Å
            (recent_data['rsi'] < 40).all() or  # –°–∏–ª—å–Ω—ã–π –º–µ–¥–≤–µ–∂–∏–π –∏–º–ø—É–ª—å—Å
            (recent_data['rsi'].between(45, 55)).all()  # –°—Ç–∞–±–∏–ª—å–Ω—ã–π —Ñ–ª—ç—Ç
        )
        
        # 4. –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã
        mtf_confirmation = (
            recent_data['adx'].mean() > 25 and  # –°–∏–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥
            abs(recent_data['macd_hist']).mean() > recent_data['macd_hist'].std()  # –°–∏–ª—å–Ω—ã–π MACD
        )
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ —Å–∫–æ—Ä–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
        confidence_score = (
            0.3 * trend_consistency +
            0.3 * volume_confirmation +
            0.2 * momentum_confirmation +
            0.2 * mtf_confirmation
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π
        if prediction == 'bullish':
            prediction_valid = (
                trend_consistency > 0.6 and
                volume_confirmation > 0.5 and
                recent_data['rsi'].mean() > 55
            )
        elif prediction == 'bearish':
            prediction_valid = (
                trend_consistency > 0.6 and
                volume_confirmation > 0.5 and
                recent_data['rsi'].mean() < 45
            )
        else:  # flat
            prediction_valid = (
                trend_consistency < 0.4 and
                0.3 < volume_confirmation < 0.7 and
                40 < recent_data['rsi'].mean() < 60
            )
        
        return {
            'prediction': prediction,
            'is_valid': prediction_valid,
            'confidence': confidence_score,
            'confirmations': {
                'trend_consistency': trend_consistency,
                'volume_confirmation': volume_confirmation,
                'momentum_confirmation': momentum_confirmation,
                'mtf_confirmation': mtf_confirmation
            }
        }
    
    
    def classify_market_conditions(self, data, window=20):
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä—ã–Ω–∫–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è–º–∏
        """
        if len(data) < window:
            logging.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {len(data)} < {window}")
            return 'flat'  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º flat –≤–º–µ—Å—Ç–æ uncertain –∫–∞–∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            
        if not hasattr(self, 'previous_market_type'):
            self.previous_market_type = 'flat'  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–µ–º flat

        # –ë–∞–∑–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        adx = data['adx'].iloc[-1]
        rsi = data['rsi'].iloc[-1]
        macd_hist = data['macd_hist'].iloc[-1]
        willr = data['willr'].iloc[-1]
        volume_ratio = data['volume_ratio'].iloc[-1]
        price = data['close'].iloc[-1]
        support = data['support_level'].iloc[-1]
        resistance = data['resistance_level'].iloc[-1]
        
        # –†–∞—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ —É—Ä–æ–≤–Ω–µ–π –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
        distance_to_support = ((price - support) / price) * 100
        distance_to_resistance = ((resistance - price) / price) * 100
        
        logging.info(f"""
        –¢–µ–∫—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤:
        ADX: {adx:.2f}
        RSI: {rsi:.2f}
        MACD Histogram: {macd_hist:.2f}
        Williams %R: {willr:.2f}
        Volume Ratio: {volume_ratio:.2f}
        –¶–µ–Ω–∞: {price:.2f}
        –ü–æ–¥–¥–µ—Ä–∂–∫–∞: {support:.2f} (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance_to_support:.2f}%)
        –°–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ: {resistance:.2f} (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance_to_resistance:.2f}%)
        """)

        # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞ —á–µ—Ä–µ–∑ MA —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        ma_trends = []
        for period in [10, 20, 50]:
            is_above = price > data[f'sma_{period}'].iloc[-1]
            ma_trends.append(is_above)
            logging.info(f"–¶–µ–Ω–∞ –≤—ã—à–µ SMA{period}: {is_above}")
        trend_confirmation = sum(ma_trends)
        logging.info(f"–û–±—â–µ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞: {trend_confirmation}/3")

        # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–≤–µ—á–∞—Ö
        recent_data = data.tail(window)
        price_direction = recent_data['close'].diff().apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)
        trend_consistency = abs(price_direction.sum()) / window
        volume_confirmation = (recent_data['volume'] > recent_data['volume_sma']).mean()
        momentum_strength = abs(recent_data['rsi'] - 50).mean() / 50

        logging.info(f"""
        –ú–µ—Ç—Ä–∏–∫–∏ —Ç—Ä–µ–Ω–¥–∞:
        –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–¥–∞: {trend_consistency:.2f}
        –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ–±—ä—ë–º–æ–º: {volume_confirmation:.2f}
        –°–∏–ª–∞ –∏–º–ø—É–ª—å—Å–∞: {momentum_strength:.2f}
        """)

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ä—ã–Ω–∫–∞ —Å –±–æ–ª–µ–µ –º—è–≥–∫–∏–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏
        market_type = 'uncertain'
        confidence_score = 0

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±—ã—á—å–µ–≥–æ —Ä—ã–Ω–∫–∞
        if (adx > 20 and rsi > 45 and volume_ratio > 1.0 and
            trend_confirmation >= 1 and macd_hist > 0 and
            distance_to_resistance > 0.5):  # –ï—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è —Ä–æ—Å—Ç–∞
            
            bullish_confidence = (
                0.25 * (trend_consistency if price_direction.sum() > 0 else 0) +
                0.25 * volume_confirmation +
                0.25 * (momentum_strength if rsi > 45 else 0) +
                0.25 * (distance_to_resistance / 5)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è
            )
            
            logging.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –±—ã—á—å–µ–≥–æ —Ä—ã–Ω–∫–∞. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {bullish_confidence:.2f}")
            
            if bullish_confidence > 0.5:
                market_type = 'bullish'
                confidence_score = bullish_confidence

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞
        elif (adx > 20 and rsi < 55 and volume_ratio > 1.0 and
              trend_confirmation <= 2 and macd_hist < 0 and
              distance_to_support > 0.5):  # –ï—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–ª—è –ø–∞–¥–µ–Ω–∏—è
            
            bearish_confidence = (
                0.25 * (trend_consistency if price_direction.sum() < 0 else 0) +
                0.25 * volume_confirmation +
                0.25 * (momentum_strength if rsi < 55 else 0) +
                0.25 * (distance_to_support / 5)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –ø–æ–¥–¥–µ—Ä–∂–∫–∏
            )
            
            logging.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {bearish_confidence:.2f}")
            
            if bearish_confidence > 0.5:
                market_type = 'bearish'
                confidence_score = bearish_confidence

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–ª—ç—Ç–∞
        elif (adx < 25 and 35 < rsi < 65 and
              abs(macd_hist) < 0.2 * data['macd_hist'].std() and
              0.7 < volume_ratio < 1.3 and
              support < price < resistance and
              max(distance_to_support, distance_to_resistance) < 2):  # –¶–µ–Ω–∞ –º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏
            
            flat_confidence = (
                0.25 * (1 - trend_consistency) +
                0.25 * (1 - abs(volume_ratio - 1)) +
                0.25 * (1 - momentum_strength) +
                0.25 * (1 - max(distance_to_support, distance_to_resistance) / 2)
            )
            
            logging.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–ª—ç—Ç–∞. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {flat_confidence:.2f}")
            
            if flat_confidence > 0.5:
                market_type = 'flat'
                confidence_score = flat_confidence

        # –ï—Å–ª–∏ —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
        if market_type == 'uncertain':
            # –£—á–∏—Ç—ã–≤–∞–µ–º —É—Ä–æ–≤–Ω–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è –≤ —É–ø—Ä–æ—â—ë–Ω–Ω–æ–π –ª–æ–≥–∏–∫–µ
            if rsi > 60 and distance_to_resistance > 0.5:
                market_type = 'bullish'
            elif rsi < 40 and distance_to_support > 0.5:
                market_type = 'bearish'
            else:
                market_type = 'flat'
            
            logging.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é: {market_type} (RSI: {rsi:.2f}, " +
                        f"–†–∞—Å—Å—Ç. –¥–æ —Å–æ–ø—Ä.: {distance_to_resistance:.2f}%, " +
                        f"–†–∞—Å—Å—Ç. –¥–æ –ø–æ–¥–¥.: {distance_to_support:.2f}%)")
        
        self.previous_market_type = market_type
        return market_type
    

    def remove_outliers(self, data, z_threshold=3):
        """
        –£–¥–∞–ª—è–µ—Ç –≤—ã–±—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–æ–¥–∞ Z-score.
        """
        z_scores = zscore(data[['open', 'high', 'low', 'close', 'volume']])
        mask = (np.abs(z_scores) < z_threshold).all(axis=1)
        filtered_data = data[mask]
        removed_count = len(data) - len(filtered_data)
        logging.info(f"–£–¥–∞–ª–µ–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤: {removed_count}")
        return filtered_data

    def fetch_market_events(self, api_key, start_date, end_date):
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è —á–µ—Ä–µ–∑ API.
        """
        try:
            url = "https://newsapi.org/v2/everything"
            params = {
                "q": "market crash OR economic crisis OR volatility",
                "from": start_date.strftime("%Y-%m-%d"),
                "to": end_date.strftime("%Y-%m-%d"),
                "sortBy": "relevancy",
                "language": "en",
                "apiKey": api_key,
            }
            response = requests.get(url, params=params)
            if response.status_code == 200:
                articles = response.json().get("articles", [])
                events = []
                for article in articles:
                    published_date = datetime.strptime(article['publishedAt'], "%Y-%m-%dT%H:%M:%SZ")
                    events.append({
                        "start": published_date - timedelta(days=1),
                        "end": published_date + timedelta(days=1),
                        "event": article['title']
                    })
                return events
            else:
                logging.error(f"–û—à–∏–±–∫–∞ API: {response.status_code} {response.text}")
                return []
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏–π: {e}")
            return []

    def flag_market_events(self, data, events):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ñ–ª–∞–≥–∏ –¥–ª—è —Ñ–æ—Ä—Å-–º–∞–∂–æ—Ä–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π.
        """
        data['market_event'] = 'None'
        for event in events:
            mask = (data.index >= event['start']) & (data.index <= event['end'])
            data.loc[mask, 'market_event'] = event['event']
        logging.info(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–ª–∞–≥–∏ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π –¥–æ–±–∞–≤–ª–µ–Ω—ã.")
        return data

    def fetch_and_label_all(self, symbols, start_date, end_date, save_path="labeled_data"):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ —Ä–∞–∑–º–µ—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API.
        """
        os.makedirs(save_path, exist_ok=True)
        all_data = []

        for symbol in symbols:
            try:
                logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}")
                df = self.fetch_binance_data(symbol, "1m", start_date, end_date)  # ‚úÖ –£–±—Ä–∞–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API
                df = self.add_indicators(df)
                df['market_type'] = self.classify_market_conditions(df)
                df['symbol'] = symbol
                file_path = os.path.join(save_path, f"{symbol}_data.csv")
                df.to_csv(file_path)
                logging.info(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {file_path}")
                all_data.append(df)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {symbol}: {e}")

        if not all_data:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞.")
        
        return pd.concat(all_data, ignore_index=True)


    
    def prepare_training_data(self, data_path):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —á–∞–Ω–∫–æ–≤.
        –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é: –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤,
        —É–¥–∞–ª–µ–Ω–∏–µ NaN, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ (—á–µ—Ä–µ–∑ self.add_indicators), —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤,
        –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ —Ä—ã–Ω–∫–∞ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç.
        –î–∞–Ω–Ω—ã–µ —á–∏—Ç–∞—é—Ç—Å—è –ø–æ —á–∞—Å—Ç—è–º (—á–∞–Ω–∫–∞–º–∏) –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.
        """
        logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ {data_path}")
        chunks = []
        try:
            # –ß–∏—Ç–∞–µ–º CSV —á–∞–Ω–∫–∞–º–∏ –ø–æ 10_000 —Å—Ç—Ä–æ–∫
            for chunk in pd.read_csv(data_path, index_col=0, chunksize=10000):
                # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                chunk.dropna(inplace=True)

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
                required_columns = [
                    'open', 'high', 'low', 'close', 'volume',
                    'atr', 'adx', 'rsi', 'macd', 'macd_signal', 'macd_hist',
                    'willr', 'bb_width', 'volume_ratio', 'trend_strength',
                    'market_type'
                ]
                for col in required_columns:
                    if col not in chunk.columns:
                        raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π —Å—Ç–æ–ª–±–µ—Ü: {col}")

                # –õ–æ–≥–≥–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è market_type –¥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
                logging.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è market_type –≤ —á–∞–Ω–∫–µ –¥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è: {chunk['market_type'].unique()}")

                # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ (–µ–¥–∏–Ω–æ–∂–¥—ã –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –¥–ª—è —á–∞–Ω–∫–∞)
                logging.info("–£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –∏–∑ —á–∞–Ω–∫–∞...")
                chunk = self.remove_outliers(chunk)
                logging.info(f"–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤: {chunk.shape}")

                # –ï—Å–ª–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –µ—â—ë –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã ‚Äì –¥–æ–±–∞–≤–ª—è–µ–º (—Ñ—É–Ω–∫—Ü–∏—è add_indicators –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –≤ –∫–ª–∞—Å—Å–µ)
                if 'atr' not in chunk.columns:
                    chunk = self.add_indicators(chunk)

                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ —Ä—ã–Ω–∫–∞ –≤ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                label_mapping = {'bullish': 0, 'bearish': 1, 'flat': 2}
                chunk['market_type'] = chunk['market_type'].map(label_mapping)

                # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å NaN –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è market_type
                if chunk['market_type'].isna().any():
                    bad_values = chunk[chunk['market_type'].isna()]['market_type'].unique()
                    logging.error(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ market_type! –ò—Å—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {bad_values}")
                    chunk.dropna(subset=['market_type'], inplace=True)

                chunks.append(chunk)
        except FileNotFoundError:
            logging.error(f"–§–∞–π–ª {data_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            raise

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –≤ –æ–¥–∏–Ω DataFrame
        data = pd.concat(chunks, ignore_index=True)
        features = [col for col in data.columns if col not in ['market_type', 'symbol', 'timestamp']]
        X = data[features].values
        y = data['market_type'].values.astype(int)

        logging.info(f"–§–æ—Ä–º–∞ X: {X.shape}")
        logging.info(f"–§–æ—Ä–º–∞ y: {y.shape}")
        return X, y



    def balance_classes(self, y):
        # –ü—Ä–∏–≤–æ–¥–∏–º y –∫ numpy-–º–∞—Å—Å–∏–≤—É, –µ—Å–ª–∏ —ç—Ç–æ –µ—â—ë –Ω–µ —Å–¥–µ–ª–∞–Ω–æ
        y = np.array(y)
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤ y
        present_classes = np.unique(y)
        computed_weights = compute_class_weight(
            class_weight='balanced',
            classes=present_classes,
            y=y
        )
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ç–µ—Ö –∫–ª–∞—Å—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å
        class_weights = {int(cls): weight for cls, weight in zip(present_classes, computed_weights)}
        
        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ —Ç—Ä–∏ –∫–ª–∞—Å—Å–∞: 0, 1 –∏ 2
        for cls in [0, 1, 2]:
            if cls not in class_weights:
                # –ï—Å–ª–∏ –∫–ª–∞—Å—Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ y, —Ç–æ –µ–≥–æ –≤–µ—Å –≤–∑—è—Ç—å —Ä–∞–≤–Ω—ã–º 1.0
                # (–≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –Ω–∞ –ø–æ–ª–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö —ç—Ç–æ—Ç —Å–ª—É—á–∞–π –≤–æ–∑–Ω–∏–∫–∞—Ç—å –Ω–µ –¥–æ–ª–∂–µ–Ω)
                class_weights[cls] = 1.0
        return class_weights





    def build_lstm_gru_model(self, input_shape):
        inputs = Input(shape=input_shape)

        # LSTM –±–ª–æ–∫ (—É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ—ë–≤)
        lstm_out = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)
        lstm_out = Dropout(0.3)(lstm_out)
        lstm_out = LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))(lstm_out)

        # GRU –±–ª–æ–∫ (—É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ—ë–≤)
        gru_out = GRU(128, return_sequences=True, kernel_regularizer=l2(0.01))(inputs)
        gru_out = Dropout(0.3)(gru_out)
        gru_out = GRU(64, return_sequences=True, kernel_regularizer=l2(0.01))(gru_out)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—ã—Ö–æ–¥—ã LSTM –∏ GRU
        combined = Concatenate()([lstm_out, gru_out])

        # Attention-–º–µ—Ö–∞–Ω–∏–∑–º
        attention = Attention()(combined)

        x = LSTM(64, return_sequences=False)(attention)
        x = Dropout(0.3)(x)
        x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(attention)
        x = Dropout(0.3)(x)
        x = Dense(32, activation='relu', name="embedding_layer", kernel_regularizer=l2(0.01))(x)
        outputs = Dense(3, activation='softmax')(x)

        model = tf.keras.models.Model(inputs, outputs)
        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        return model




    def train_xgboost(self, X_train, y_train, X_val, y_val):
        """–û–±—É—á–∞–µ—Ç XGBoost –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö LSTM + GRU, –∏–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DummyClassifier, –µ—Å–ª–∏ –≤ y_train —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å."""
        unique_classes = np.unique(y_train)
        if len(unique_classes) < 2:
            logging.warning("–í –æ–±—É—á–∞—é—â–µ–º –Ω–∞–±–æ—Ä–µ XGB –æ–±–Ω–∞—Ä—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å. –ò—Å–ø–æ–ª—å–∑—É–µ–º DummyClassifier.")
            dummy = DummyClassifier(strategy='constant', constant=unique_classes[0])
            dummy.fit(X_train, y_train)
            return dummy
        else:
            booster = xgb.XGBClassifier(
                objective='multi:softprob',
                num_class=3,
                learning_rate=0.1,
                n_estimators=10,
                max_depth=3,
                subsample=0.8,
                colsample_bytree=0.8,
                verbosity=1,
                use_label_encoder=False
            )
            # –ó–¥–µ—Å—å –≤–º–µ—Å—Ç–æ X_val, y_val –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, X_test_features –∏ y_test.
            booster.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_val, y_val)], verbose=False)
            return booster


    def build_ensemble(X_train, y_train):
        """–§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å: LSTM + GRU + XGBoost"""
        lstm_gru_model = build_lstm_gru_model((X_train.shape[1], X_train.shape[2]))

        # –û–±—É—á–∞–µ–º LSTM-GRU
        lstm_gru_model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)#epochs=100

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è –ø–µ—Ä–µ–¥ softmax
        feature_extractor = tf.keras.models.Model(
            inputs=lstm_gru_model.input, outputs=lstm_gru_model.layers[-3].output
        )
        X_features = feature_extractor.predict(X_train)

        # –û–±—É—á–∞–µ–º XGBoost –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö
        xgb_model = self.train_xgboost(X_features, y_train)

        # –ê–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ VotingClassifier
        ensemble = VotingClassifier(
            estimators=[
                ('lstm_gru', lstm_gru_model),
                ('xgb', xgb_model)
            ],
            voting='soft'
        )
        return ensemble


    def train_market_condition_classifier(self, data_path, model_path='market_condition_classifier.h5',
                                            scaler_path='scaler.pkl', checkpoint_path='market_condition_checkpoint.h5',
                                            epochs=100, steps_per_epoch=100, validation_steps=20):
        """
        –û–±—É—á–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–Ω—Å–∞–º–±–ª–µ–≤—É—é –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—ã–Ω–æ—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π,
        –∏—Å–ø–æ–ª—å–∑—É—è LSTM + GRU + Attention + XGBoost. –î–∞–Ω–Ω—ã–µ —Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è —á–∞–Ω–∫–∞–º–∏ —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã
        (tf.data.Dataset), —á—Ç–æ–±—ã –Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç –≤ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å.
        """
        # –í—ã—á–∏—Å–ª—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫ –Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å–Ω–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ (–±–µ–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤)
        scaler, features = self.compute_scaler(data_path, sample_size=100000, chunk_size=10000)
        joblib.dump(scaler, scaler_path)
        logging.info(f"–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {scaler_path}.")

        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π dataset
        train_dataset = self.prepare_training_dataset(data_path, scaler, features, batch_size=32,
                                                        chunk_size=10000, split='train', train_fraction=0.8)
        val_dataset = self.prepare_training_dataset(data_path, scaler, features, batch_size=32,
                                                      chunk_size=10000, split='val', train_fraction=0.8)

        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –≤—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–æ–π (1, —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
        input_shape = (1, len(features))
        with strategy.scope():
            final_model = self.build_lstm_gru_model(input_shape=input_shape)
            final_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

        callbacks = [
            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
            ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss', verbose=1),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5)
        ]

        history = final_model.fit(
            train_dataset,
            epochs=epochs,
            steps_per_epoch=steps_per_epoch,
            validation_data=val_dataset,
            validation_steps=validation_steps,
            callbacks=callbacks
        )

        # –î–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–∏: —Å–æ–±–∏—Ä–∞–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ dataset
        X_test_list, y_test_list = [], []
        for X_batch, y_batch in val_dataset.take(validation_steps):
            X_test_list.append(X_batch.numpy())
            y_test_list.append(y_batch.numpy())
        X_test = np.concatenate(X_test_list, axis=0)
        y_test = np.concatenate(y_test_list, axis=0)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è (–ø–µ—Ä–µ–¥ softmax)
        feature_extractor = tf.keras.models.Model(
            inputs=final_model.input,
            outputs=final_model.get_layer("embedding_layer").output
        )
        X_test_features = feature_extractor.predict(X_test)
        X_test_features = np.squeeze(X_test_features, axis=1)  # –µ—Å–ª–∏ –Ω—É–∂–Ω–æ —É–±—Ä–∞—Ç—å –ª–∏—à–Ω—é—é –æ—Å—å


        # –û–±—É—á–∞–µ–º XGBoost –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö; –ø–µ—Ä–µ–¥–∞–µ–º X_test_features –∏ y_test –∫–∞–∫ eval_set
        xgb_model = self.train_xgboost(X_test_features, y_test, X_val=X_test_features, y_val=y_test)

        # –û—Ü–µ–Ω–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –º–æ–¥–µ–ª–∏ LSTM+GRU
        y_pred_nn_probs = final_model.predict(X_test)
        # –ï—Å–ª–∏ –≤—ã—Ö–æ–¥ –∏–º–µ–µ—Ç –ª–∏—à–Ω—é—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, (N, 1, 3)), —É–¥–∞–ª—è–µ–º –µ—ë:
        if y_pred_nn_probs.ndim == 3:
            y_pred_nn_probs = np.squeeze(y_pred_nn_probs, axis=1)
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –∫–ª–∞—Å—Å—ã:
        y_pred_nn = np.argmax(y_pred_nn_probs, axis=1)

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –ø–æ–º–æ—â—å—é feature_extractor
        X_test_features = feature_extractor.predict(X_test)
        # –ï—Å–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–º–µ—é—Ç –ª–∏—à–Ω—é—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, (N, 1, d)), —É–¥–∞–ª—è–µ–º –µ—ë:
        if X_test_features.ndim == 3:
            X_test_features = np.squeeze(X_test_features, axis=1)

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç XGBoost
        y_pred_xgb_probs = xgb_model.predict_proba(X_test_features)
        y_pred_xgb = np.argmax(y_pred_xgb_probs, axis=1)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ–±–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ —á–∏—Å–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        assert y_pred_nn.shape[0] == y_pred_xgb.shape[0], f"Mismatch: {y_pred_nn.shape[0]} vs {y_pred_xgb.shape[0]}"

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ–º —á–µ—Ä–µ–∑ mode)
        y_pred_ensemble = mode(np.vstack([y_pred_nn, y_pred_xgb]), axis=0)[0].flatten()


        y_pred_classes = y_pred_ensemble
        accuracy = accuracy_score(y_test, y_pred_classes)
        precision = precision_score(y_test, y_pred_classes, average='weighted')
        recall = recall_score(y_test, y_pred_classes, average='weighted')
        f1 = f1_score(y_test, y_pred_classes, average='weighted')


        logging.info(f"""
            –ú–µ—Ç—Ä–∏–∫–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏:
            Accuracy: {accuracy:.4f}
            Precision: {precision:.4f}
            Recall: {recall:.4f}
            F1-Score: {f1:.4f}
        """)
        
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–º –∫–∞—á–µ—Å—Ç–≤–µ
        '''
        if f1 >= 0.80:
            # –ü–∞–ø–∫–∞, –≥–¥–µ –±—É–¥—É—Ç –ª–µ–∂–∞—Ç—å –º–æ–¥–µ–ª–∏ (–≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ RunPod)
            saved_models_dir = "/workspace/saved_models/Market_Classifier"
            os.makedirs(saved_models_dir, exist_ok=True)

            # –ü—É—Ç—å –¥–ª—è LSTM-GRU –º–æ–¥–µ–ª–∏
            model_path = os.path.join(saved_models_dir, "final_model.h5")
            final_model.save(model_path)
                
            # –ü—É—Ç—å –¥–ª—è XGBoost-–º–æ–¥–µ–ª–∏
            xgb_path = os.path.join(saved_models_dir, "classifier_xgb_model.pkl")
            joblib.dump(xgb_model, xgb_path)

            logging.info(f"–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å LSTM-GRU —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
            logging.info(f"XGBoost-–º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {xgb_path}")
            return final_model
        else:
            logging.warning("–§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–æ–≤–æ–≥–æ (80% F1-score). –ú–æ–¥–µ–ª—å –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")
            return None
            '''
        # –ü–∞–ø–∫–∞, –≥–¥–µ –±—É–¥—É—Ç –ª–µ–∂–∞—Ç—å –º–æ–¥–µ–ª–∏ (–≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ RunPod)
        saved_models_dir = "/workspace/saved_models/Market_Classifier"
        os.makedirs(saved_models_dir, exist_ok=True)

        # –ü—É—Ç—å –¥–ª—è LSTM-GRU –º–æ–¥–µ–ª–∏
        model_path = os.path.join(saved_models_dir, "final_model.h5")
        final_model.save(model_path)
                
        # –ü—É—Ç—å –¥–ª—è XGBoost-–º–æ–¥–µ–ª–∏
        xgb_path = os.path.join(saved_models_dir, "classifier_xgb_model.pkl")
        joblib.dump(xgb_model, xgb_path)

        logging.info(f"–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å LSTM-GRU —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
        logging.info(f"XGBoost-–º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {xgb_path}")
        return final_model     




if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (TPU –∏–ª–∏ CPU/GPU)
    strategy = initialize_strategy()
    
    symbols = ['BTCUSDC', 'ETHUSDC']
    
    start_date = datetime(2021, 1, 1)
    end_date = datetime(2021, 1, 30)
    
    data_path = os.path.join("/workspace/data", "labeled_market_data.csv")
    os.makedirs(os.path.dirname(data_path), exist_ok=True)

    model_path = os.path.join("/workspace/saved_models", "market_condition_classifier.h5")
    os.makedirs(os.path.dirname(model_path), exist_ok=True)

    scaler_path = os.path.join("/workspace/saved_models", "scaler.pkl")
    os.makedirs(os.path.dirname(scaler_path), exist_ok=True)

    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    classifier = MarketClassifier()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        logging.info("–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.")
        labeled_data = classifier.fetch_and_label_all(
            symbols=symbols,
            start_date=start_date,
            end_date=end_date,
            save_path="labeled_data"
        )
        labeled_data.to_csv(data_path, index=True)
        logging.info(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {data_path}.")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        exit(1)

    # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    try:
        logging.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞.")
        classifier.train_market_condition_classifier(
            data_path=data_path,
            model_path=model_path,
            scaler_path=scaler_path
        )
        logging.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ.")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è: {e}")
        exit(1)

