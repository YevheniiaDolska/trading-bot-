import pandas as pd
import numpy as np
import tensorflow as tf
import time
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Add, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau
from tensorflow.keras import backend as K
from ta.trend import MACD, SMAIndicator, IchimokuIndicator
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.volatility import BollingerBands, AverageTrueRange
from ta.volume import OnBalanceVolumeIndicator
from binance.client import Client
from datetime import datetime, timedelta
import logging
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from sklearn.cluster import KMeans
from imblearn.combine import SMOTETomek
from ta.trend import SMAIndicator, MACD, IchimokuIndicator
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.volatility import BollingerBands, AverageTrueRange
from ta.volume import OnBalanceVolumeIndicator, ChaikinMoneyFlowIndicator
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
import sys
from tensorflow.keras.backend import clear_session
import glob
import requests
import zipfile
from io import BytesIO
from threading import Lock
from ta.trend import SMAIndicator
import matplotlib.pyplot as plt
from xgboost import XGBRegressor
from tensorflow.keras.models import Model
from xgboost import XGBClassifier
from sklearn.metrics import f1_score
import joblib
from filterpy.kalman import KalmanFilter
from utils_output import ensure_directory, copy_output, save_model_output
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.model_selection import TimeSeriesSplit
from sklearn.utils.class_weight import compute_class_weight

# –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
required_dirs = [
    "/workspace/logs",
    "/workspace/saved_models/bearish",
    "/workspace/checkpoints/bearish",
    "/workspace/data",
    "/workspace/output/bearish_ensemble"
]

for directory in required_dirs:
    os.makedirs(directory, exist_ok=True)
    
nan_threshold = 0.05
n_features = 32
    
    
class PnLCallback(tf.keras.callbacks.Callback):
    def __init__(self, val_data, returns_val, commission=0.0002):
        super().__init__()
        self.val_data = val_data
        self.returns_val = returns_val
        self.commission = commission

    def on_epoch_end(self, epoch, logs=None):
        X_val, y_val = self.val_data
        y_pred = np.argmax(self.model.predict(X_val, verbose=0), axis=1)
        pnl = 0.0
        for yp, yt, r in zip(y_pred, y_val, self.returns_val):
            if yp == 2 and yt == 2:  # SELL on true fall
                pnl += abs(r) - self.commission
            elif yp == 1 and yt == 1:
                pnl += abs(r) - self.commission
            else:
                pnl -= self.commission
        print(f"Epoch {epoch+1}: PnL (validation): {pnl:.6f}")

    
    
# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è DataFrame
def apply_in_chunks(df, func, chunk_size=100000):
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é func –∫ DataFrame df –ø–æ —á–∞–Ω–∫–∞–º –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.
    –ï—Å–ª–∏ df –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DataFrame, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç func(df).
    """
    import pandas as pd
    if not isinstance(df, pd.DataFrame):
        return func(df)
    # –ï—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –º–µ–Ω—å—à–µ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞, —Å—Ä–∞–∑—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if len(df) <= chunk_size:
        return func(df)
    chunks = [df.iloc[i:i+chunk_size] for i in range(0, len(df), chunk_size)]
    processed_chunks = [func(chunk) for chunk in chunks]
    return pd.concat(processed_chunks)

    

def initialize_strategy():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è GPU, –µ—Å–ª–∏ –æ–Ω–∏ –¥–æ—Å—Ç—É–ø–Ω—ã.
    –ï—Å–ª–∏ GPU –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é (CPU –∏–ª–∏ –æ–¥–∏–Ω GPU, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å).
    """
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –≤–∫–ª—é—á–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏,
            # —á—Ç–æ–±—ã TensorFlow –Ω–µ –∑–∞–Ω–∏–º–∞–ª –≤—Å—é –ø–∞–º—è—Ç—å —Å—Ä–∞–∑—É
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            strategy = tf.distribute.MirroredStrategy()  # –†–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU
            print("Running on GPU(s) with strategy:", strategy)
        except RuntimeError as e:
            print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:", e)
            strategy = tf.distribute.get_strategy()
    else:
        print("GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.")
        strategy = tf.distribute.get_strategy()
    return strategy


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

network_name = "bearish_neural_network"  # –ò–º—è –º–æ–¥–µ–ª–∏
checkpoint_path_regular = os.path.join("/workspace/checkpoints/bearish", f"{network_name}_checkpoint_epoch_{{epoch:02d}}.h5")
checkpoint_path_best = os.path.join("/workspace/checkpoints/bearish", f"{network_name}_best_model.h5")

# –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
nn_model_filename = os.path.join("/workspace/saved_models/bearish", 'bearish_nn_model.h5')
log_file = os.path.join("/workspace/logs", "training_log_bearish_nn.txt")


def save_logs_to_file(log_message):
    with open(log_file, 'a') as log_f:
        log_f.write(f"{datetime.now()}: {log_message}\n")
        
        

def cleanup_training_files():
    """
    –£–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª—ã —Å –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
    """
    files_to_delete = glob.glob("binance_data*.csv")  # –ò—â–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö
    for file_path in files_to_delete:
        try:
            os.remove(file_path)
            logging.info(f"üóë –£–¥–∞–ª—ë–Ω —Ñ–∞–π–ª: {file_path}")
        except Exception as e:
            logging.error(f"‚ö† –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {file_path}: {e}")
            
        
        
def calculate_cross_coin_features(data_dict):
    """
    –î–ª—è —Å–∫–∞–ª—å–ø–∏–Ω–≥–∞ —É–º–µ–Ω—å—à–∞–µ–º –æ–∫–Ω–∞ rolling —Å 30 –¥–æ 15, –∞ rolling(10) –¥–æ 5,
    —á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–µ–µ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ –∫–æ–ª–µ–±–∞–Ω–∏—è.
    """
    btc_data = data_dict['BTCUSDC']
    for symbol, df in data_dict.items():
        # CHANGED FOR SCALPING
        df['btc_corr'] = df['close'].rolling(15).corr(btc_data['close'])  # –±—ã–ª–æ 30
        df['rel_strength_btc'] = (df['close'].pct_change() - btc_data['close'].pct_change())
        
        # CHANGED FOR SCALPING
        df['beta_btc'] = (
            df['close'].pct_change().rolling(15).cov(btc_data['close'].pct_change())
            / btc_data['close'].pct_change().rolling(15).var()
        )  # –±—ã–ª–æ 30
        
        # CHANGED FOR SCALPING
        df['lead_lag_btc'] = df['close'].pct_change().shift(1).rolling(5).corr(
            btc_data['close'].pct_change()
        )  # –±—ã–ª–æ 10
        data_dict[symbol] = df
    return data_dict


def detect_anomalies(data):
    """
    –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ —Å–≤–µ—á–∏.
    –î–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ –Ω–∞ –∫–æ–ª–µ–±–∞–Ω–∏—è—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–∫–Ω–æ (10 —Å–≤–µ—á–µ–π) –∏ —Å–Ω–∏–∂–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è.
    """
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º z-score –¥–ª—è –æ–±—ä—ë–º–∞ –∏ —Ü–µ–Ω—ã –ø–æ –æ–∫–Ω—É –∏–∑ 10 —Å–≤–µ—á–µ–π
    data['volume_zscore'] = ((data['volume'] - data['volume'].rolling(10).mean()) / 
                             data['volume'].rolling(10).std())
    data['price_zscore'] = ((data['close'] - data['close'].rolling(10).mean()) / 
                            data['close'].rolling(10).std())
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ 2.5 –≤–º–µ—Å—Ç–æ 3 –¥–ª—è –±–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤
    data['is_anomaly'] = ((abs(data['volume_zscore']) > 2.5) & (data['close'] < data['close'].shift(1))) | \
                         (abs(data['price_zscore']) > 2.5)
    return data


def validate_volume_confirmation(data):
    """
    –£–∫–æ—Ä–∞—á–∏–≤–∞–µ–º –æ–∫–Ω–æ rolling(10) –¥–æ rolling(5), –∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ ‚Äì —Å 3 –¥–æ 2,
    —á—Ç–æ–±—ã —Å–∏–≥–Ω–∞–ª—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –±—ã–ª–∏ –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã–º–∏.
    """
    # CHANGED FOR SCALPING
    data['volume_trend_conf'] = np.where(
        (data['close'] > data['close'].shift(1)) & 
        (data['volume'] > data['volume'].rolling(5).mean()),  # –±—ã–ª–æ 10
        1,
        np.where(
            (data['close'] < data['close'].shift(1)) & 
            (data['volume'] > data['volume'].rolling(5).mean()),  # –±—ã–ª–æ 10
            -1,
            0
        )
    )
    # CHANGED FOR SCALPING
    data['volume_strength'] = (
        data['volume'] / data['volume'].rolling(5).mean()
    ) * data['volume_trend_conf']  # –±—ã–ª–æ 10
    
    # CHANGED FOR SCALPING
    data['volume_accumulation'] = data['volume_trend_conf'].rolling(2).sum()  # –±—ã–ª–æ 3
    return data


def remove_noise(data):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à—É–º–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞.
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏–ª—å—Ç—Ä–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –∫–æ–ª–µ–±–∞–Ω–∏—è–º –Ω–∞ 1-–º–∏–Ω—É—Ç–Ω–æ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–µ.
    """
    kf = KalmanFilter(dim_x=2, dim_z=1)
    kf.x = np.array([[data['close'].iloc[0]], [0.]])
    kf.F = np.array([[1., 1.], [0., 1.]])
    kf.H = np.array([[1., 0.]])
    kf.P *= 10
    kf.R = 2  # –ü–æ–Ω–∏–∂–µ–Ω–æ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –≤–ª–∏—è–Ω–∏—è —à—É–º–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è
    kf.Q = np.array([[0.1, 0.1], [0.1, 0.1]])  # –ü–æ–Ω–∏–∂–µ–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–∫–ª–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞
    smoothed_prices = []
    for price in data['close']:
        kf.predict()
        kf.update(price)
        smoothed_prices.append(float(kf.x[0]))
    data['smoothed_close'] = smoothed_prices
    return data



def preprocess_market_data(data_dict):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π.
    """
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    data_dict = calculate_cross_coin_features(data_dict)
    
    for symbol, df in data_dict.items():
        # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –∞–Ω–æ–º–∞–ª–∏–∏
        df = detect_anomalies(df)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ–±—ä–µ–º–æ–º
        df = validate_volume_confirmation(df)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —à—É–º
        df = remove_noise(df)
        
        data_dict[symbol] = df
    
    return data_dict
    

# –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—é —É–±—ã—Ç–∫–æ–≤
def custom_profit_loss(y_true, y_pred, commission=0.0002, missed_drop_penalty=2.0):
    class_weights = tf.constant([0.0, 1.0, 2.0], dtype=tf.float32)
    y_pred_scalar = tf.tensordot(y_pred, class_weights, axes=1)
    y_true = tf.cast(y_true, dtype=tf.float32)
    diff = y_pred_scalar - y_true
    log_factor = tf.math.log1p(tf.abs(diff) + 1e-7)

    false_long_penalty = 1.0
    false_short_penalty = 1.0
    missed_rally_penalty = 2.0
    # missed_drop_penalty —É—Å–∏–ª–∏–≤–∞–µ–º –µ—â–µ —Å–∏–ª—å–Ω–µ–µ –¥–ª—è SELL
    missed_drop_penalty = missed_drop_penalty

    # Case A: Hold
    loss_hold = tf.where(
        y_pred_scalar > 0.5,
        false_long_penalty * tf.abs(diff) * log_factor,
        tf.where(
            y_pred_scalar < 0.2,
            false_short_penalty * tf.abs(diff) * log_factor,
            tf.abs(diff) * log_factor
        )
    )
    # Case B: Buy
    loss_buy = tf.where(
        y_pred_scalar <= 0.5,
        missed_rally_penalty * tf.abs(diff) * log_factor,
        tf.abs(diff) * log_factor
    )
    # Case C: Sell
    loss_sell = tf.where(
        y_pred_scalar >= 0.2,
        missed_drop_penalty * tf.abs(diff) * log_factor,
        tf.abs(diff) * log_factor
    )
    base_loss = tf.where(
        tf.equal(y_true, 0.0),
        loss_hold,
        tf.where(
            tf.equal(y_true, 1.0),
            loss_buy,
            loss_sell
        )
    )
    uncertainty_penalty = tf.where(
        tf.logical_and(y_pred_scalar > 0.3, y_pred_scalar < 0.7),
        0.5 * tf.abs(diff) * log_factor,
        0.0
    )
    batch_size = tf.shape(diff)[0]
    time_indices = tf.cast(tf.range(batch_size), tf.float32)
    time_penalty = 0.1 * tf.abs(diff) * time_indices / tf.cast(batch_size, tf.float32)

    # –ö–æ–º–∏—Å—Å–∏—è –∏ flip-flop
    transaction_cost = commission * tf.reduce_sum(tf.abs(y_pred_scalar[1:] - y_pred_scalar[:-1]))
    flip_flop_penalty = 0.002 * tf.reduce_sum(tf.abs(y_pred_scalar[1:] - y_pred_scalar[:-1]))

    total_loss = tf.reduce_mean(base_loss + uncertainty_penalty + time_penalty) + transaction_cost + flip_flop_penalty

    tf.print("Batch max loss:", tf.reduce_max(base_loss))

    return total_loss



# Attention Layer
class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], input_shape[-1]), initializer='glorot_uniform', trainable=True)
        self.b = self.add_weight(name='att_bias', shape=(input_shape[-1],), initializer='zeros', trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        e = K.tanh(K.dot(inputs, self.W) + self.b)
        a = K.softmax(e, axis=1)
        output = inputs * a
        return K.sum(output, axis=1)


def load_all_data(symbols, start_date, end_date, interval='1m'):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.
    
    Args:
        symbols (list): –°–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä
        start_date (datetime): –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
        end_date (datetime): –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞
        interval (str): –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–≤–µ—á–µ–π
    
    Returns:
        pd.DataFrame: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ –≤—Å–µ–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    """
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞–∂–¥–æ–π –º–æ–Ω–µ—Ç–µ
    symbol_data_dict = {}
    
    logging.info(f"–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤: {symbols}")
    logging.info(f"–ü–µ—Ä–∏–æ–¥: —Å {start_date} –ø–æ {end_date}, –∏–Ω—Ç–µ—Ä–≤–∞–ª: {interval}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {executor.submit(get_historical_data, symbol, interval, start_date, end_date): symbol 
                  for symbol in symbols}
        
        for future in futures:
            symbol = futures[future]
            try:
                logging.info(f"–û–∂–∏–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}")
                symbol_data = future.result()
                
                if symbol_data is not None:
                    # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–Ω–µ—Ç—ã
                    symbol_data = detect_anomalies(symbol_data)
                    symbol_data = validate_volume_confirmation(symbol_data)
                    symbol_data = remove_noise(symbol_data)
                    
                    symbol_data_dict[symbol] = symbol_data
                    logging.info(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {len(symbol_data)}")
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
                save_logs_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
    
    if not symbol_data_dict:
        error_msg = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞"
        logging.error(error_msg)
        save_logs_to_file(error_msg)
        raise ValueError(error_msg)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    try:
        logging.info("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        symbol_data_dict = calculate_cross_coin_features(symbol_data_dict)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        all_data = []
        for symbol, df in symbol_data_dict.items():
            df['symbol'] = symbol
            all_data.append(df)
        
        data = pd.concat(all_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        expected_features = ['btc_corr', 'rel_strength_btc', 'beta_btc', 'lead_lag_btc',
                           'volume_strength', 'volume_accumulation', 'is_anomaly', 
                           'clean_returns']
        
        missing_features = [f for f in expected_features if f not in data.columns]
        if missing_features:
            logging.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        initial_rows = len(data)
        data = data.dropna()
        dropped_rows = initial_rows - len(data)
        if dropped_rows > 0:
            logging.info(f"–£–¥–∞–ª–µ–Ω–æ {dropped_rows} —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏")
        
        logging.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
        save_logs_to_file(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
        
        return data
        
    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}"
        logging.error(error_msg)
        save_logs_to_file(error_msg)
        raise


# –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
def get_historical_data(symbols, bearish_periods, interval="1m", save_path="/workspace/data/binance_data_bearish.csv"
):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å Binance (–∞—Ä—Ö–∏–≤) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –æ–¥–∏–Ω CSV-—Ñ–∞–π–ª.

    :param symbols: —Å–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä (–ø—Ä–∏–º–µ—Ä: ['BTCUSDC', 'ETHUSDC'])
    :param bearish_periods: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–µ—Ä–∏–æ–¥–∞–º–∏ (–ø—Ä–∏–º–µ—Ä: [{"start": "2019-01-01", "end": "2019-12-31"}])
    :param interval: –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "1m" - 1 –º–∏–Ω—É—Ç–∞)
    :param save_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'binance_data_bearish.csv')
    """
    base_url_monthly = "https://data.binance.vision/data/spot/monthly/klines"
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    
    all_data = []
    downloaded_files = set()
    download_lock = Lock()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º threading.Lock

    def download_and_process(symbol, period):
        start_date = datetime.strptime(period["start"], "%Y-%m-%d")
        end_date = datetime.strptime(period["end"], "%Y-%m-%d")
        temp_data = []

        for current_date in pd.date_range(start=start_date, end=end_date, freq='MS'):  # MS = Monthly Start
            year = current_date.year
            month = current_date.month
            month_str = f"{month:02d}"

            file_name = f"{symbol}-{interval}-{year}-{month_str}.zip"
            file_url = f"{base_url_monthly}/{symbol}/{interval}/{file_name}"

            # –ë–ª–æ–∫–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø –∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å–∫–∞—á–∞–Ω
            with download_lock:
                if file_name in downloaded_files:
                    logging.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {file_name}, —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ

                logging.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞: {file_url}")
                response = requests.head(file_url, timeout=5)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_url}")
                    continue

                logging.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ {file_url}...")
                response = requests.get(file_url, timeout=15)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_url}: –ö–æ–¥ {response.status_code}")
                    continue

                logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω {file_name}")
                downloaded_files.add(file_name)  # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

            try:
                zip_file = zipfile.ZipFile(BytesIO(response.content))
                csv_file = file_name.replace('.zip', '.csv')

                with zip_file.open(csv_file) as file:
                    df = pd.read_csv(file, header=None, names=[
                        "timestamp", "open", "high", "low", "close", "volume",
                        "close_time", "quote_asset_volume", "number_of_trades",
                        "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", "ignore"
                    ], dtype={
                        "timestamp": "int64",
                        "open": "float32",
                        "high": "float32",
                        "low": "float32",
                        "close": "float32",
                        "volume": "float32",
                        "quote_asset_volume": "float32",
                        "number_of_trades": "int32",
                        "taker_buy_base_asset_volume": "float32",
                        "taker_buy_quote_asset_volume": "float32"
                    })

                    # üõ† –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ timestamp
                    if "timestamp" not in df.columns:
                        logging.error(f"‚ùå –û—à–∏–±–∫–∞: –ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ df –¥–ª—è {symbol}")
                        return None

                    # üõ† –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime –∏ —Å—Ç–∞–≤–∏–º –∏–Ω–¥–µ–∫—Å
                    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", errors="coerce")
                    df.set_index("timestamp", inplace=True)
                    
                    # –ï—Å–ª–∏ –ø–æ –∫–∞–∫–æ–π-—Ç–æ –ø—Ä–∏—á–∏–Ω–µ —Å—Ç–æ–ª–±—Ü–∞ 'timestamp' –±–æ–ª—å—à–µ –Ω–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
                    if "timestamp" not in df.columns:
                        df["timestamp"] = df.index
                    
                    df["symbol"] = symbol

                    temp_data.append(df)
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {symbol} –∑–∞ {current_date.strftime('%Y-%m')}: {e}")

            time.sleep(0.5)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è–º–∏

        return pd.concat(temp_data) if temp_data else None

    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(download_and_process, symbol, period) for symbol in symbols for period in bearish_periods]
        for future in futures:
            result = future.result()
            if result is not None:
                all_data.append(result)

    if not all_data:
        logging.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –º–µ—Å—è—Ü–∞ –¥–∞–Ω–Ω—ã—Ö.")
        return None

    df = pd.concat(all_data, ignore_index=False)  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º ignore_index, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å timestamp  

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å –≤ DataFrame
    logging.info(f"üìä –ö–æ–ª–æ–Ω–∫–∏ –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º df: {df.columns}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å
    if "timestamp" not in df.columns and not isinstance(df.index, pd.DatetimeIndex):
        logging.error(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {df.columns}")
        return None

    # –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å resample
    df = df.resample('1min').ffill()  # –ú–∏–Ω—É—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã, –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º NaN
    num_nans = df.isna().sum().sum()
    if num_nans > 0:
        nan_percentage = num_nans / len(df)
        if nan_percentage > 0.05:  # –ï—Å–ª–∏ –±–æ–ª–µ–µ 5% –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–ø—É—â–µ–Ω—ã
            logging.warning(f"‚ö† –ü—Ä–æ–ø—É—â–µ–Ω–æ {nan_percentage:.2%} –¥–∞–Ω–Ω—ã—Ö! –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏.")
            df.dropna(inplace=True)
        else:
            logging.info(f"üîß –ó–∞–ø–æ–ª–Ω—è–µ–º {nan_percentage:.2%} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ffill.")
            df.fillna(method='ffill', inplace=True)  # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏

    df.to_csv(save_path)
    logging.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}")

    return save_path


def load_bearish_data(symbols, bearish_periods, interval="1m", save_path="binance_data_bearish.csv"):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø–µ—Ä–∏–æ–¥–æ–≤.
    –ï—Å–ª–∏ —Ñ–∞–π–ª save_path —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è —Å —É–∂–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å, –≥–¥–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è DataFrame —Å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
    –ß—Ç–µ–Ω–∏–µ CSV –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ —á–∞–Ω–∫–∞–º –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ –ø–∞–º—è—Ç—å.
    """
    CHUNK_SIZE = 200000  # —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è CSV

    # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äì —á–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ —á–∞–Ω–∫–∞–º
    if os.path.exists(save_path):
        try:
            chunks = []
            for chunk in pd.read_csv(save_path,
                                     index_col='timestamp',
                                     parse_dates=['timestamp'],
                                     on_bad_lines='skip',
                                     chunksize=CHUNK_SIZE):
                chunk = chunk.reset_index(drop=False)
                if 'timestamp' not in chunk.columns:
                    if 'index' in chunk.columns:
                        chunk.rename(columns={'index': 'timestamp'}, inplace=True)
                # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É
                numeric_cols = [
                    "open", "high", "low", "close", "volume",
                    "quote_asset_volume", "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume"
                ]
                for col in numeric_cols:
                    if col in chunk.columns:
                        chunk[col] = pd.to_numeric(chunk[col], errors='coerce')
                chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce', utc=True)
                chunk = chunk.dropna(subset=['timestamp'])
                chunk = chunk.set_index('timestamp')
                chunks.append(chunk)


            existing_data = pd.concat(chunks, ignore_index=False)
            logging.info(f"–°—á–∏—Ç–∞–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {save_path}, —Å—Ç—Ä–æ–∫: {len(existing_data)}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ {save_path}: {e}")
            existing_data = pd.DataFrame()
    else:
        existing_data = pd.DataFrame()

    all_data = {}  # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞–∂–¥–æ–º—É —Å–∏–º–≤–æ–ª—É
    logging.info(f"üöÄ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤: {symbols}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {
            executor.submit(get_historical_data, [symbol], bearish_periods, interval, save_path): symbol
            for symbol in symbols
        }
        for future in futures:
            symbol = futures[future]
            try:
                temp_file_path = future.result()
                if temp_file_path is not None:
                    # –ß–∏—Ç–∞–µ–º —Å–∫–∞—á–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–æ —á–∞–Ω–∫–∞–º
                    chunks = []
                    for chunk in pd.read_csv(temp_file_path,
                                             index_col='timestamp',
                                             parse_dates=['timestamp'],
                                             on_bad_lines='skip',
                                             chunksize=CHUNK_SIZE):
                        chunk = chunk.reset_index(drop=False)
                        if 'timestamp' not in chunk.columns:
                            if 'index' in chunk.columns:
                                chunk.rename(columns={'index': 'timestamp'}, inplace=True)
                        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É
                        numeric_cols = [
                            "open", "high", "low", "close", "volume",
                            "quote_asset_volume", "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume"
                        ]
                        for col in numeric_cols:
                            if col in chunk.columns:
                                chunk[col] = pd.to_numeric(chunk[col], errors='coerce')
                        chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce', utc=True)
                        chunk = chunk.dropna(subset=['timestamp'])
                        chunk = chunk.set_index('timestamp')
                        chunks.append(chunk)

                    if chunks:
                        new_data = pd.concat(chunks, ignore_index=False)
                    else:
                        new_data = pd.DataFrame()
                    
                    if symbol in all_data:
                        all_data[symbol].append(new_data)
                    else:
                        all_data[symbol] = [new_data]
                    logging.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–ª—è {symbol}. –§–∞–π–ª–æ–≤: {len(all_data[symbol])}")
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    for symbol in list(all_data.keys()):
        if all_data[symbol]:
            all_data[symbol] = pd.concat(all_data[symbol], ignore_index=False)
        else:
            del all_data[symbol]

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ –æ–¥–∏–Ω DataFrame
    if all_data:
        new_combined = pd.concat(all_data.values(), ignore_index=False)
    else:
        new_combined = pd.DataFrame()

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–µ—Å–ª–∏ –∏–º–µ—é—Ç—Å—è)
    if not existing_data.empty:
        combined = pd.concat([existing_data, new_combined], ignore_index=False)
    else:
        combined = new_combined

    # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ ---
    # –°–±—Ä–æ—Å–∏–º –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –¥–∞—Ç–∞–º–∏
    combined = combined.reset_index(drop=False)
    if 'timestamp' not in combined.columns:
        if 'index' in combined.columns:
            combined.rename(columns={'index': 'timestamp'}, inplace=True)
            logging.info("–°—Ç–æ–ª–±–µ—Ü 'index' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ 'timestamp' –ø—Ä–∏ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏.")
        else:
            logging.error("–ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü 'timestamp' –∏–ª–∏ 'index' –≤ –∏—Ç–æ–≥–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
            raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏.")
    combined['timestamp'] = pd.to_datetime(combined['timestamp'], errors='coerce', utc=True)
    combined = combined.dropna(subset=['timestamp'])
    combined = combined.set_index('timestamp')

    if not isinstance(combined.index, pd.DatetimeIndex):
        logging.error(f"–ü–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏—Ç–æ–≥–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –∏–º–µ–µ—Ç —Ç–∏–ø: {type(combined.index)}")
        raise ValueError("–ò–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex.")
    else:
        logging.info("–ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ DatetimeIndex.")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π DataFrame —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏–º–µ–Ω–∏ –∫–æ–ª–æ–Ω–∫–∏ –∏–Ω–¥–µ–∫—Å–∞
    combined.to_csv(save_path, index_label='timestamp')
    logging.info(f"üíæ –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path} (–∏—Ç–æ–≥–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫: {len(combined)})")
    return all_data


'''def aggregate_to_2min(data):
    """
    –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ 1 –º–∏–Ω—É—Ç–∞ –¥–æ 2 –º–∏–Ω—É—Ç.
    
    Parameters:
        data (pd.DataFrame): –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π –≤ –∫–æ–ª–æ–Ω–∫–µ 'timestamp'.
    
    Returns:
        pd.DataFrame: –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ 'timestamp'
    if 'timestamp' not in data.columns:
        logging.error("–ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏.")
        if isinstance(data.index, pd.DatetimeIndex):
            data['timestamp'] = data.index
            logging.info("–ò–Ω–¥–µ–∫—Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –∫–æ–ª–æ–Ω–∫—É 'timestamp'.")
        else:
            raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö.")

    # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    data['timestamp'] = pd.to_datetime(data['timestamp'])  # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ datetime
    data = data.set_index('timestamp')

    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    data = data.resample('2T').agg({
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum'
    }).dropna()  # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏

    logging.info(f"–ê–≥—Ä–µ–≥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(data)} —Å—Ç—Ä–æ–∫.")
    return data'''


def adjust_target(data, threshold=-0.0005, trend_window=50):
    """
    –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –∞–∫—Ü–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —Ä–µ–∑–∫–∏—Ö –ø–∞–¥–µ–Ω–∏—è—Ö.
    
    Parameters:
        data (pd.DataFrame): –î–∞–Ω–Ω—ã–µ —Å –∫–æ–ª–æ–Ω–∫–æ–π 'returns'.
        threshold (float): –ü–æ—Ä–æ–≥ –ø–∞–¥–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, -0.05 –¥–ª—è –ø–∞–¥–µ–Ω–∏–π > 5%).
        
    Returns:
        pd.DataFrame: –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –∫–æ–ª–æ–Ω–∫–æ–π 'target'.
    """
    data['target'] = (data['returns'] < threshold).astype(int)
    data['trend'] = (data['close'] < data['close'].rolling(trend_window).mean()).astype(int)
    data['target'] = np.where(data['target'] + data['trend'] > 0, 1, 0)
    logging.info(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {data['target'].value_counts().to_dict()}")
    return data

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def extract_features(data, commission=0.0002):
    logging.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞")
    data = data.copy()
    data = remove_noise(data)

    # –í—Å–µ —Ç–≤–æ–∏ –±–∞–∑–æ–≤—ã–µ —Ä–∞—Å—á—ë—Ç—ã –Ω–∏–∂–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ...
    returns = data['close'].pct_change()
    volume_agg = data['volume'].rolling(10).agg(['mean', 'std'])
    data['volume_ma'] = volume_agg['mean']
    data['volume_ratio'] = data['volume'] / (volume_agg['mean'] + 1e-7)
    price_acceleration = returns.diff()
    macd = MACD(data['smoothed_close'], window_slow=26, window_fast=12, window_sign=9)
    data['macd'] = macd.macd()
    data['macd_signal'] = macd.macd_signal()
    data['macd_diff'] = data['macd'] - data['macd_signal']
    data['macd_slope'] = data['macd_diff'].diff()
    data['rsi_5'] = RSIIndicator(data['close'], window=5).rsi()
    bb = BollingerBands(data['smoothed_close'], window=20)
    data['bb_high'] = bb.bollinger_hband()
    data['bb_low'] = bb.bollinger_lband()
    data['bb_width'] = bb.bollinger_wband()
    data['bb_position'] = (data['close'] - data['bb_low']) / ((data['bb_high'] - data['bb_low']) + 1e-7)

    def calculate_dynamic_thresholds(window=10):
        vol = returns.rolling(window).std()
        avg_vol = vol.rolling(100).mean()
        vol_ratio = vol / (avg_vol + 1e-7)
        base_strong = -0.001
        base_medium = -0.0005
        strong_threshold = base_strong * np.where(vol_ratio > 1.5, 1.5, np.where(vol_ratio < 0.5, 0.5, vol_ratio))
        medium_threshold = base_medium * np.where(vol_ratio > 1.5, 1.5, np.where(vol_ratio < 0.5, 0.5, vol_ratio))
        return strong_threshold, medium_threshold

    strong_threshold, medium_threshold = calculate_dynamic_thresholds()

    # --- –ö–û–†–†–ï–ö–¢–ò–†–û–í–ö–ê TARGET! ---
    # SELL —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø–∞–¥–µ–Ω–∏–µ —Å–∏–ª—å–Ω–µ–µ –∫–æ–º–∏—Å—Å–∏–∏
    future_ret = data['close'].shift(-1) / data['close'] - 1
    sell = ((future_ret < -commission) & (data['macd_diff'] < 0))
    # BUY –µ—Å–ª–∏ —è–≤–Ω—ã–π –æ—Ç–∫–∞—Ç –≤–≤–µ—Ä—Ö
    buy = ((future_ret > commission) & (data['bb_position'] < 0.6) & (data['rsi_5'] < 50))
    # HOLD ‚Äî –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ

    # –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è: –¥—Ä–æ–ø–∞–µ–º "—Å–ª–∞–±—ã–µ" –¥–≤–∏–∂–µ–Ω–∏—è –≤–æ–æ–±—â–µ (–º–æ–≥—É—Ç –ø–æ—Ä—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ!)
    mask_strong_move = sell | buy
    data = data[mask_strong_move].copy()  # —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ –ø—Ä–∏–±—ã–ª—å–Ω—ã–µ —Å–¥–µ–ª–∫–∏

    # –î–µ–ª–∞–µ–º –∏–º–µ–Ω–Ω–æ —Ç–≤–æ–π —Ñ–æ—Ä–º–∞—Ç target, –Ω–æ —Å —É—á–µ—Ç–æ–º –∫–æ–º–∏—Å—Å–∏–∏ –∏ —á–∏—Å—Ç–æ—Ç—ã –∫–ª–∞—Å—Å–æ–≤:
    data['target'] = np.where(
        sell, 2,
        np.where(buy, 1, 0)
    )

    # –í—Å–µ —Ç–≤–æ–∏ —Ä–∞—Å—á–µ—Ç—ã –Ω–∏–∂–µ –û–°–¢–ê–í–õ–Ø–Æ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π:
    data['log_returns'] = np.log(data['close'] / data['close'].shift(1))
    data['selling_pressure'] = data['volume'] * np.abs(data['close'] - data['open']) * np.where(data['close'] < data['open'], 1, 0)
    data['buying_pressure'] = data['volume'] * np.abs(data['close'] - data['open']) * np.where(data['close'] > data['open'], 1, 0)
    data['pressure_ratio'] = data['selling_pressure'] / (data['buying_pressure'].replace(0, 1))
    data['volatility'] = returns.rolling(10).std()
    data['volatility_ma'] = data['volatility'].rolling(20).mean()
    data['volatility_ratio'] = data['volatility'] / (data['volatility_ma'] + 1e-7)

    for period in [3, 5, 8, 13, 21]:
        data[f'sma_{period}'] = SMAIndicator(data['smoothed_close'], window=period).sma_indicator()
        data[f'ema_{period}'] = data['smoothed_close'].ewm(span=period, adjust=False).mean()

    data['obv'] = OnBalanceVolumeIndicator(data['close'], data['volume']).on_balance_volume()
    data['cmf'] = ChaikinMoneyFlowIndicator(data['high'], data['low'], data['close'], data['volume']).chaikin_money_flow()
    data['volume_change'] = data['volume'].pct_change()
    data['volume_ma_ratio'] = data['volume'] / data['volume'].rolling(20).mean()

    for period in [7, 14, 21]:
        data[f'rsi_{period}'] = RSIIndicator(data['close'], window=period).rsi()
    data['stoch_k'] = StochasticOscillator(data['high'], data['low'], data['close'], window=7).stoch()
    data['stoch_d'] = StochasticOscillator(data['high'], data['low'], data['close'], window=7).stoch_signal()
    data['support_level'] = data['low'].rolling(20).min()
    data['resistance_level'] = data['high'].rolling(20).max()
    data['price_to_support'] = data['close'] / data['support_level']

    data['candle_body'] = np.abs(data['close'] - data['open'])
    data['upper_shadow'] = data['high'] - np.maximum(data['close'], data['open'])
    data['lower_shadow'] = np.minimum(data['close'], data['open']) - data['low']
    data['body_to_shadow_ratio'] = data['candle_body'] / ((data['upper_shadow'] + data['lower_shadow']).replace(0, 0.001))

    data['price_level_breach'] = np.where(
        data['close'] < data['support_level'].shift(1), -1,
        np.where(data['close'] > data['resistance_level'].shift(1), 1, 0)
    )

    data['price_acceleration'] = returns.diff()
    data['volume_acceleration'] = data['volume_change'].diff()

    bb2 = BollingerBands(data['smoothed_close'], window=20)
    data['bb_high'] = bb2.bollinger_hband()
    data['bb_low'] = bb2.bollinger_lband()
    data['bb_width'] = bb2.bollinger_wband()
    data['bb_position'] = (data['close'] - data['bb_low']) / ((data['bb_high'] - data['bb_low']) + 1e-7)

    for period in [5, 10, 20]:
        data[f'atr_{period}'] = AverageTrueRange(data['high'], data['low'], data['close'], window=period).average_true_range()

    data['micro_trend'] = np.where(
        data['smoothed_close'] > data['smoothed_close'].shift(1), 1,
        np.where(data['smoothed_close'] < data['smoothed_close'].shift(1), -1, 0)
    )
    data['micro_trend_sum'] = data['micro_trend'].rolling(5).sum()
    data['volume_acceleration_5m'] = (data['volume'].diff() / data['volume'].rolling(5).mean()).fillna(0)

    if 'clean_returns' not in data.columns:
        data['clean_returns'] = data['smoothed_close'].pct_change()

    data['bearish_strength'] = np.where(
        (data['close'] < data['open']) &
        (data['volume'] > data['volume'].rolling(20).mean() * 1.5) &
        (data['close'] == data['low']) &
        (data['clean_returns'] < 0),
        3,
        np.where(
            (data['close'] < data['open']) &
            (data['volume'] > data['volume'].rolling(20).mean()) &
            (data['clean_returns'] < 0),
            2,
            np.where(data['close'] < data['open'], 1, 0)
        )
    )
    # –í—Å–µ —Ç–≤–æ–∏ –ø–æ–ª–µ–∑–Ω—ã–µ —Ñ–∏—á–∏ ‚Äî –Ω–∞ –º–µ—Å—Ç–µ!

    features = {}
    features['target'] = data['target']
    for col in data.columns:
        if col not in ['market_type']:
            features[col] = data[col]

    # –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ —Ñ–∏—á–∏
    for f in ['btc_corr', 'rel_strength_btc', 'beta_btc']:
        if f in data.columns:
            features[f] = data[f]

    for f in ['volume_strength', 'volume_accumulation']:
        if f in data.columns:
            features[f] = data[f]

    if 'clean_returns' in data.columns:
        features['clean_returns'] = data['clean_returns']

    features_df = pd.DataFrame(features)
    logging.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features_df.columns)}")
    logging.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN: {features_df.isna().sum().sum()}")
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:\n{features_df['target'].value_counts()}")
    logging.info(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {list(data.columns)}")

    num_nans = data.isna().sum().sum()
    if num_nans > 0:
        logging.warning(f"‚ö† –ù–∞–π–¥–µ–Ω–æ {num_nans} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π. –ó–∞–ø–æ–ª–Ω—è–µ–º...")
        data.fillna(0, inplace=True)
        
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –≤ —Ç–∞—Ä–≥–µ—Ç–µ —Ä–æ–≤–Ω–æ 3 –∫–ª–∞—Å—Å–∞ (0, 1, 2), –∏–Ω–∞—á–µ –æ—à–∏–±–∫–∞
    target_classes = sorted(data['target'].dropna().unique())
    if set(target_classes) != {0, 1, 2}:
        raise ValueError(f"Target must have exactly 3 classes [0,1,2], found: {target_classes}")

    return data.replace([np.inf, -np.inf], np.nan).ffill().bfill()



def remove_weak_moves(data, commission=0.0002):
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –¥–≤–∏–∂–µ–Ω–∏—è —Å–∏–ª—å–Ω–µ–µ –∫–æ–º–∏—Å—Å–∏–∏!
    data['future_returns'] = data['close'].shift(-1) / data['close'] - 1
    data = data[(np.abs(data['future_returns']) > commission)]
    data = data.drop(columns=['future_returns'])
    return data


def remove_outliers(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    return data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]


def add_clustering_feature(data):
    features_for_clustering = [
        'close', 'volume', 'rsi', 'macd', 'atr', 'sma_3',  'sma_5', 'sma_8', 'ema_3', 'ema_5','ema_8',
        'bb_width', 'macd_diff', 'obv', 'returns', 'log_returns'
    ]
    
    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ, –±–µ—Ä—ë–º —Å—ç–º–ø–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä, 200k —Å—Ç—Ä–æ–∫)
    max_for_kmeans = 200_000
    if len(data) > max_for_kmeans:
        sample_df = data.sample(n=max_for_kmeans, random_state=42)
    else:
        sample_df = data
        
    kmeans = KMeans(n_clusters=5, random_state=42)
    kmeans.fit(sample_df[features_for_clustering])
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å –∫–æ –≤—Å–µ–º—É DataFrame
    data['cluster'] = kmeans.fit_predict(data[features_for_clustering])
    return data

def prepare_data(data):
    logging.info("–ù–∞—á–∞–ª–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
    if data.empty:
        raise ValueError("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã")
    logging.info(f"–ò—Å—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {data.shape}")

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    if not isinstance(data.index, pd.DatetimeIndex):
        if 'timestamp' in data.columns:
            data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')
            data.set_index('timestamp', inplace=True)
        else:
            raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ 'timestamp'.")

    def filter_bad_features(df, nan_threshold=0.10):
        bad_cols = [col for col in df.columns if df[col].isna().mean() + np.isinf(df[col]).mean() > nan_threshold]
        if bad_cols:
            logging.warning(f"–£–¥–∞–ª—è—é—Ç—Å—è –ø–ª–æ—Ö–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {bad_cols}")
            df = df.drop(columns=bad_cols)
        return df

    def process_chunk(df_chunk):
        df_chunk = extract_features(df_chunk)
        df_chunk = df_chunk.replace([np.inf, -np.inf], np.nan).dropna()
        df_chunk = remove_weak_moves(df_chunk)
        df_chunk = df_chunk.loc[:, ~df_chunk.columns.duplicated()]
        df_chunk = remove_outliers(df_chunk)
        df_chunk = add_clustering_feature(df_chunk)
        return df_chunk

    # –ß–∞–Ω–∫–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–ø–æ –∫—É—Å–∫–∞–º –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ)
    data = apply_in_chunks(data, process_chunk, chunk_size=100000)
    data = filter_bad_features(data, nan_threshold)
    logging.info(f"–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è): {data.shape}")

    # –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ‚Äî –ò–°–ö–õ–Æ–ß–ê–ï–ú timestamp –∏ –¥—Ä—É–≥–∏–µ –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    features = [col for col in data.columns if col not in ['target', 'timestamp'] and pd.api.types.is_numeric_dtype(data[col])]
    selector = SelectKBest(mutual_info_classif, k=min(n_features, len(features)))
    X_selected = selector.fit_transform(data[features], data['target'])
    selected_features = [features[i] for i in range(len(features)) if selector.get_support()[i]]
    logging.info(f"–¢–æ–ø-{len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {selected_features}")
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ target:\n{data['target'].value_counts()}")

    return data[selected_features + ['target']], selected_features


def clean_data(X, y):
    logging.info("–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ NaN")
    mask = np.isfinite(X).all(axis=1)
    X_clean = X[mask]
    y_clean = y[mask]
    return X_clean, y_clean


def load_last_saved_model(model_filename):
    try:
        models = sorted(
            [f for f in os.listdir() if f.startswith(model_filename)],
            key=lambda x: int(x.split('_')[-1].split('.')[0])
        )
        last_model = models[-1] if models else None
        if last_model:
            logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –º–æ–¥–µ–ª–∏: {last_model}")
            return load_model(last_model)
        else:
            return None
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –º–æ–¥–µ–ª–∏: {e}")
        return None

def balance_classes(X, y, max_for_smote=300_000):
    """
    –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π numpy –∏ pandas.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: X_resampled (DataFrame), y_resampled (Series)
    """
    logging.info("–ù–∞—á–∞–ª–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤")
    if isinstance(X, pd.DataFrame):
        # –ü–æ–¥–≤—ã–±–æ—Ä–∫–∞ –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö
        if len(X) > max_for_smote:
            X_sample = X.sample(n=max_for_smote, random_state=42)
            y_sample = y.loc[X_sample.index] if isinstance(y, pd.Series) else y[X_sample.index.to_numpy()]
        else:
            X_sample = X
            y_sample = y
    else:
        n_samples = X.shape[0]
        idx = np.random.choice(n_samples, size=max_for_smote, replace=False) if n_samples > max_for_smote else np.arange(n_samples)
        X_sample = X[idx]
        y_sample = y[idx]

    if len(X_sample) == 0 or len(y_sample) == 0:
        raise ValueError("–î–∞–Ω–Ω—ã–µ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –ø—É—Å—Ç—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")

    smote_tomek = SMOTETomek(random_state=42)
    X_resampled, y_resampled = smote_tomek.fit_resample(X_sample, y_sample)
    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ pd.DataFrame/pd.Series
    if not isinstance(X_resampled, pd.DataFrame):
        X_resampled = pd.DataFrame(X_resampled, columns=X_sample.columns if hasattr(X_sample, 'columns') else None)
    if not isinstance(y_resampled, pd.Series):
        y_resampled = pd.Series(y_resampled)
    logging.info(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: X={X_resampled.shape}, y={y_resampled.shape}")
    logging.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {y_resampled.value_counts().to_dict()}")
    return X_resampled, y_resampled


def train_xgboost_on_embeddings(X_emb, y):
    """
    –û–±—É—á–∞–µ—Ç XGBoost-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö, –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö –∏–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
    –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è y –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ {0, 1, 2}.
    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞, –Ω–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ
    –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ª—É—á—à–µ–π –ø–æ–¥–≥–æ–Ω–∫–∏ –ø–æ–¥ –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ.
    """
    logging.info("–û–±—É—á–µ–Ω–∏–µ XGBoost –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö...")
    xgb_model = XGBClassifier(
        objective='multi:softprob',  # –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∑–∞–¥–∞—á–∞
        n_estimators=10,
        max_depth=3,
        learning_rate=0.01,
        random_state=42,
        num_class=3  # 3 –∫–ª–∞—Å—Å–∞
    )
    xgb_model.fit(X_emb, y)
    logging.info("XGBoost –æ–±—É—á–µ–Ω –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö.")
    return xgb_model



def prepare_timestamp_column(data):
    """
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å–æ–∑–¥–∞—ë—Ç —Å—Ç–æ–ª–±–µ—Ü 'timestamp' –¥–ª—è DataFrame.
    
    –ê–ª–≥–æ—Ä–∏—Ç–º:
      1. –ï—Å–ª–∏ —Å—Ç–æ–ª–±–µ—Ü 'timestamp' —É–∂–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –æ–Ω —É–¥–∞–ª—è–µ—Ç—Å—è.
      2. –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è data.reset_index(), —á—Ç–æ–±—ã –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å (–∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å DatetimeIndex)
         –≤ —Å—Ç–æ–ª–±–µ—Ü. –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –∏–º–µ–µ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–º—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'timestamp' –∏–ª–∏ –¥—Ä—É–≥–æ–µ), –µ–≥–æ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –≤ 'timestamp'.
      3. –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è ‚Äî DataFrame, –≥–¥–µ —Å—Ç–æ–ª–±–µ—Ü 'timestamp' —Ç–æ—á–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç.
      
    –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å–æ–∑–¥–∞—ë—Ç –Ω—É–∂–Ω—ã–π —Å—Ç–æ–ª–±–µ—Ü –±–µ–∑ —Ä–∏—Å–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è.
    """
    logging.info("–£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Å—Ç–æ–ª–±–µ—Ü 'timestamp' –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É—è reset_index().")
    
    # –ï—Å–ª–∏ 'timestamp' —É–∂–µ –µ—Å—Ç—å, —É–¥–∞–ª—è–µ–º –µ–≥–æ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    if 'timestamp' in data.columns:
        logging.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω —Å—Ç–æ–ª–±–µ—Ü 'timestamp'. –£–¥–∞–ª—è–µ–º –µ–≥–æ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ.")
        data = data.drop(columns=['timestamp'])
    
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å: –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex, —Ç–æ –æ–Ω –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—Å—è –≤ –∫–æ–ª–æ–Ω–∫—É
    data = data.reset_index()
    
    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —Å–±—Ä–æ—Å–∞ –∏–Ω–¥–µ–∫—Å –Ω–∞–∑—ã–≤–∞–ª—Å—è –Ω–µ 'timestamp', –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –µ–≥–æ
    if 'timestamp' not in data.columns:
        # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —Å–±—Ä–æ—à–µ–Ω –∫–∞–∫ 'index', —Ç–æ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –µ–≥–æ –≤ 'timestamp'
        if 'index' in data.columns:
            data.rename(columns={'index': 'timestamp'}, inplace=True)
            logging.info("–ö–æ–ª–æ–Ω–∫–∞ 'index' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –≤ 'timestamp'.")
        else:
            # –ï—Å–ª–∏ –Ω–∏ 'timestamp', –Ω–∏ 'index' –Ω–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç, —Å–æ–∑–¥–∞—ë–º —Å—Ç–æ–ª–±–µ—Ü –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
            data['timestamp'] = data.index
            logging.info("–°—Ç–æ–ª–±–µ—Ü 'timestamp' —Å–æ–∑–¥–∞–Ω –∏–∑ –∏–Ω–¥–µ–∫—Å–∞.")
    else:
        # –ü—Ä–∏–≤–æ–¥–∏–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Å—Ç–æ–ª–±–µ—Ü –∫ —Ç–∏–ø—É datetime
        data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')
        logging.info("–°—Ç–æ–ª–±–µ—Ü 'timestamp' –ø—Ä–∏–≤–µ–¥—ë–Ω –∫ —Ç–∏–ø—É datetime.")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–Ω–æ –ø–µ—Ä–µ—Å—Ç–∞–≤–∏—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'timestamp' –Ω–∞ –ø–µ—Ä–≤—É—é –ø–æ–∑–∏—Ü–∏—é, –µ—Å–ª–∏ —ç—Ç–æ –Ω—É–∂–Ω–æ
    cols = list(data.columns)
    if cols[0] != 'timestamp':
        cols.insert(0, cols.pop(cols.index('timestamp')))
        data = data[cols]
        logging.info("–°—Ç–æ–ª–±–µ—Ü 'timestamp' –ø–µ—Ä–µ—Å—Ç–∞–≤–ª–µ–Ω –≤ –Ω–∞—á–∞–ª–æ DataFrame.")
    
    return data


def build_bearish_neural_network(data, model_filename):
    """
    –û–±—É—á–∞–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞ —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–∏.

    Parameters:
        data (pd.DataFrame): –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        model_filename (str): –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ (.h5).

    Returns:
        ensemble_model: dict with keys nn_model, xgb_model, feature_extractor,
                        ensemble_weight_nn, ensemble_weight_xgb
        scaler: the fitted RobustScaler
    """
    logging.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–µ–¥–≤–µ–∂—å–µ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏")

    network_name = "bearish_nn"
    checkpoint_path_regular = f"checkpoints/{network_name}_checkpoint_epoch_{{epoch:02d}}.h5"
    checkpoint_path_best    = f"checkpoints/{network_name}_best.h5"

    # 1. –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å–æ–∑–¥–∞—ë–º —Å—Ç–æ–ª–±–µ—Ü timestamp
    data = prepare_timestamp_column(data)

    # 2. –í—ã–±–æ—Ä —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features = [c for c in data.columns if c not in ['target','timestamp'] and pd.api.types.is_numeric_dtype(data[c])]
    X_df = data[features].astype(float).reset_index(drop=True).copy()
    y_series = data['target'].astype(int).reset_index(drop=True).copy()

    # 3. –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π DataFrame –¥–ª—è returns_val (–≤–∞–∂–Ω–æ –¥–ª—è —Ü–µ–ø–æ—á–∫–∏!)
    data_for_returns = data.reset_index(drop=True).copy()
    logging.info(f"–ò—Å—Ö–æ–¥–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã: X={X_df.shape}, y={y_series.shape}")

    # 4. –£–¥–∞–ª—è–µ–º NaN –∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏
    mask = X_df.notnull().all(axis=1) & np.isfinite(X_df).all(axis=1)
    X_df, y_series, data_for_returns = X_df.loc[mask], y_series.loc[mask], data_for_returns.loc[mask]
    logging.info(f"–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: X={X_df.shape}, y={y_series.shape}")

    # 5. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
    X_bal, y_bal = balance_classes(X_df, y_series)
    X_bal = pd.DataFrame(X_bal).reset_index(drop=True)
    y_bal = pd.Series(y_bal).reset_index(drop=True)
    logging.info(f"–ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: X={X_bal.shape}, y={y_bal.shape}")

    # 6. rolling split —Ñ—É–Ω–∫—Ü–∏—è (–∏–Ω–¥–µ–∫—Å—ã ‚Äî —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ .values –∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø–æ –¥–ª–∏–Ω–µ!)
    def rolling_train_val_split(X, y, n_splits=5):
        tscv = TimeSeriesSplit(n_splits=n_splits)
        for train_idx, val_idx in tscv.split(X):
            yield train_idx, val_idx

    # 7. –ú–µ—Ç—Ä–∏–∫–∏ (–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å)
    def hft_metrics(y_true, y_pred):
        rt   = tf.reduce_mean(tf.abs(y_pred[1:] - y_pred[:-1]))
        stab = tf.reduce_mean(tf.abs(y_pred[2:] - 2*y_pred[1:-1] + y_pred[:-2]))
        return rt, stab

    def profit_ratio(y_true, y_pred):
        succ  = tf.reduce_sum(tf.where(tf.logical_and(y_true>=1, y_pred>=0.5), 1.0, 0.0))
        fals = tf.reduce_sum(tf.where(tf.logical_and(y_true==0, y_pred>=0.5), 1.0, 0.0))
        return succ / (fals + K.epsilon())

    # 8. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    os.makedirs('checkpoints', exist_ok=True)
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        strategy = tf.distribute.MirroredStrategy()
        logging.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è MirroredStrategy (GPU)")
    else:
        strategy = tf.distribute.get_strategy()
        logging.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è (CPU)")
        
    best_f1 = -1
    best_models = {}

    # 9. –†–æ–ª–ª–∏–Ω–≥-—Å–ø–ª–∏—Ç. –í –∫–∞–∂–¥–æ–º —Å–ø–ª–∏—Ç–µ –æ—Ç–¥–µ–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ
    for split_num, (train_idx, val_idx) in enumerate(rolling_train_val_split(X_bal.values, y_bal.values, n_splits=5)):
        print(f"Split {split_num+1}: Train {len(train_idx)}, Val {len(val_idx)}")

        X_train_df, X_val_df = X_bal.iloc[train_idx], X_bal.iloc[val_idx]
        y_train, y_val = y_bal.iloc[train_idx], y_bal.iloc[val_idx]

        # 10. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ø–ª–∏—Ç–∞!
        scaler = RobustScaler()
        X_train_np = scaler.fit_transform(X_train_df)
        X_val_np   = scaler.transform(X_val_df)

        # 11. –ö–ª–∞—Å—Å-–≤–µ—Å–∞ (–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã —Ç–æ–ª—å–∫–æ train-–≤—ã–±–æ—Ä–∫–µ!)
        classes = np.unique(y_train)
        weights = compute_class_weight('balanced', classes=classes, y=y_train)
        weights = [w*2 if cls==2 else w for w, cls in zip(weights, classes)]
        class_weights = {int(cls): float(w) for cls, w in zip(classes, weights)}

        # 12. LSTM input shape
        X_train = X_train_np.reshape((X_train_np.shape[0], 1, X_train_np.shape[1]))
        X_val   = X_val_np.reshape((X_val_np.shape[0], 1, X_val_np.shape[1]))

        # 13. tf.data.Dataset
        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32).prefetch(tf.data.AUTOTUNE)
        val_ds   = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(32).prefetch(tf.data.AUTOTUNE)

        # 14. returns_val ‚Äî –°–¢–†–û–ì–û –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º –≤–∞–ª–∏–¥–∞—Ü–∏–∏!
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç df, —Å –∫–æ—Ç–æ—Ä–æ–≥–æ –≤—Å–µ –Ω–∞—á–∏–Ω–∞–ª–∏, —á—Ç–æ–±—ã —Å–æ–≤–ø–∞–ª–æ –ø–æ –¥–ª–∏–Ω–µ
        returns_val = data_for_returns.iloc[val_idx]['close'].pct_change().fillna(0).values
        returns_val = np.nan_to_num(returns_val, nan=0)
        # –í–∞–∂–Ω–æ! returns_val –∏ y_val –æ–¥–Ω–æ–π –¥–ª–∏–Ω—ã

        # 15. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ –∫–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
        with strategy.scope():
            inp = Input(shape=(X_train.shape[1], X_train.shape[2]))
            def branch(x):
                x = LSTM(256, return_sequences=True)(x)
                x = BatchNormalization()(x)
                return Dropout(0.2)(x)
            b1, b2, b3 = branch(inp), branch(inp), branch(inp)
            x = Add()([b1, b2, b3])
            x = LSTM(256, return_sequences=False)(x)
            x = BatchNormalization()(x); x = Dropout(0.3)(x)
            x = Dense(128, activation='relu', name='embedding_layer')(x)
            x = BatchNormalization()(x); x = Dropout(0.3)(x)
            x = Dense(256, activation='relu')(x)
            x = BatchNormalization()(x); x = Dropout(0.3)(x)
            out = Dense(3, activation='softmax')(x)
            model = Model(inp, out)
            model.compile(
                optimizer=Adam(1e-3),
                loss=custom_profit_loss,
                metrics=[hft_metrics, profit_ratio]
            )

        # 16. Callbacks
        cb_reg  = ModelCheckpoint(checkpoint_path_regular, save_weights_only=True, verbose=1)
        cb_best = ModelCheckpoint(checkpoint_path_best, save_weights_only=True, save_best_only=True,
                                    monitor='val_loss', mode='min', verbose=1)
        cb_lr   = ReduceLROnPlateau('val_loss', factor=0.5, patience=3, verbose=1)
        cb_es   = EarlyStopping('val_loss', patience=15, restore_best_weights=True)
        pnl_callback = PnLCallback((X_val, y_val), returns_val, commission=0.0002)
        history = model.fit(train_ds, epochs=200, validation_data=val_ds,
                            class_weight=class_weights,
                            callbacks=[cb_reg, cb_best, cb_lr, cb_es, pnl_callback])

        # 17. –ß–∏—Å—Ç–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
        for f in glob.glob(f'checkpoints/{network_name}_checkpoint_epoch_*.h5'):
            if not f.endswith(f'{network_name}_best.h5'):
                os.remove(f)

        # 18. –ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å XGBoost (embeddings)
        feat_ext = Model(model.input, model.get_layer('embedding_layer').output)
        emb_tr = feat_ext.predict(X_train)
        emb_vl = feat_ext.predict(X_val)
        xgb_m = train_xgboost_on_embeddings(emb_tr, y_train)
        nn_pred = model.predict(X_val)
        xgb_pred = xgb_m.predict_proba(emb_vl)
        ens = 0.5*nn_pred + 0.5*xgb_pred
        cls = np.argmax(ens, axis=1)
        logging.info(f"F1 ensemble bearish: {f1_score(y_val, cls, average='weighted')}")

        # 19. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ø–ª–∏—Ç–∞ (–∏–ª–∏ –º–æ–∂–µ—à—å –¥–ª—è –ª—É—á—à–µ–≥–æ –∏–∑ –≤—Å–µ—Ö ‚Äî –¥–æ–±–∞–≤—å best split selection)
        # F1-score –∞–Ω—Å–∞–º–±–ª—è
        f1 = f1_score(y_val, cls, average='weighted')
        logging.info(f"F1 ensemble bearish (split {split_num+1}): {f1:.5f}")

        if f1 > best_f1:
            best_f1 = f1
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏, —Å–∫–µ–π–ª–µ—Ä, —Ñ–∏—á–∏ —ç—Ç–æ–≥–æ —Å–ø–ª–∏—Ç–∞
            best_models = {
                "nn_model": model,
                "xgb_model": xgb_m,
                "feature_extractor": feat_ext,
                "ensemble_weight_nn": 0.5,
                "ensemble_weight_xgb": 0.5,
                "scaler": scaler
            }
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –¥–∏—Å–∫ (–º–æ–∂–Ω–æ —Å—Ä–∞–∑—É —Å _best)
            model.save(model_filename)
            joblib.dump(xgb_m, os.path.join(os.path.dirname(model_filename), 'xgb_bearish.pkl'))
    # –í–û–ó–í–†–ê–©–ê–ï–ú –õ–£–ß–®–ò–ô –°–ü–õ–ò–¢
    return best_models, best_models["scaler"]



if __name__ == "__main__":
    try:
        strategy = initialize_strategy()
        
        symbols = ['BTCUSDC', 'ETHUSDC', 'BNBUSDC','XRPUSDC', 'ADAUSDC', 'SOLUSDC', 'DOTUSDC', 'LINKUSDC', 'TONUSDC', 'NEARUSDC']
        
        bearish_periods = [
            {"start": "2018-01-17", "end": "2018-03-31"},
            {"start": "2018-09-01", "end": "2018-12-31"},
            {"start": "2021-05-12", "end": "2021-08-31"},
            {"start": "2022-05-01", "end": "2022-07-31"},
            {"start": "2022-09-01", "end": "2022-12-15"},
            {"start": "2022-12-16", "end": "2023-01-31"}
        ]
        
        logging.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞...")
        data_dict = load_bearish_data(symbols, bearish_periods, interval="1m")
        if not data_dict:
            raise ValueError("‚ùå –û—à–∏–±–∫–∞: –î–∞–Ω–Ω—ã–µ –Ω–µ –±—ã–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
        data = pd.concat(data_dict.values(), ignore_index=False)
        if data.empty:
            raise ValueError("‚ùå –û—à–∏–±–∫–∞: –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã!")
        # –ó–¥–µ—Å—å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã–∑—ã–≤–∞–µ–º prepare_timestamp_column, —á—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü 'timestamp'
        data = prepare_timestamp_column(data)
        logging.info(f"‚Ñπ –ü–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Å—Ç–æ–ª–±—Ü–∞, –∫–æ–ª–æ–Ω–∫–∏: {data.columns.tolist()}")
        logging.info(f"üìà –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏: {data.shape}")
        logging.info("üõ† –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö...")
        data, selected_features = prepare_data(data)

        if data.empty:
            raise ValueError("‚ùå –û—à–∏–±–∫–∞: –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã!")
        logging.info("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞...")
        build_bearish_neural_network(data, nn_model_filename)
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã: {e}")
    finally:
        logging.info("üóë –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        cleanup_training_files()
        logging.info("üßπ –û—á–∏—Å—Ç–∫–∞ —Å–µ—Å—Å–∏–∏ TensorFlow...")
        clear_session()
        logging.info("‚úÖ –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    sys.exit(0)
