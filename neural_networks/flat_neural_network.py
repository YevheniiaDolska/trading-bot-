import pandas as pd
import numpy as np
import tensorflow as tf
import time
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Add, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau
from tensorflow.keras import backend as K
from ta.trend import MACD, SMAIndicator, IchimokuIndicator
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.volatility import BollingerBands, AverageTrueRange
from ta.volume import OnBalanceVolumeIndicator, ChaikinMoneyFlowIndicator
from binance.client import Client
from datetime import datetime, timedelta
import logging
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from sklearn.cluster import KMeans
from imblearn.combine import SMOTETomek
from ta.trend import SMAIndicator, MACD, IchimokuIndicator
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.volatility import BollingerBands, AverageTrueRange
from ta.volume import OnBalanceVolumeIndicator
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
import glob
from filterpy.kalman import KalmanFilter
from sklearn.feature_selection import SelectKBest, f_classif
import xgboost as xgb
from tensorflow.keras.models import Model
from xgboost import XGBClassifier
from sklearn.metrics import f1_score
from threading import Lock
import requests
import zipfile
from io import BytesIO
from sklearn.feature_selection import SelectKBest, f_classif
from xgboost import XGBClassifier
import xgboost as xgb  # –µ—Å–ª–∏ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è
import joblib
from utils_output import ensure_directory, copy_output, save_model_output



# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TPU
def initialize_strategy():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è GPU, –µ—Å–ª–∏ –æ–Ω–∏ –¥–æ—Å—Ç—É–ø–Ω—ã.
    –ï—Å–ª–∏ GPU –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é (CPU –∏–ª–∏ –æ–¥–∏–Ω GPU, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å).
    """
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –≤–∫–ª—é—á–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏,
            # —á—Ç–æ–±—ã TensorFlow –Ω–µ –∑–∞–Ω–∏–º–∞–ª –≤—Å—é –ø–∞–º—è—Ç—å —Å—Ä–∞–∑—É
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            strategy = tf.distribute.MirroredStrategy()  # –†–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU
            print("Running on GPU(s) with strategy:", strategy)
        except RuntimeError as e:
            print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:", e)
            strategy = tf.distribute.get_strategy()
    else:
        print("GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.")
        strategy = tf.distribute.get_strategy()
    return strategy

required_dirs = [
    "/workspace/logs",
    "/workspace/saved_models/flat",
    "/workspace/checkpoints/flat",
    "/workspace/data",
    "/workspace/output/flat_ensemble"
]

for directory in required_dirs:
    os.makedirs(directory, exist_ok=True)
    
    
# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è DataFrame
def apply_in_chunks(df, func, chunk_size=100000):
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é func –∫ DataFrame df –ø–æ —á–∞–Ω–∫–∞–º –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.
    –ï—Å–ª–∏ df –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DataFrame, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç func(df).
    """
    import pandas as pd
    if not isinstance(df, pd.DataFrame):
        return func(df)
    # –ï—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –º–µ–Ω—å—à–µ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞, —Å—Ä–∞–∑—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if len(df) <= chunk_size:
        return func(df)
    chunks = [df.iloc[i:i+chunk_size] for i in range(0, len(df), chunk_size)]
    processed_chunks = [func(chunk) for chunk in chunks]
    return pd.concat(processed_chunks)


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
nn_model_filename = os.path.join("/workspace/saved_models/flat", 'flat_nn_model.h5')
log_file = os.path.join("/workspace/logs", "training_log_flat_nn.txt")


network_name = "flat_neural_network"  # –ò–º—è –º–æ–¥–µ–ª–∏
checkpoint_path_regular = os.path.join("/workspace/checkpoints/flat", f"{network_name}_checkpoint_epoch_{{epoch:02d}}.h5")
checkpoint_path_best = os.path.join("/workspace/checkpoints/flat", f"{network_name}_best_model.h5")


def save_logs_to_file(log_message):
    with open(log_file, 'a') as log_f:
        log_f.write(f"{datetime.now()}: {log_message}\n")
        
        
def calculate_cross_coin_features(data_dict):
    btc_data = data_dict['BTCUSDC']
    for symbol, df in data_dict.items():
        # CHANGED FOR SCALPING
        df['btc_corr'] = df['close'].rolling(15).corr(btc_data['close'])
        df['rel_strength_btc'] = df['close'].pct_change() - btc_data['close'].pct_change()
        
        # CHANGED FOR SCALPING
        df['beta_btc'] = df['close'].pct_change().rolling(15).cov(btc_data['close'].pct_change()) / \
                         btc_data['close'].pct_change().rolling(15).var()
        
        # CHANGED FOR SCALPING
        df['lead_lag_btc'] = df['close'].pct_change().shift(1).rolling(5).corr(btc_data['close'].pct_change())
        data_dict[symbol] = df
    return data_dict


def detect_anomalies(data):
    """
    –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ —Å–≤–µ—á–∏.
    –î–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ –Ω–∞ –∫–æ–ª–µ–±–∞–Ω–∏—è—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–∫–Ω–æ (10 —Å–≤–µ—á–µ–π) –∏ —Å–Ω–∏–∂–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è.
    """
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º z-score –¥–ª—è –æ–±—ä—ë–º–∞ –∏ —Ü–µ–Ω—ã –ø–æ –æ–∫–Ω—É –∏–∑ 10 —Å–≤–µ—á–µ–π
    data['volume_zscore'] = ((data['volume'] - data['volume'].rolling(10).mean()) / 
                             data['volume'].rolling(10).std())
    data['price_zscore'] = ((data['close'] - data['close'].rolling(10).mean()) / 
                            data['close'].rolling(10).std())
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ 2.5 –≤–º–µ—Å—Ç–æ 3 –¥–ª—è –±–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤
    data['is_anomaly'] = ((abs(data['volume_zscore']) > 2.5) & (data['close'] < data['close'].shift(1))) | \
                         (abs(data['price_zscore']) > 2.5)
    return data


def validate_volume_confirmation(data):
    # CHANGED FOR SCALPING
    data['volume_trend_conf'] = np.where(
        (data['close'] > data['close'].shift(1)) & 
        (data['volume'] > data['volume'].rolling(5).mean()),
        1,
        np.where(
            (data['close'] < data['close'].shift(1)) & 
            (data['volume'] > data['volume'].rolling(5).mean()),
            -1,
            0
        )
    )
    data['volume_strength'] = (
        data['volume'] / data['volume'].rolling(5).mean()
    ) * data['volume_trend_conf']
    data['volume_accumulation'] = data['volume_trend_conf'].rolling(2).sum()
    return data



def remove_noise(data):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à—É–º–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞.
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏–ª—å—Ç—Ä–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –∫–æ–ª–µ–±–∞–Ω–∏—è–º –Ω–∞ 1-–º–∏–Ω—É—Ç–Ω–æ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–µ.
    """

    kf = KalmanFilter(dim_x=2, dim_z=1)
    kf.x = np.array([[data['close'].iloc[0]], [0.]])
    kf.F = np.array([[1., 1.], [0., 1.]])
    kf.H = np.array([[1., 0.]])
    kf.P *= 10
    kf.R = 2
    kf.Q = np.array([[0.1, 0.1], [0.1, 0.1]])  # –ü–æ–Ω–∏–∂–µ–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–∫–ª–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞
    smoothed_prices = []
    for price in data['close']:
        kf.predict()
        kf.update(price)
        smoothed_prices.append(float(kf.x[0]))
    data['smoothed_close'] = smoothed_prices
    return data



def preprocess_market_data(data_dict):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π.
    """
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    data_dict = calculate_cross_coin_features(data_dict)
    
    for symbol, df in data_dict.items():
        # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –∞–Ω–æ–º–∞–ª–∏–∏
        df = detect_anomalies(df)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ–±—ä–µ–º–æ–º
        df = validate_volume_confirmation(df)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —à—É–º
        if 'clean_returns' not in data.columns:
            raise ValueError("‚ùå ERROR: 'clean_returns' –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω –≤ remove_noise()!")

        # –ü–æ—Å–ª–µ remove_noise()
        df = remove_noise(df)

        if 'clean_returns' not in df.columns:
            raise ValueError("‚ùå ERROR: 'clean_returns' –ø—Ä–æ–ø–∞–ª –ø–æ—Å–ª–µ remove_noise()!")

        # –ü–µ—Ä–µ–¥ extract_features()
        if 'clean_returns' not in data.columns:
            data['clean_returns'] = 0.0
            print("üîß WARNING: 'clean_returns' –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞–ª, –¥–æ–±–∞–≤–ª–µ–Ω –≤—Ä—É—á–Ω—É—é!")

        
        data_dict[symbol] = df
    
    
    return data_dict

# –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—é –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ —á–∞—Å—Ç–æ—Ç—É —Å–¥–µ–ª–æ–∫
def custom_profit_loss(y_true, y_pred):
    """
    –í–∞—à "diff"-–ø–æ–¥—Ö–æ–¥ (BUY/SELL/HOLD) –≤ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–º –≤–∞—Ä–∏–∞–Ω—Ç–µ:
      - y_true: (batch,) ‚àà {0=HOLD,1=SELL,2=BUY}
      - y_pred: (batch,3), softmax: [p(HOLD), p(SELL), p(BUY)]
      
    –õ–æ–≥–∏–∫–∞ (–∞–Ω–∞–ª–æ–≥ –≤–∞—à–µ–π):
      diff = y_pred - y_true_onehot
      log_factor = log1p( sum(|diff|) )  [–Ω–∞ —Å—ç–º–ø–ª]
      underestimation_penalty = (y_true_onehot > y_pred)? (..)^2 : 0
      overestimation_penalty  = (y_true_onehot < y_pred)? (..)^2 : 0
      gain = max(diff,0)
      loss = abs(min(diff,0))
      total_loss = mean( loss*2 + log_factor*1.5 + underest*3 - gain*1.5 + overest*2 )
    """
    # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –º–µ—Ç–∫–∏ –≤ one-hot
    y_true_onehot = tf.one_hot(tf.cast(y_true, tf.int32), depth=3)  # (batch,3)

    # diff (batch,3)
    diff = y_pred - y_true_onehot

    # –õ–æ–≥-—Ñ–∞–∫—Ç–æ—Ä (–¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö –æ—à–∏–±–æ–∫)
    eps = 1e-7
    # –°—É–º–º–∞ |diff| –ø–æ –∫–ª–∞—Å—Å–∞–º => —Å–∫–∞–ª—è—Ä –Ω–∞ —Å—ç–º–ø–ª
    diff_magnitude = tf.reduce_sum(tf.abs(diff), axis=1)
    log_factor = tf.math.log1p(diff_magnitude + eps)  # (batch,)

    # –ù–µ–¥–æ–æ—Ü–µ–Ω–∫–∞ (–∫–æ–≥–¥–∞ y_true_onehot > y_pred)
    underestimation_penalty = tf.where(y_true_onehot > y_pred,
                                       tf.square(y_true_onehot - y_pred), 0.0)

    # –ü–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞ (–∫–æ–≥–¥–∞ y_true_onehot < y_pred)
    overestimation_penalty = tf.where(y_true_onehot < y_pred,
                                      tf.square(y_pred - y_true_onehot), 0.0)

    # gain = max(diff,0), loss = abs(min(diff,0)) (–ø–æ–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω–æ)
    gain = tf.math.maximum(diff, 0.0)      # (batch,3)
    negative_part = tf.math.minimum(diff, 0.0)
    loss_ = tf.math.abs(negative_part)     # (batch,3)

    # –°–±–æ—Ä–∫–∞ —á–∞—Å—Ç–µ–π
    # –ü–µ—Ä-–∫–ª–∞—Å—Å–Ω–∞—è —Å—É–º–º–∞: loss_*2 + underest*3 - gain*1.5 + overest*2
    per_class_term = (
        loss_ * 2.0 +
        underestimation_penalty * 3.0 -
        gain * 1.5 +
        overestimation_penalty * 2.0
    )  # shape=(batch,3)

    # –°–∫–ª–∞–¥—ã–≤–∞–µ–º –ø–æ –∫–ª–∞—Å—Å–∞–º
    per_sample_sum = tf.reduce_sum(per_class_term, axis=1)  # (batch,)

    # –î–æ–±–∞–≤–ª—è–µ–º log_factor *1.5
    total = per_sample_sum + log_factor * 1.5

    # –°—Ä–µ–¥–Ω–µ–µ –ø–æ –±–∞—Ç—á—É
    return tf.reduce_mean(total)



# Attention Layer
class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='att_weight',
                                 shape=(input_shape[-1], input_shape[-1]),
                                 initializer='glorot_uniform',
                                 trainable=True)
        self.b = self.add_weight(name='att_bias',
                                 shape=(input_shape[-1],),
                                 initializer='zeros',
                                 trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        e = tf.math.tanh(tf.tensordot(inputs, self.W, axes=[[2], [0]]) + self.b)
        a = tf.nn.softmax(e, axis=1)
        output = inputs * tf.expand_dims(a, -1)
        return tf.math.reduce_sum(output, axis=1)
    
    
def load_all_data(symbols, start_date, end_date, interval='1m'):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.
    
    Args:
        symbols (list): –°–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä
        start_date (datetime): –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
        end_date (datetime): –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞
        interval (str): –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–≤–µ—á–µ–π
    
    Returns:
        pd.DataFrame: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ –≤—Å–µ–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    """
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞–∂–¥–æ–π –º–æ–Ω–µ—Ç–µ
    symbol_data_dict = {}
    
    logging.info(f"–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤: {symbols}")
    logging.info(f"–ü–µ—Ä–∏–æ–¥: —Å {start_date} –ø–æ {end_date}, –∏–Ω—Ç–µ—Ä–≤–∞–ª: {interval}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {executor.submit(get_historical_data, symbol, interval, start_date, end_date): symbol 
                  for symbol in symbols}
        
        for future in futures:
            symbol = futures[future]
            try:
                logging.info(f"–û–∂–∏–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}")
                symbol_data = future.result()
                
                if symbol_data is not None:
                    # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–Ω–µ—Ç—ã
                    symbol_data = detect_anomalies(symbol_data)
                    symbol_data = validate_volume_confirmation(symbol_data)
                    symbol_data = remove_noise(symbol_data)
                    
                    symbol_data_dict[symbol] = symbol_data
                    logging.info(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {len(symbol_data)}")
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
                save_logs_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
    
    if not symbol_data_dict:
        error_msg = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞"
        logging.error(error_msg)
        save_logs_to_file(error_msg)
        raise ValueError(error_msg)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    try:
        logging.info("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        symbol_data_dict = calculate_cross_coin_features(symbol_data_dict)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        all_data = []
        for symbol, df in symbol_data_dict.items():
            df['symbol'] = symbol
            all_data.append(df)
        
        data = pd.concat(all_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        expected_features = ['btc_corr', 'rel_strength_btc', 'beta_btc', 'lead_lag_btc',
                           'volume_strength', 'volume_accumulation', 'is_anomaly', 
                           'clean_returns']
        
        missing_features = [f for f in expected_features if f not in data.columns]
        if missing_features:
            logging.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        initial_rows = len(data)
        data = data.dropna()
        dropped_rows = initial_rows - len(data)
        if dropped_rows > 0:
            logging.info(f"–£–¥–∞–ª–µ–Ω–æ {dropped_rows} —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏")
        
        logging.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
        save_logs_to_file(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
        
        return data
        
    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}"
        logging.error(error_msg)
        save_logs_to_file(error_msg)
        raise


def get_historical_data(symbols, flat_periods, interval="1m", save_path="/workspace/data/binance_data_flat.csv"
):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å Binance (–∞—Ä—Ö–∏–≤) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –æ–¥–∏–Ω CSV-—Ñ–∞–π–ª.

    :param symbols: —Å–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä (–ø—Ä–∏–º–µ—Ä: ['BTCUSDC', 'ETHUSDC'])
    :param flat_periods: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–µ—Ä–∏–æ–¥–∞–º–∏ (–ø—Ä–∏–º–µ—Ä: [{"start": "2019-01-01", "end": "2019-12-31"}])
    :param interval: –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "1m" - 1 –º–∏–Ω—É—Ç–∞)
    :param save_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'binance_data_flat.csv')
    """
    base_url_monthly = "https://data.binance.vision/data/spot/monthly/klines"
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    
    all_data = []
    downloaded_files = set()
    download_lock = Lock()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º threading.Lock

    def download_and_process(symbol, period):
        start_date = datetime.strptime(period["start"], "%Y-%m-%d")
        end_date = datetime.strptime(period["end"], "%Y-%m-%d")
        temp_data = []

        for current_date in pd.date_range(start=start_date, end=end_date, freq='MS'):  # MS = Monthly Start
            year = current_date.year
            month = current_date.month
            month_str = f"{month:02d}"

            file_name = f"{symbol}-{interval}-{year}-{month_str}.zip"
            file_url = f"{base_url_monthly}/{symbol}/{interval}/{file_name}"

            # –ë–ª–æ–∫–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø –∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å–∫–∞—á–∞–Ω
            with download_lock:
                if file_name in downloaded_files:
                    logging.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {file_name}, —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ

                logging.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞: {file_url}")
                response = requests.head(file_url, timeout=5)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_url}")
                    continue

                logging.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ {file_url}...")
                response = requests.get(file_url, timeout=15)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_url}: –ö–æ–¥ {response.status_code}")
                    continue

                logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω {file_name}")
                downloaded_files.add(file_name)  # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

            try:
                zip_file = zipfile.ZipFile(BytesIO(response.content))
                csv_file = file_name.replace('.zip', '.csv')

                with zip_file.open(csv_file) as file:
                    df = pd.read_csv(file, header=None, names=[
                        "timestamp", "open", "high", "low", "close", "volume",
                        "close_time", "quote_asset_volume", "number_of_trades",
                        "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", "ignore"
                    ], dtype={
                        "timestamp": "int64",
                        "open": "float32",
                        "high": "float32",
                        "low": "float32",
                        "close": "float32",
                        "volume": "float32",
                        "quote_asset_volume": "float32",
                        "number_of_trades": "int32",
                        "taker_buy_base_asset_volume": "float32",
                        "taker_buy_quote_asset_volume": "float32"
                    })

                    # üõ† –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ timestamp
                    if "timestamp" not in df.columns:
                        logging.error(f"‚ùå –û—à–∏–±–∫–∞: –ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ df –¥–ª—è {symbol}")
                        return None

                    # üõ† –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime –∏ —Å—Ç–∞–≤–∏–º –∏–Ω–¥–µ–∫—Å
                    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", errors="coerce")
                    df.set_index("timestamp", inplace=True)
                    
                    # –ï—Å–ª–∏ –ø–æ –∫–∞–∫–æ–π-—Ç–æ –ø—Ä–∏—á–∏–Ω–µ —Å—Ç–æ–ª–±—Ü–∞ 'timestamp' –±–æ–ª—å—à–µ –Ω–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
                    if "timestamp" not in df.columns:
                        df["timestamp"] = df.index
                    
                    df["symbol"] = symbol

                    temp_data.append(df)
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {symbol} –∑–∞ {current_date.strftime('%Y-%m')}: {e}")

            time.sleep(0.5)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è–º–∏

        return pd.concat(temp_data) if temp_data else None

    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(download_and_process, symbol, period) for symbol in symbols for period in flat_periods]
        for future in futures:
            result = future.result()
            if result is not None:
                all_data.append(result)

    if not all_data:
        logging.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –º–µ—Å—è—Ü–∞ –¥–∞–Ω–Ω—ã—Ö.")
        return None

    df = pd.concat(all_data, ignore_index=False)  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º ignore_index, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å timestamp  

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å –≤ DataFrame
    logging.info(f"üìä –ö–æ–ª–æ–Ω–∫–∏ –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º df: {df.columns}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å
    if "timestamp" not in df.columns and not isinstance(df.index, pd.DatetimeIndex):
        logging.error(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {df.columns}")
        return None

    # –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å resample
    df = df.resample('1min').ffill()  # –ú–∏–Ω—É—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã, –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º NaN
    num_nans = df.isna().sum().sum()
    if num_nans > 0:
        nan_percentage = num_nans / len(df)
        if nan_percentage > 0.05:  # –ï—Å–ª–∏ –±–æ–ª–µ–µ 5% –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–ø—É—â–µ–Ω—ã
            logging.warning(f"‚ö† –ü—Ä–æ–ø—É—â–µ–Ω–æ {nan_percentage:.2%} –¥–∞–Ω–Ω—ã—Ö! –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏.")
            df.dropna(inplace=True)
        else:
            logging.info(f"üîß –ó–∞–ø–æ–ª–Ω—è–µ–º {nan_percentage:.2%} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ffill.")
            df.fillna(method='ffill', inplace=True)  # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏

    df.to_csv(save_path)
    logging.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}")

    return save_path


def load_flat_data(symbols, flat_periods, interval="1m", save_path="binance_data_flat.csv"):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø–µ—Ä–∏–æ–¥–æ–≤.
    –ï—Å–ª–∏ —Ñ–∞–π–ª save_path —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è —Å —É–∂–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å, –≥–¥–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è DataFrame —Å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
    –ß—Ç–µ–Ω–∏–µ CSV –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ —á–∞–Ω–∫–∞–º –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ –ø–∞–º—è—Ç—å.
    """
    CHUNK_SIZE = 200000  # —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è CSV

    # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äì —á–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ —á–∞–Ω–∫–∞–º
    if os.path.exists(save_path):
        try:
            chunks = []
            for chunk in pd.read_csv(save_path,
                                     index_col='timestamp',
                                     parse_dates=['timestamp'],
                                     on_bad_lines='skip',
                                     chunksize=CHUNK_SIZE):
                # –°–±—Ä–æ—Å –∏–Ω–¥–µ–∫—Å–∞, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –¥–∞—Ç–∞–º–∏
                chunk = chunk.reset_index(drop=False)
                if 'timestamp' not in chunk.columns:
                    if 'index' in chunk.columns:
                        chunk.rename(columns={'index': 'timestamp'}, inplace=True)
                        logging.info("–°—Ç–æ–ª–±–µ—Ü 'index' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ 'timestamp'.")
                    else:
                        raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏.")
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç–æ–ª–±–µ—Ü 'timestamp' –≤ datetime —Å utc=True
                chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce', utc=True)
                # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏
                chunk = chunk.dropna(subset=['timestamp'])
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 'timestamp' –∫–∞–∫ –∏–Ω–¥–µ–∫—Å
                chunk = chunk.set_index('timestamp')
                chunks.append(chunk)
            existing_data = pd.concat(chunks, ignore_index=False)
            logging.info(f"–°—á–∏—Ç–∞–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {save_path}, —Å—Ç—Ä–æ–∫: {len(existing_data)}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ {save_path}: {e}")
            existing_data = pd.DataFrame()
    else:
        existing_data = pd.DataFrame()

    all_data = {}  # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞–∂–¥–æ–º—É —Å–∏–º–≤–æ–ª—É
    logging.info(f"üöÄ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤: {symbols}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {
            executor.submit(get_historical_data, [symbol], flat_periods, interval, save_path): symbol
            for symbol in symbols
        }
        for future in futures:
            symbol = futures[future]
            try:
                temp_file_path = future.result()
                if temp_file_path is not None:
                    # –ß–∏—Ç–∞–µ–º —Å–∫–∞—á–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–æ —á–∞–Ω–∫–∞–º
                    chunks = []
                    for chunk in pd.read_csv(temp_file_path,
                                             index_col='timestamp',
                                             parse_dates=['timestamp'],
                                             on_bad_lines='skip',
                                             chunksize=CHUNK_SIZE):
                        chunk = chunk.reset_index(drop=False)
                        if 'timestamp' not in chunk.columns:
                            if 'index' in chunk.columns:
                                chunk.rename(columns={'index': 'timestamp'}, inplace=True)
                                logging.info("–°—Ç–æ–ª–±–µ—Ü 'index' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ 'timestamp' –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")
                            else:
                                raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ –≤ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")
                        chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce', utc=True)
                        chunk = chunk.dropna(subset=['timestamp'])
                        chunk = chunk.set_index('timestamp')
                        chunks.append(chunk)
                    if chunks:
                        new_data = pd.concat(chunks, ignore_index=False)
                    else:
                        new_data = pd.DataFrame()
                    
                    if symbol in all_data:
                        all_data[symbol].append(new_data)
                    else:
                        all_data[symbol] = [new_data]
                    logging.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–ª—è {symbol}. –§–∞–π–ª–æ–≤: {len(all_data[symbol])}")
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    for symbol in list(all_data.keys()):
        if all_data[symbol]:
            all_data[symbol] = pd.concat(all_data[symbol], ignore_index=False)
        else:
            del all_data[symbol]

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ –æ–¥–∏–Ω DataFrame
    if all_data:
        new_combined = pd.concat(all_data.values(), ignore_index=False)
    else:
        new_combined = pd.DataFrame()

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–µ—Å–ª–∏ –∏–º–µ—é—Ç—Å—è)
    if not existing_data.empty:
        combined = pd.concat([existing_data, new_combined], ignore_index=False)
    else:
        combined = new_combined

    # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ ---
    # –°–±—Ä–æ—Å–∏–º –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –¥–∞—Ç–∞–º–∏
    combined = combined.reset_index(drop=False)
    if 'timestamp' not in combined.columns:
        if 'index' in combined.columns:
            combined.rename(columns={'index': 'timestamp'}, inplace=True)
            logging.info("–°—Ç–æ–ª–±–µ—Ü 'index' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ 'timestamp' –ø—Ä–∏ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏.")
        else:
            logging.error("–ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü 'timestamp' –∏–ª–∏ 'index' –≤ –∏—Ç–æ–≥–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
            raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏.")
    combined['timestamp'] = pd.to_datetime(combined['timestamp'], errors='coerce', utc=True)
    combined = combined.dropna(subset=['timestamp'])
    combined = combined.set_index('timestamp')

    if not isinstance(combined.index, pd.DatetimeIndex):
        logging.error(f"–ü–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏—Ç–æ–≥–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –∏–º–µ–µ—Ç —Ç–∏–ø: {type(combined.index)}")
        raise ValueError("–ò–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex.")
    else:
        logging.info("–ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ DatetimeIndex.")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π DataFrame —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏–º–µ–Ω–∏ –∫–æ–ª–æ–Ω–∫–∏ –∏–Ω–¥–µ–∫—Å–∞
    combined.to_csv(save_path, index_label='timestamp')
    logging.info(f"üíæ –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path} (–∏—Ç–æ–≥–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫: {len(combined)})")
    return all_data


'''def aggregate_to_2min(data):
    """
    –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ 1 –º–∏–Ω—É—Ç–∞ –¥–æ 2 –º–∏–Ω—É—Ç.
    """
    logging.info("–ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ 1 –º–∏–Ω—É—Ç–∞ –¥–æ 2 –º–∏–Ω—É—Ç")
    
    # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    if not isinstance(data.index, pd.DatetimeIndex):
        if 'timestamp' in data.columns:
            data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')
            data.set_index('timestamp', inplace=True)
        else:
            raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏ –∏–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex.")

    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    aggregated_data = data.resample('2T').agg({
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum'
    }).dropna()

    logging.info(f"–ê–≥—Ä–µ–≥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(aggregated_data)} —Å—Ç—Ä–æ–∫")
    return aggregated_data'''



def smooth_data(data, window=5):
    """
    –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞ —Å –ø–æ–º–æ—â—å—é —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ.
    
    Parameters:
        data (pd.Series): –ò—Å—Ö–æ–¥–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥.
        window (int): –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è.
        
    Returns:
        pd.Series: –°–≥–ª–∞–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
    """
    return data.rolling(window=window, min_periods=1).mean()



def extract_features(data):
    logging.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞")
    data = data.copy()
    
    # 1. –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    data['returns'] = data['close'].pct_change()
    data['log_returns'] = np.log(data['close'] / data['close'].shift(1))
    
    # 2. –ú–µ—Ç—Ä–∏–∫–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
    data['range_width'] = data['high'] - data['low']
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º rolling –¥–ª—è range_width —Å –æ–∫–Ω–æ–º 10 –∏ 20
    roll_range_10 = data['range_width'].rolling(10)
    data['range_stability'] = roll_range_10.std()
    roll_range_20 = data['range_width'].rolling(20)
    data['range_mean_20'] = roll_range_20.mean()
    data['range_ratio'] = data['range_width'] / data['range_mean_20']
    data['price_in_range'] = (data['close'] - data['low']) / data['range_width']
    
    # 3. –ë—ã—Å—Ç—Ä—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è HFT
    data['sma_3'] = SMAIndicator(data['close'], window=3).sma_indicator()
    data['ema_5'] = data['close'].ewm(span=5, adjust=False).mean()
    data['ema_8'] = data['close'].ewm(span=8, adjust=False).mean()
    if 'clean_returns' in data.columns:
        data['clean_volatility'] = data['clean_returns'].rolling(20).std()
    
    # 4. –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä—ã
    data['rsi_3'] = RSIIndicator(data['close'], window=3).rsi()
    data['rsi_5'] = RSIIndicator(data['close'], window=5).rsi()
    stoch = StochasticOscillator(data['high'], data['low'], data['close'], window=5)
    data['stoch_k'] = stoch.stoch()
    data['stoch_d'] = stoch.stoch_signal()
    
    # 5. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –º–∞–ª—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
    bb = BollingerBands(data['close'], window=10)
    data['bb_width'] = bb.bollinger_wband()
    data['bb_position'] = (data['close'] - bb.bollinger_lband()) / (bb.bollinger_hband() - bb.bollinger_lband())
    data['atr_5'] = AverageTrueRange(data['high'], data['low'], data['close'], window=5).average_true_range()
    
    # 6. –û–±—ä–µ–º–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ ‚Äì –≥—Ä—É–ø–ø–∏—Ä—É–µ–º rolling –ø–æ volume —Å –æ–∫–Ω–æ–º 10
    roll_volume_10 = data['volume'].rolling(10)
    data['volume_ma'] = roll_volume_10.mean()
    data['volume_std'] = roll_volume_10.std()
    data['volume_ratio'] = data['volume'] / data['volume_ma']
    data['volume_stability'] = data['volume_std'] / data['volume_ma']
    
    # 7. –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –ø—Ä–æ–±–æ—è
    data['breakout_intensity'] = abs(data['close'] - data['close'].shift(1)) / data['range_width']
    data['false_breakout'] = (data['high'] > data['high'].shift(1)) & (data['close'] < data['close'].shift(1))
    
    # 8. –ú–∏–∫—Ä–æ-–ø–∞—Ç—Ç–µ—Ä–Ω—ã
    data['micro_trend'] = np.where(
        data['close'] > data['close'].shift(1), 1,
        np.where(data['close'] < data['close'].shift(1), -1, 0)
    )
    data['micro_trend_change'] = (data['micro_trend'] != data['micro_trend'].shift(1)).astype(int)
    
    # 9. –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞
    volatility = data['returns'].rolling(20).std()
    avg_volatility = volatility.rolling(100).mean()
    threshold = 0.0002
    data['target'] = np.where(
        (data['returns'].shift(-1) > threshold) &
        (data['volume'] > data['volume_ma']) &
        (data['rsi_3'] < 40) &
        (data['bb_position'] < 0.3),
        2,
        np.where(
            (data['returns'].shift(-1) < -threshold) &
            (data['volume'] > data['volume_ma']) &
            (data['rsi_3'] > 60) &
            (data['bb_position'] > 0.7),
            1,
            0
        )
    )
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features = {}
    
    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤—Å–µ, —á—Ç–æ —É–∂–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ)
    for col in data.columns:
        if col not in ['target', 'market_type']:
            features[col] = data[col]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if 'btc_corr' in data.columns:
        features['btc_corr'] = data['btc_corr']
    if 'rel_strength_btc' in data.columns:
        features['rel_strength_btc'] = data['rel_strength_btc']
    if 'beta_btc' in data.columns:
        features['beta_btc'] = data['beta_btc']
            
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –æ–±—ä–µ–º–æ–º, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if 'volume_strength' in data.columns:
        features['volume_strength'] = data['volume_strength']
    if 'volume_accumulation' in data.columns:
        features['volume_accumulation'] = data['volume_accumulation']
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –æ—Ç —à—É–º–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if 'clean_returns' in data.columns:
        features['clean_returns'] = data['clean_returns']
        
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ DataFrame
    features_df = pd.DataFrame(features)
    
    return data.replace([np.inf, -np.inf], np.nan).ffill().bfill()


def remove_outliers(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    return data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]

def add_clustering_feature(data):
    features_for_clustering = [
        'close', 'volume', 'rsi', 'macd', 'atr', 'sma_3', 'ema_5', 'ema_8',
        'bb_width', 'macd_diff', 'obv', 'returns', 'log_returns'
    ]
    max_for_kmeans = 200_000
    if len(data) > max_for_kmeans:
        sample_df = data.sample(n=max_for_kmeans, random_state=42)
    else:
        sample_df = data
    kmeans = KMeans(n_clusters=5, random_state=42)
    kmeans.fit(sample_df[features_for_clustering])
    data['cluster'] = kmeans.fit_predict(data[features_for_clustering])
    return data

def prepare_data(data):
    logging.info("–ù–∞—á–∞–ª–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ
    if data.empty:
        raise ValueError("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã")

    logging.info(f"–ò—Å—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {data.shape}")

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    if not isinstance(data.index, pd.DatetimeIndex):
        if 'timestamp' in data.columns:
            data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')
            data.set_index('timestamp', inplace=True)
        else:
            raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ 'timestamp'.")

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    logging.info(f"–°—Ç–æ–ª–±—Ü—ã –≤ data –ø–µ—Ä–µ–¥ extract_features: {list(data.columns)}")
    
    # üöÄ –ü–µ—Ä–µ–¥ extract_features() –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ clean_returns
    missing_columns = [col for col in ['clean_returns'] if col not in data.columns]
    if missing_columns:
        print(f"üî¥ ERROR: –≠—Ç–∏ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–æ–ø–∞–ª–∏ –ø–µ—Ä–µ–¥ extract_features(): {missing_columns}")
        print("üîß –î–æ–±–∞–≤–ª—è–µ–º clean_returns –≤—Ä—É—á–Ω—É—é...")
        data['clean_returns'] = 0.0  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å–æ–∑–¥–∞—ë–º

    print(f"‚úÖ –ö–æ–ª–æ–Ω–∫–∏ –≤ data –ø–µ—Ä–µ–¥ extract_features: {list(data.columns)}")

    def process_chunk(df_chunk):
        df_chunk = extract_features(df_chunk)
        df_chunk = remove_outliers(df_chunk)
        df_chunk = add_clustering_feature(df_chunk)
        return df_chunk

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–æ —á–∞–Ω–∫–∞–º, –µ—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π
    data = apply_in_chunks(data, process_chunk, chunk_size=100000)
    logging.info(f"–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è): {data.shape}")


    # –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features = [col for col in data.columns if col != 'target']
    logging.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features)}")
    logging.info(f"–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features}")
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ target:\n{data['target'].value_counts()}")

    return data, features


def clean_data(X, y):
    logging.info("–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ NaN")
    mask = np.isfinite(X).all(axis=1)
    X_clean = X[mask]
    y_clean = y[mask]
    return X_clean, y_clean

def load_last_saved_model(model_filename):
    try:
        models = sorted(
            [f for f in os.listdir() if f.startswith(model_filename)],
            key=lambda x: int(x.split('_')[-1].split('.')[0])
        )
        last_model = models[-1] if models else None
        if last_model:
            logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –º–æ–¥–µ–ª–∏: {last_model}")
            return load_model(last_model)
        else:
            return None
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –º–æ–¥–µ–ª–∏: {e}")
        return None
    
    
def balance_classes(X, y, max_for_smote=300_000):
    """
    –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π numpy –∏ pandas.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    X: numpy.ndarray –∏–ª–∏ pandas.DataFrame ‚Äî –ø—Ä–∏–∑–Ω–∞–∫–∏
    y: numpy.ndarray –∏–ª–∏ pandas.Series ‚Äî –º–µ—Ç–∫–∏
    max_for_smote: int ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è SMOTETomek

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    X_resampled, y_resampled
    """
    logging.info("–ù–∞—á–∞–ª–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤")
    logging.info(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: X={getattr(X, 'shape', None)}, y={getattr(y, 'shape', None)}")
    logging.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã –≤ y: {np.unique(y, return_counts=True)}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏ –¥–µ–ª–∞–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if isinstance(X, pd.DataFrame):
        # –î–ª—è pandas DataFrame
        if len(X) > max_for_smote:
            X_sample = X.sample(n=max_for_smote, random_state=42)
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º pandas.Series –∏–ª–∏ numpy.ndarray –¥–ª—è y
            if isinstance(y, pd.Series):
                y_sample = y.loc[X_sample.index]
            else:
                # y ‚Äî numpy.ndarray, –∏—Å–ø–æ–ª—å–∑—É–µ–º iloc –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º DataFrame
                y_sample = y[X_sample.index.to_numpy()]
        else:
            X_sample = X
            y_sample = y
    else:
        # –î–ª—è numpy.ndarray –∏–ª–∏ –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤
        n_samples = X.shape[0]
        if n_samples > max_for_smote:
            idx = np.random.choice(n_samples, size=max_for_smote, replace=False)
            X_sample = X[idx]
            y_sample = y[idx]
        else:
            X_sample = X
            y_sample = y

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
    if len(X_sample) == 0 or len(y_sample) == 0:
        raise ValueError("–î–∞–Ω–Ω—ã–µ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –ø—É—Å—Ç—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")

    # –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTETomek
    smote_tomek = SMOTETomek(random_state=42)
    X_resampled, y_resampled = smote_tomek.fit_resample(X_sample, y_sample)

    logging.info(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: X={getattr(X_resampled, 'shape', None)}, y={getattr(y_resampled, 'shape', None)}")
    return X_resampled, y_resampled


def check_feature_quality(X, y):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é SelectKBest (f_classif)
    –∏ –ª–æ–≥–∏—Ä—É–µ—Ç —Ç–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏.
    """
    logging.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    logging.info(f"–§–æ—Ä–º–∞ X: {X.shape}")
    logging.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ X: {np.isnan(X).sum()}")
    
    selector = SelectKBest(score_func=f_classif, k='all')
    selector.fit(X, y)
    scores = selector.scores_
    feature_names = np.array([f"feature_{i}" for i in range(X.shape[1])])
    importance_df = pd.DataFrame({
        "Feature": feature_names,
        "Score": scores
    }).sort_values("Score", ascending=False)
    
    logging.info("–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    logging.info(importance_df.head(10).to_string(index=False))
    return importance_df

def train_xgboost_on_embeddings(X_emb, y):
    """
    –û–±—É—á–∞–µ—Ç XGBoost –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö, –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö –∏–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.
    –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –∏–º–µ–µ—Ç 3 –∫–ª–∞—Å—Å–∞.
    """
    logging.info("–û–±—É—á–µ–Ω–∏–µ XGBoost –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö...")
    xgb_model = XGBClassifier(
        objective='multi:softprob',
        num_class=3,
        learning_rate=0.1,
        n_estimators=100,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        verbosity=1
    )
    xgb_model.fit(X_emb, y)
    logging.info("XGBoost –æ–±—É—á–µ–Ω.")
    return xgb_model

def ensemble_predict(nn_model, xgb_model, feature_extractor, X_seq, weight_nn=0.5, weight_xgb=0.5):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö X_seq, –∫–æ–º–±–∏–Ω–∏—Ä—É—è –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∏ XGBoost
    —á–µ—Ä–µ–∑ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
      - nn_model: –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞).
      - xgb_model: –æ–±—É—á–µ–Ω–Ω–∞—è XGBoost-–º–æ–¥–µ–ª—å.
      - feature_extractor: –º–æ–¥–µ–ª—å, –∏–∑–≤–ª–µ–∫–∞—é—â–∞—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ nn_model.
      - X_seq: –≤—Ö–æ–¥–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.
      - weight_nn, weight_xgb: –≤–µ—Å–∞ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è (—Å—É–º–º–∏—Ä—É—é—Ç—Å—è –¥–æ 1).
      
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - –ò—Ç–æ–≥–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–∫–ª–∞—Å—Å—ã –∏–∑ {0, 1, 2}).
    """
    nn_pred_proba = nn_model.predict(X_seq)
    embeddings = feature_extractor.predict(X_seq)
    xgb_pred_proba = xgb_model.predict_proba(embeddings)
    final_pred_proba = weight_nn * nn_pred_proba + weight_xgb * xgb_pred_proba
    final_pred_classes = np.argmax(final_pred_proba, axis=1)
    return final_pred_classes


def build_flat_neural_network(data, model_filename):
    """
    –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞.

    Parameters:
        data (pd.DataFrame): –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        model_filename (str): –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.

    Returns:
        (ensemble_model: dict, scaler: RobustScaler)
    """
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    os.makedirs("checkpoints", exist_ok=True)
    network_name = "flat_neural_network"
    checkpoint_regular = f"checkpoints/{network_name}_checkpoint_epoch_{{epoch:02d}}.h5"
    checkpoint_best = f"checkpoints/{network_name}_best_model.h5"

    # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
    if os.path.exists(model_filename):
        try:
            model = load_model(model_filename, custom_objects={"custom_profit_loss": custom_profit_loss})
            logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –∏–∑ '{model_filename}', –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ.")
            return {"nn_model": model}, None
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            logging.info("–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –±—É–¥–µ—Ç –∑–∞–ø—É—â–µ–Ω–∞ –∑–∞–Ω–æ–≤–æ.")

    logging.info("–ù–∞—á–∞–ª–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ Flat NN")
    # –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features = [c for c in data.columns if c not in ['target','symbol','timestamp'] and pd.api.types.is_numeric_dtype(data[c])]
    logging.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {features}")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ X –∏ y
    X_df = data[features].astype(float).copy()
    y = data['target'].astype(int).copy()
    # –£–¥–∞–ª–µ–Ω–∏–µ NaN –∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–µ–π
    mask = X_df.notnull().all(axis=1) & np.isfinite(X_df).all(axis=1)
    X_df, y = X_df.loc[mask], y.loc[mask]
    logging.info(f"–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: X={X_df.shape}, y={y.shape}")

    # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    logging.info(X_df.describe().to_string())
    check_feature_quality(X_df.values, y.values)

    # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞
    X_bal, y_bal = balance_classes(X_df, y)
    logging.info(f"–ü–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: X={X_bal.shape}, y={y_bal.shape}")

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    X_train_df, X_val_df, y_train, y_val = train_test_split(
        X_bal, y_bal, test_size=0.2, random_state=42, stratify=y_bal
    )
    logging.info(f"Train: {X_train_df.shape}, Val: {X_val_df.shape}")

    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    scaler = RobustScaler()
    X_train_np = scaler.fit_transform(X_train_df)
    X_val_np = scaler.transform(X_val_df)
    # reshape –¥–ª—è LSTM
    X_train = X_train_np.reshape(X_train_np.shape[0], 1, X_train_np.shape[1])
    X_val = X_val_np.reshape(X_val_np.shape[0], 1, X_val_np.shape[1])
    logging.info(f"–ü–æ—Å–ª–µ reshape: X_train={X_train.shape}, X_val={X_val.shape}")

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32).prefetch(tf.data.AUTOTUNE)
    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(32).prefetch(tf.data.AUTOTUNE)

    # –°—Ç—Ä–∞—Ç–µ–≥–∏—è
    try:
        strategy = tf.distribute.MirroredStrategy()
        logging.info("MirroredStrategy –∞–∫—Ç–∏–≤–Ω–∞.")
    except:
        strategy = tf.distribute.get_strategy()
        logging.info("Default strategy.")

    with strategy.scope():
        inp = Input(shape=(X_train.shape[1], X_train.shape[2]))
        # —Ç—Ä–∏ –≤–µ—Ç–≤–∏
        def branch(x):
            x = LSTM(256, return_sequences=True)(x)
            x = BatchNormalization()(x)
            return Dropout(0.3)(x)
        b1, b2, b3 = branch(inp), branch(inp), branch(inp)
        x = Add()([b1, b2, b3])
        x = LSTM(256)(x)
        x = BatchNormalization()(x); x = Dropout(0.3)(x)
        x = Dense(256, activation='relu')(x)
        x = BatchNormalization()(x); x = Dropout(0.3)(x)
        x = Dense(128, activation='relu', name='embedding_layer')(x)
        x = BatchNormalization()(x); x = Dropout(0.3)(x)
        x = Dense(64, activation='relu')(x)
        x = BatchNormalization()(x)
        out = Dense(3, activation='softmax')(x)
        model = Model(inp, out)
        # –º–µ—Ç—Ä–∏–∫–∞ –∏ –∫–æ–º–ø–∏–ª—è—Ü–∏—è
        def flat_trading_metric(y_true, y_pred):
            tr = tf.reduce_max(y_true, axis=0) - tf.reduce_min(y_true, axis=0)
            pr = tf.reduce_max(y_pred, axis=0) - tf.reduce_min(y_pred, axis=0)
            return tf.abs(tr - pr)
        model.compile(optimizer=Adam(1e-3), loss=custom_profit_loss, metrics=[flat_trading_metric])

    # Callbacks
    cb_reg = ModelCheckpoint(checkpoint_regular, save_weights_only=True, verbose=1)
    cb_best = ModelCheckpoint(checkpoint_best, save_weights_only=True, save_best_only=True,
                               monitor='val_flat_trading_metric', mode='min', verbose=1)
    cb_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, mode='min')
    cb_es = EarlyStopping(monitor='val_flat_trading_metric', patience=15, restore_best_weights=True)

    # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    class_weights = {0:1.0, 1:2.5, 2:2.5}

    # –û–±—É—á–µ–Ω–∏–µ
    history = model.fit(
        train_ds, epochs=200, validation_data=val_ds,
        class_weight=class_weights,
        callbacks=[cb_reg, cb_best, cb_lr, cb_es]
    )
    # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    for f in glob.glob(f"checkpoints/{network_name}_checkpoint_epoch_*.h5"): 
        if f!=checkpoint_best: os.remove(f)

    # –ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
    feat_ext = Model(model.input, model.get_layer('embedding_layer').output)
    emb_tr = feat_ext.predict(X_train)
    emb_vl = feat_ext.predict(X_val)
    xgb_m = train_xgboost_on_embeddings(emb_tr, y_train)
    nn_pred = model.predict(X_val)
    xgb_pred = xgb_m.predict_proba(emb_vl)
    ens = 0.5*nn_pred + 0.5*xgb_pred
    classes = np.argmax(ens, axis=1)
    logging.info(f"F1 ensemble: {f1_score(y_val, classes, average='weighted')}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    model.save(model_filename)
    joblib.dump(xgb_m, os.path.join(os.path.dirname(model_filename),'xgb_flat.pkl'))

    return {"nn_model":model, "xgb_model":xgb_m, "feature_extractor":feat_ext,
            "ensemble_weight_nn":0.5, "ensemble_weight_xgb":0.5}, scaler




if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (TPU –∏–ª–∏ CPU/GPU)
    strategy = initialize_strategy()
    
    symbols = ['BTCUSDC', 'ETHUSDC', 'BNBUSDC','XRPUSDC', 'ADAUSDC', 'SOLUSDC', 'DOTUSDC', 'LINKUSDC', 'TONUSDC', 'NEARUSDC']

    # –ü–µ—Ä–∏–æ–¥—ã —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞
    flat_periods = [
        {"start": "2019-02-01", "end": "2019-04-30"},
        {"start": "2019-06-01", "end": "2019-08-31"},
        {"start": "2020-01-01", "end": "2020-02-29"},
        {"start": "2020-07-01", "end": "2020-08-31"},
        {"start": "2020-09-01", "end": "2020-10-31"},
        {"start": "2021-09-01", "end": "2021-10-31"},
        {"start": "2023-04-01", "end": "2023-05-31"}
    ]


    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã –∏–∑ —Å—Ç—Ä–æ–∫ –≤ datetime –æ–±—ä–µ–∫—Ç—ã
    start_date = datetime.strptime(flat_periods[0]["start"], "%Y-%m-%d")
    end_date = datetime.strptime(flat_periods[0]["end"], "%Y-%m-%d")

    data = load_flat_data(symbols, flat_periods, interval="1m")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ data ‚Äî —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å, –∏ –æ–Ω –Ω–µ –ø—É—Å—Ç
    if not isinstance(data, dict) or not data:
        raise ValueError("–û—à–∏–±–∫–∞: load_flat_data() –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å!")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ DataFrame –∏–∑ —Å–ª–æ–≤–∞—Ä—è data –≤ –æ–¥–∏–Ω –æ–±—â–∏–π DataFrame
    data = pd.concat(data.values(), ignore_index=False)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ 'timestamp' –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö
    if 'timestamp' not in data.columns:
        logging.warning("'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–¥–µ–∫—Å.")
        if isinstance(data.index, pd.DatetimeIndex):
            data['timestamp'] = data.index
            logging.info("–ò–Ω–¥–µ–∫—Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –∫–æ–ª–æ–Ω–∫—É 'timestamp'.")
        else:
            raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏ –∏–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex.")

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞,
    # —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å–æ–∑–¥–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –≤–∫–ª—é—á–∞—è 'clean_returns'
    data = detect_anomalies(data)
    data = validate_volume_confirmation(data)
    data = remove_noise(data)
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    data = extract_features(data)

    # –£–¥–∞–ª–µ–Ω–∏–µ NaN
    data.dropna(inplace=True)
    data.reset_index(drop=True, inplace=True)

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model, scaler = build_flat_neural_network(
        data, 
        model_filename="flat_nn_model.h5"
    )