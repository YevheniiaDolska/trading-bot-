import pandas as pd
import numpy as np
import time
import tensorflow as tf
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Add, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau
from tensorflow.keras import backend as K
from ta.trend import MACD, SMAIndicator, IchimokuIndicator
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.volatility import BollingerBands, AverageTrueRange
from ta.volume import OnBalanceVolumeIndicator
from binance.client import Client
from datetime import datetime, timedelta
import logging
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from sklearn.cluster import KMeans
from imblearn.combine import SMOTETomek
from ta.trend import SMAIndicator, MACD, IchimokuIndicator
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.volatility import BollingerBands, AverageTrueRange
from ta.volume import OnBalanceVolumeIndicator
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
import sys
from tensorflow.keras.backend import clear_session
import glob
from sklearn.feature_selection import SelectKBest, f_classif
from xgboost import XGBRegressor
from sklearn.metrics import f1_score
from tensorflow.keras.models import Model
import requests
import zipfile
from io import BytesIO
from threading import Lock
import joblib
from xgboost import XGBClassifier
from filterpy.kalman import KalmanFilter
from utils_output import ensure_directory, copy_output, save_model_output
from sklearn.impute import SimpleImputer


# –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
required_dirs = [
    "/workspace/logs",
    "/workspace/saved_models/bullish",
    "/workspace/checkpoints/bullish",
    "/workspace/data",
    "/workspace/output/bullish_ensemble"
]

for directory in required_dirs:
    os.makedirs(directory, exist_ok=True)
    

# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è DataFrame
def apply_in_chunks(df, func, chunk_size=100000):
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é func –∫ DataFrame df –ø–æ —á–∞–Ω–∫–∞–º –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.
    –ï—Å–ª–∏ df –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DataFrame, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç func(df).
    """
    import pandas as pd
    if not isinstance(df, pd.DataFrame):
        return func(df)
    # –ï—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –º–µ–Ω—å—à–µ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞, —Å—Ä–∞–∑—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if len(df) <= chunk_size:
        return func(df)
    chunks = [df.iloc[i:i+chunk_size] for i in range(0, len(df), chunk_size)]
    processed_chunks = [func(chunk) for chunk in chunks]
    return pd.concat(processed_chunks)



network_name = "bullish_neural_network"
checkpoint_path_regular = f"checkpoints/bullish/{network_name}_checkpoint_epoch_{{epoch:02d}}.h5"
checkpoint_path_best = f"checkpoints/bullish/{network_name}_best_model.h5"

# –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
nn_model_filename = os.path.join("/workspace/saved_models/bullish", 'bullish_nn_model.h5')
log_file = os.path.join("/workspace/logs", "training_log_bullish_nn.txt")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TPU
def initialize_strategy():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è GPU, –µ—Å–ª–∏ –æ–Ω–∏ –¥–æ—Å—Ç—É–ø–Ω—ã.
    –ï—Å–ª–∏ GPU –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é (CPU –∏–ª–∏ –æ–¥–∏–Ω GPU, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å).
    """
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –≤–∫–ª—é—á–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏,
            # —á—Ç–æ–±—ã TensorFlow –Ω–µ –∑–∞–Ω–∏–º–∞–ª –≤—Å—é –ø–∞–º—è—Ç—å —Å—Ä–∞–∑—É
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            strategy = tf.distribute.MirroredStrategy()  # –†–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU
            print("Running on GPU(s) with strategy:", strategy)
        except RuntimeError as e:
            print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:", e)
            strategy = tf.distribute.get_strategy()
    else:
        print("GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.")
        strategy = tf.distribute.get_strategy()
    return strategy

    
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def save_logs_to_file(log_message):
    with open(log_file, 'a') as log_f:
        log_f.write(f"{datetime.now()}: {log_message}\n")
        
def check_feature_quality(X, y):
    logging.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    logging.info(f"–§–æ—Ä–º–∞ X: {X.shape}")

    if isinstance(X, pd.DataFrame):
        logging.info("X –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤ –≤–∏–¥–µ DataFrame.")
        for col in X.columns:
            X[col] = pd.to_numeric(X[col], errors='coerce')
        logging.info(f"–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –≤ X –ø–æ—Å–ª–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è:\n{X.dtypes}")
    elif isinstance(X, np.ndarray):
        logging.info("X –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤ –≤–∏–¥–µ NumPy –º–∞—Å—Å–∏–≤–∞.")
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–∞—Å—Å–∏–≤ –≤ DataFrame —Å –∞–≤—Ç–æ-—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏ —Å—Ç–æ–ª–±—Ü–æ–≤
        X = pd.DataFrame(X, columns=[f"feature_{i}" for i in range(X.shape[1])])
        # –ï—Å–ª–∏ —Ç–∏–ø –∫–æ–ª–æ–Ω–æ–∫ –≤—Å–µ –∂–µ 'object', –ø—Ä–∏–≤–æ–¥–∏–º –∏—Ö –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É
        for col in X.columns:
            X[col] = pd.to_numeric(X[col], errors='coerce')
        logging.info(f"–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –≤ X –ø–æ—Å–ª–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è:\n{X.dtypes}")

    else:
        logging.error(f"–û—à–∏–±–∫–∞: X –∏–º–µ–µ—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø: {type(X)}. –û–∂–∏–¥–∞–µ—Ç—Å—è DataFrame –∏–ª–∏ NumPy –º–∞—Å—Å–∏–≤.")
        raise ValueError(f"–û—à–∏–±–∫–∞: –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö X ({type(X)})")

    # –£–¥–∞–ª—è–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    non_numeric_cols = X.columns[X.dtypes == 'object'].tolist()
    if non_numeric_cols:
        logging.warning(f"–£–¥–∞–ª—è–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {non_numeric_cols}")
        X.drop(columns=non_numeric_cols, inplace=True)

    if X.shape[1] == 0:
        logging.error("–û—à–∏–±–∫–∞: –í X –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
        raise ValueError("X –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ —Å –ø–æ–º–æ—â—å—é SimpleImputer,
    # —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å –≤—Å–µ —Å—Ç—Ä–æ–∫–∏, –∫–∞–∫ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç dropna()
    imputer = SimpleImputer(strategy='median')
    X = imputer.fit_transform(X)
    X = X.astype(np.float32)

    logging.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ X: {np.isnan(X).sum()}")

    # –í—ã—á–∏—Å–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    selector = SelectKBest(score_func=f_classif, k='all')
    selector.fit(X, y)
    scores = selector.scores_

    feature_names = np.array([f"feature_{i}" for i in range(X.shape[1])])
    importance_df = pd.DataFrame({
        "Feature": feature_names,
        "Score": scores
    }).sort_values("Score", ascending=False)

    logging.info("–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    logging.info(importance_df.head(10).to_string(index=False))

    return importance_df


def train_xgboost_on_embeddings(X_emb, y):
    logging.info("–û–±—É—á–µ–Ω–∏–µ XGBoost –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö...")
    xgb_model = XGBClassifier(
        objective='multi:softprob',  # –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∑–∞–¥–∞—á–∞
        n_estimators=100,
        max_depth=4,
        learning_rate=0.01,
        random_state=42,
        num_class=3
    )
    xgb_model.fit(X_emb, y)
    logging.info("XGBoost –æ–±—É—á–µ–Ω.")
    return xgb_model


        
def calculate_cross_coin_features(data_dict):
    btc_data = data_dict['BTCUSDC']
    for symbol, df in data_dict.items():
        # CHANGED FOR SCALPING
        df['btc_corr'] = df['close'].rolling(15).corr(btc_data['close'])  # –±—ã–ª–æ 30
        df['rel_strength_btc'] = (df['close'].pct_change() - btc_data['close'].pct_change())
        
        # CHANGED FOR SCALPING
        df['beta_btc'] = (
            df['close'].pct_change().rolling(15).cov(btc_data['close'].pct_change())
            / btc_data['close'].pct_change().rolling(15).var()
        )  # –±—ã–ª–æ 30
        
        # CHANGED FOR SCALPING
        df['lead_lag_btc'] = df['close'].pct_change().shift(1).rolling(5).corr(
            btc_data['close'].pct_change()
        )  # –±—ã–ª–æ 10
        data_dict[symbol] = df
    return data_dict



def detect_anomalies(data):
    """
    –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ —Å–≤–µ—á–∏.
    –î–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ –Ω–∞ –∫–æ–ª–µ–±–∞–Ω–∏—è—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–∫–Ω–æ (10 —Å–≤–µ—á–µ–π) –∏ —Å–Ω–∏–∂–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è.
    """
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º z-score –¥–ª—è –æ–±—ä—ë–º–∞ –∏ —Ü–µ–Ω—ã –ø–æ –æ–∫–Ω—É –∏–∑ 10 —Å–≤–µ—á–µ–π
    data['volume_zscore'] = ((data['volume'] - data['volume'].rolling(10).mean()) / 
                             data['volume'].rolling(10).std())
    data['price_zscore'] = ((data['close'] - data['close'].rolling(10).mean()) / 
                            data['close'].rolling(10).std())
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ 2.5 –≤–º–µ—Å—Ç–æ 3 –¥–ª—è –±–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤
    data['is_anomaly'] = ((abs(data['volume_zscore']) > 2.5) & (data['close'] < data['close'].shift(1))) | \
                         (abs(data['price_zscore']) > 2.5)
    return data


def validate_volume_confirmation(data):
    # CHANGED FOR SCALPING
    data['volume_trend_conf'] = np.where(
        (data['close'] > data['close'].shift(1)) & 
        (data['volume'] > data['volume'].rolling(5).mean()),  # –±—ã–ª–æ 10
        1,
        np.where(
            (data['close'] < data['close'].shift(1)) & 
            (data['volume'] > data['volume'].rolling(5).mean()),
            -1,
            0
        )
    )
    data['volume_strength'] = (
        data['volume'] / data['volume'].rolling(5).mean()
    ) * data['volume_trend_conf']  # –±—ã–ª–æ 10
    data['volume_accumulation'] = data['volume_trend_conf'].rolling(2).sum()  # –±—ã–ª–æ 3
    return data



def remove_noise(data):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à—É–º–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–∏–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞.
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏–ª—å—Ç—Ä–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –∫–æ–ª–µ–±–∞–Ω–∏—è–º –Ω–∞ 1-–º–∏–Ω—É—Ç–Ω–æ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–µ.
    """
    
    kf = KalmanFilter(dim_x=2, dim_z=1)
    kf.x = np.array([[data['close'].iloc[0]], [0.]])
    kf.F = np.array([[1., 1.], [0., 1.]])
    kf.H = np.array([[1., 0.]])
    kf.P *= 10
    kf.R = 2
    kf.Q = np.array([[0.1, 0.1], [0.1, 0.1]])  # –ü–æ–Ω–∏–∂–µ–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–∫–ª–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞
    smoothed_prices = []
    for price in data['close']:
        kf.predict()
        kf.update(price)
        smoothed_prices.append(float(kf.x[0]))
    data['smoothed_close'] = smoothed_prices
    return data


def preprocess_market_data(data_dict):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π.
    """
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    data_dict = calculate_cross_coin_features(data_dict)
    
    for symbol, df in data_dict.items():
        # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –∞–Ω–æ–º–∞–ª–∏–∏
        df = detect_anomalies(df)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ–±—ä–µ–º–æ–º
        df = validate_volume_confirmation(df)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —à—É–º
        df = remove_noise(df)
        
        data_dict[symbol] = df
    
    return data_dict

# –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –±—ã—á—å–µ–≥–æ —Ä—ã–Ω–∫–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –ø—Ä–∏–±—ã–ª—å
def custom_profit_loss(y_true, y_pred):
    """
    –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–∏–≥–Ω–∞–ª–æ–≤ HOLD –∏
    —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã–¥–∞–≤–∞—Ç—å –±–æ–ª—å—à–µ —è–≤–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ BUY/SELL –Ω–∞ –±—ã—á—å–µ–º —Ä—ã–Ω–∫–µ.
    
    –ü—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è:
      - y_true: —Ç–µ–Ω–∑–æ—Ä –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫, –≥–¥–µ 0 = HOLD, 1 = BUY, 2 = SELL.
      - y_pred: —Ç–µ–Ω–∑–æ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π) –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1]:
            < 0.2  => SELL,
         > 0.7  => BUY,
         –º–µ–∂–¥—É 0.2 –∏ 0.7 => HOLD.
    """
    y_true = tf.cast(y_true, dtype=tf.float32)
    y_pred = tf.cast(y_pred, dtype=tf.float32)
    diff = y_pred - y_true
    log_factor = tf.math.log1p(tf.abs(diff) + 1e-7)
    
    # –î–ª—è –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ HOLD (0):
    # –ï—Å–ª–∏ y_pred –ø–æ–ø–∞–¥–∞–µ—Ç –≤ –∏–Ω—Ç–µ—Ä–≤–∞–ª HOLD (0.2 - 0.7), —à—Ç—Ä–∞—Ñ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è (2.0√ó),
    # –∞ –µ—Å–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —à—Ç—Ä–∞—Ñ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è (0.5√ó).
    loss_hold = tf.where(
        y_pred > 0.7,
        0.5 * tf.abs(diff) * log_factor,
        tf.where(
            y_pred < 0.2,
            0.5 * tf.abs(diff) * log_factor,
            2.0 * tf.abs(diff) * log_factor
        )
    )
    
    # –î–ª—è –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ BUY (1): –µ—Å–ª–∏ y_pred –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç 0.7, –ø—Ä–∏–º–µ–Ω—è–µ–º –ø–æ–≤—ã—à–µ–Ω–Ω—ã–π —à—Ç—Ä–∞—Ñ (3.0√ó)
    loss_buy = tf.where(
        y_pred <= 0.7,
        3.0 * tf.abs(diff) * log_factor,
        tf.abs(diff) * log_factor
    )
    
    # –î–ª—è –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ SELL (2): –µ—Å–ª–∏ y_pred –Ω–µ –Ω–∏–∂–µ 0.2, –ø—Ä–∏–º–µ–Ω—è–µ–º –ø–æ–≤—ã—à–µ–Ω–Ω—ã–π —à—Ç—Ä–∞—Ñ (3.0√ó)
    loss_sell = tf.where(
        y_pred >= 0.2,
        3.0 * tf.abs(diff) * log_factor,
        tf.abs(diff) * log_factor
    )
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–ª—É—á–∞–∏ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é y_true (0: HOLD, 1: BUY, 2: SELL)
    base_loss = tf.where(
        tf.equal(y_true, 0.0),
        loss_hold,
        tf.where(
            tf.equal(y_true, 1.0),
            loss_buy,
            loss_sell
        )
    )
    
    # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: –µ—Å–ª–∏ y_pred –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø—Ä–æ–º–µ–∂—É—Ç–∫–µ [0.3, 0.7], –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —à—Ç—Ä–∞—Ñ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç 1.0)
    uncertainty_penalty = tf.where(
        tf.logical_and(y_pred > 0.3, y_pred < 0.7),
        1.0 * tf.abs(diff) * log_factor,
        0.0
    )
    
    # –®—Ç—Ä–∞—Ñ –∑–∞ –∑–∞–¥–µ—Ä–∂–∫—É —Ä–µ–∞–∫—Ü–∏–∏: –Ω–µ–±–æ–ª—å—à–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —à—Ç—Ä–∞—Ñ–∞ —Å —Ä–æ—Å—Ç–æ–º –Ω–æ–º–µ—Ä–∞ –ø—Ä–∏–º–µ—Ä–∞
    time_penalty = 0.1 * tf.abs(diff) * (
    tf.expand_dims(tf.cast(tf.range(tf.shape(diff)[0]), tf.float32), axis=1) / tf.cast(tf.shape(diff)[0], tf.float32)
        )

    
    # –®—Ç—Ä–∞—Ñ –∑–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–µ –∏–∑–¥–µ—Ä–∂–∫–∏: —É—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–∑–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —Ç–æ—á–∫–∞–º–∏
    transaction_cost = 0.001 * tf.reduce_sum(tf.abs(y_pred[1:] - y_pred[:-1]))
    
    total_loss = tf.reduce_mean(base_loss + uncertainty_penalty + time_penalty) + transaction_cost
    return total_loss



# Attention Layer
class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='att_weight',
                                 shape=(input_shape[-1], input_shape[-1]),
                                 initializer='glorot_uniform',
                                 trainable=True)
        self.b = self.add_weight(name='att_bias',
                                 shape=(input_shape[-1],),
                                 initializer='zeros',
                                 trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        e = tf.math.tanh(tf.matmul(inputs, self.W) + self.b)
        a = tf.nn.softmax(e, axis=1)
        output = inputs * a
        return tf.math.reduce_sum(output, axis=1)


def load_all_data(symbols, start_date, end_date, interval='1m'):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.
    
    Args:
        symbols (list): –°–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä
        start_date (datetime): –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
        end_date (datetime): –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞
        interval (str): –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–≤–µ—á–µ–π
    
    Returns:
        pd.DataFrame: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ –≤—Å–µ–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    """
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞–∂–¥–æ–π –º–æ–Ω–µ—Ç–µ
    symbol_data_dict = {}
    
    logging.info(f"–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤: {symbols}")
    logging.info(f"–ü–µ—Ä–∏–æ–¥: —Å {start_date} –ø–æ {end_date}, –∏–Ω—Ç–µ—Ä–≤–∞–ª: {interval}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {executor.submit(get_historical_data, symbol, interval, start_date, end_date): symbol 
                  for symbol in symbols}
        
        for future in futures:
            symbol = futures[future]
            try:
                logging.info(f"–û–∂–∏–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}")
                symbol_data = future.result()
                
                if symbol_data is not None:
                    # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–Ω–µ—Ç—ã
                    symbol_data = detect_anomalies(symbol_data)
                    symbol_data = validate_volume_confirmation(symbol_data)
                    symbol_data = remove_noise(symbol_data)
                    
                    symbol_data_dict[symbol] = symbol_data
                    logging.info(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {len(symbol_data)}")
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
                save_logs_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
    
    if not symbol_data_dict:
        error_msg = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞"
        logging.error(error_msg)
        save_logs_to_file(error_msg)
        raise ValueError(error_msg)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    try:
        logging.info("–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        symbol_data_dict = calculate_cross_coin_features(symbol_data_dict)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        all_data = []
        for symbol, df in symbol_data_dict.items():
            df['symbol'] = symbol
            all_data.append(df)
        
        data = pd.concat(all_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        expected_features = ['btc_corr', 'rel_strength_btc', 'beta_btc', 'lead_lag_btc',
                           'volume_strength', 'volume_accumulation', 'is_anomaly', 
                           'clean_returns']
        
        missing_features = [f for f in expected_features if f not in data.columns]
        if missing_features:
            logging.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features}")
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        initial_rows = len(data)
        data = data.dropna()
        dropped_rows = initial_rows - len(data)
        if dropped_rows > 0:
            logging.info(f"–£–¥–∞–ª–µ–Ω–æ {dropped_rows} —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏")
        
        logging.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
        save_logs_to_file(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
        
        return data
        
    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}"
        logging.error(error_msg)
        save_logs_to_file(error_msg)
        raise

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å Binance
def get_historical_data(symbols, bullish_periods, interval="1m", save_path="/workspace/data/binance_data_bullish.csv"
):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å Binance (–∞—Ä—Ö–∏–≤) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –æ–¥–∏–Ω CSV-—Ñ–∞–π–ª.

    :param symbols: —Å–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä (–ø—Ä–∏–º–µ—Ä: ['BTCUSDC', 'ETHUSDC'])
    :param bullish_periods: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–µ—Ä–∏–æ–¥–∞–º–∏ (–ø—Ä–∏–º–µ—Ä: [{"start": "2019-01-01", "end": "2019-12-31"}])
    :param interval: –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "1m" - 1 –º–∏–Ω—É—Ç–∞)
    :param save_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'binance_data_bullish.csv')
    """
    base_url_monthly = "https://data.binance.vision/data/spot/monthly/klines"
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    
    all_data = []
    downloaded_files = set()
    download_lock = Lock()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º threading.Lock

    def download_and_process(symbol, period):
        start_date = datetime.strptime(period["start"], "%Y-%m-%d")
        end_date = datetime.strptime(period["end"], "%Y-%m-%d")
        temp_data = []

        for current_date in pd.date_range(start=start_date, end=end_date, freq='MS'):  # MS = Monthly Start
            year = current_date.year
            month = current_date.month
            month_str = f"{month:02d}"

            file_name = f"{symbol}-{interval}-{year}-{month_str}.zip"
            file_url = f"{base_url_monthly}/{symbol}/{interval}/{file_name}"

            # –ë–ª–æ–∫–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø –∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å–∫–∞—á–∞–Ω
            with download_lock:
                if file_name in downloaded_files:
                    logging.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {file_name}, —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ

                logging.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞: {file_url}")
                response = requests.head(file_url, timeout=5)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_url}")
                    continue

                logging.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ {file_url}...")
                response = requests.get(file_url, timeout=15)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_url}: –ö–æ–¥ {response.status_code}")
                    continue

                logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω {file_name}")
                downloaded_files.add(file_name)  # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

            try:
                zip_file = zipfile.ZipFile(BytesIO(response.content))
                csv_file = file_name.replace('.zip', '.csv')

                with zip_file.open(csv_file) as file:
                    df = pd.read_csv(file, header=None, names=[
                        "timestamp", "open", "high", "low", "close", "volume",
                        "close_time", "quote_asset_volume", "number_of_trades",
                        "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", "ignore"
                    ], dtype={
                        "timestamp": "int64",
                        "open": "float32",
                        "high": "float32",
                        "low": "float32",
                        "close": "float32",
                        "volume": "float32",
                        "quote_asset_volume": "float32",
                        "number_of_trades": "int32",
                        "taker_buy_base_asset_volume": "float32",
                        "taker_buy_quote_asset_volume": "float32"
                    })

                    # üõ† –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ `timestamp`
                    if "timestamp" not in df.columns:
                        logging.error(f"‚ùå –û—à–∏–±–∫–∞: –ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ df –¥–ª—è {symbol}")
                        return None

                    # üõ† –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime –∏ —Å—Ç–∞–≤–∏–º –∏–Ω–¥–µ–∫—Å
                    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", errors="coerce")
                    df.set_index("timestamp", inplace=True)
                    
                    # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex, –ø—ã—Ç–∞–µ–º—Å—è –µ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å
                    if not isinstance(df.index, pd.DatetimeIndex):
                        try:
                            df.index = pd.to_datetime(df.index, errors="coerce")
                        except Exception as e:
                            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ –≤ DatetimeIndex: {e}")
                                    
                    # –ï—Å–ª–∏ –ø–æ –∫–∞–∫–æ–π-—Ç–æ –ø—Ä–∏—á–∏–Ω–µ —Å—Ç–æ–ª–±—Ü–∞ 'timestamp' –±–æ–ª—å—à–µ –Ω–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
                    if "timestamp" not in df.columns:
                        df["timestamp"] = df.index
                                        
                    df["symbol"] = symbol

                    temp_data.append(df)
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {symbol} –∑–∞ {current_date.strftime('%Y-%m')}: {e}")

            time.sleep(0.5)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è–º–∏

        return pd.concat(temp_data) if temp_data else None

    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(download_and_process, symbol, period) for symbol in symbols for period in bullish_periods]
        for future in futures:
            result = future.result()
            if result is not None:
                all_data.append(result)

    if not all_data:
        logging.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –º–µ—Å—è—Ü–∞ –¥–∞–Ω–Ω—ã—Ö.")
        return None

    df = pd.concat(all_data, ignore_index=False)  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º ignore_index, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å `timestamp`  

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å –≤ DataFrame
    logging.info(f"üìä –ö–æ–ª–æ–Ω–∫–∏ –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º df: {df.columns}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å
    if "timestamp" not in df.columns and not isinstance(df.index, pd.DatetimeIndex):
        logging.error(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {df.columns}")
        return None

    # –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å resample
    df = df.resample('1min').ffill()  # –ú–∏–Ω—É—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã, –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º NaN
    num_nans = df.isna().sum().sum()
    if num_nans > 0:
        nan_percentage = num_nans / len(df)
        if nan_percentage > 0.05:  # –ï—Å–ª–∏ –±–æ–ª–µ–µ 5% –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–ø—É—â–µ–Ω—ã
            logging.warning(f"‚ö† –ü—Ä–æ–ø—É—â–µ–Ω–æ {nan_percentage:.2%} –¥–∞–Ω–Ω—ã—Ö! –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏.")
            df.dropna(inplace=True)
        else:
            logging.info(f"üîß –ó–∞–ø–æ–ª–Ω—è–µ–º {nan_percentage:.2%} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö `ffill`.")
            df.fillna(method='ffill', inplace=True)  # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏

    df.to_csv(save_path, index_label='timestamp')
    logging.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}")

    return save_path


def load_bullish_data(symbols, bullish_periods, interval="1m", save_path="binance_data_bullish.csv"):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø–µ—Ä–∏–æ–¥–æ–≤.
    –ï—Å–ª–∏ —Ñ–∞–π–ª save_path —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è —Å —É–∂–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å, –≥–¥–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è DataFrame —Å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
    –ß—Ç–µ–Ω–∏–µ CSV –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ —á–∞–Ω–∫–∞–º –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ –ø–∞–º—è—Ç—å.
    """
    CHUNK_SIZE = 200000  # —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è CSV

    # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äì —á–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ —á–∞–Ω–∫–∞–º
    if os.path.exists(save_path):
        try:
            chunks = []
            for chunk in pd.read_csv(save_path,
                                     index_col='timestamp',
                                     parse_dates=['timestamp'],
                                     on_bad_lines='skip',
                                     chunksize=CHUNK_SIZE):
                # –°–±—Ä–æ—Å –∏–Ω–¥–µ–∫—Å–∞, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –¥–∞—Ç–∞–º–∏
                chunk = chunk.reset_index(drop=False)
                if 'timestamp' not in chunk.columns:
                    if 'index' in chunk.columns:
                        chunk.rename(columns={'index': 'timestamp'}, inplace=True)
                        logging.info("–°—Ç–æ–ª–±–µ—Ü 'index' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ 'timestamp'.")
                    else:
                        raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏.")
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç–æ–ª–±–µ—Ü 'timestamp' –≤ datetime —Å utc=True
                chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce', utc=True)
                # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏
                chunk = chunk.dropna(subset=['timestamp'])
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 'timestamp' –∫–∞–∫ –∏–Ω–¥–µ–∫—Å
                chunk = chunk.set_index('timestamp')
                chunks.append(chunk)
            existing_data = pd.concat(chunks, ignore_index=False)
            logging.info(f"–°—á–∏—Ç–∞–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {save_path}, —Å—Ç—Ä–æ–∫: {len(existing_data)}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ {save_path}: {e}")
            existing_data = pd.DataFrame()
    else:
        existing_data = pd.DataFrame()

    all_data = {}  # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞–∂–¥–æ–º—É —Å–∏–º–≤–æ–ª—É
    logging.info(f"üöÄ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤: {symbols}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {
            executor.submit(get_historical_data, [symbol], bullish_periods, interval, save_path): symbol
            for symbol in symbols
        }
        for future in futures:
            symbol = futures[future]
            try:
                temp_file_path = future.result()
                if temp_file_path is not None:
                    # –ß–∏—Ç–∞–µ–º —Å–∫–∞—á–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–æ —á–∞–Ω–∫–∞–º
                    chunks = []
                    for chunk in pd.read_csv(temp_file_path,
                                             index_col='timestamp',
                                             parse_dates=['timestamp'],
                                             on_bad_lines='skip',
                                             chunksize=CHUNK_SIZE):
                        chunk = chunk.reset_index(drop=False)
                        if 'timestamp' not in chunk.columns:
                            if 'index' in chunk.columns:
                                chunk.rename(columns={'index': 'timestamp'}, inplace=True)
                                logging.info("–°—Ç–æ–ª–±–µ—Ü 'index' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ 'timestamp' –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")
                            else:
                                raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ –≤ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")
                        chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce', utc=True)
                        chunk = chunk.dropna(subset=['timestamp'])
                        chunk = chunk.set_index('timestamp')
                        chunks.append(chunk)
                    if chunks:
                        new_data = pd.concat(chunks, ignore_index=False)
                    else:
                        new_data = pd.DataFrame()
                    
                    if symbol in all_data:
                        all_data[symbol].append(new_data)
                    else:
                        all_data[symbol] = [new_data]
                    logging.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–ª—è {symbol}. –§–∞–π–ª–æ–≤: {len(all_data[symbol])}")
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    for symbol in list(all_data.keys()):
        if all_data[symbol]:
            all_data[symbol] = pd.concat(all_data[symbol], ignore_index=False)
        else:
            del all_data[symbol]

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ –æ–¥–∏–Ω DataFrame
    if all_data:
        new_combined = pd.concat(all_data.values(), ignore_index=False)
    else:
        new_combined = pd.DataFrame()

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–µ—Å–ª–∏ –∏–º–µ—é—Ç—Å—è)
    if not existing_data.empty:
        combined = pd.concat([existing_data, new_combined], ignore_index=False)
    else:
        combined = new_combined

    # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ ---
    # –°–±—Ä–æ—Å–∏–º –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –¥–∞—Ç–∞–º–∏
    combined = combined.reset_index(drop=False)
    if 'timestamp' not in combined.columns:
        if 'index' in combined.columns:
            combined.rename(columns={'index': 'timestamp'}, inplace=True)
            logging.info("–°—Ç–æ–ª–±–µ—Ü 'index' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ 'timestamp' –ø—Ä–∏ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏.")
        else:
            logging.error("–ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü 'timestamp' –∏–ª–∏ 'index' –≤ –∏—Ç–æ–≥–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
            raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏.")
    combined['timestamp'] = pd.to_datetime(combined['timestamp'], errors='coerce', utc=True)
    combined = combined.dropna(subset=['timestamp'])
    combined = combined.set_index('timestamp')

    if not isinstance(combined.index, pd.DatetimeIndex):
        logging.error(f"–ü–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏—Ç–æ–≥–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –∏–º–µ–µ—Ç —Ç–∏–ø: {type(combined.index)}")
        raise ValueError("–ò–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex.")
    else:
        logging.info("–ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ DatetimeIndex.")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π DataFrame —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏–º–µ–Ω–∏ –∫–æ–ª–æ–Ω–∫–∏ –∏–Ω–¥–µ–∫—Å–∞
    combined.to_csv(save_path, index_label='timestamp')
    logging.info(f"üíæ –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path} (–∏—Ç–æ–≥–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫: {len(combined)})")
    return all_data



'''def aggregate_to_2min(data):
    """
    –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ 1 –º–∏–Ω—É—Ç–∞ –¥–æ 2 –º–∏–Ω—É—Ç.
    """
    logging.info("–ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ 1 –º–∏–Ω—É—Ç–∞ –¥–æ 2 –º–∏–Ω—É—Ç")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å
    if not isinstance(data.index, pd.DatetimeIndex):
         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        if 'timestamp' in data.columns:
            data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ datetime
            data.set_index('timestamp', inplace=True)
        else:
            raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏ –∏–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex.")

        # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∏–Ω–¥–µ–∫—Å —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex
        if not isinstance(data.index, pd.DatetimeIndex):
            raise ValueError("–ò–Ω–¥–µ–∫—Å –¥–∞–Ω–Ω—ã—Ö –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex –¥–∞–∂–µ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.")
        
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    data = data.resample('2T').agg({
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum'
    }).dropna()
    
    logging.info(f"–ê–≥—Ä–µ–≥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(data)} —Å—Ç—Ä–æ–∫")
    logging.info(f"–ü–æ—Å–ª–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ 2 –º–∏–Ω—É—Ç—ã: NaN = {data.isna().sum().sum()}")
    return data'''

def convert_df_dtypes(df):
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º float64 –≤ float32
    float_cols = df.select_dtypes(include=['float64']).columns
    for col in float_cols:
        df[col] = df[col].astype(np.float32)
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º int64 –≤ int32
    int_cols = df.select_dtypes(include=['int64']).columns
    for col in int_cols:
        df[col] = df[col].astype(np.int32)
    return df


# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def extract_features(data):
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –æ—Ç–¥–µ–ª—å–Ω–æ.
    """
    if 'symbol' in data.columns:
        grouped = data.groupby('symbol')
        features = grouped.apply(_extract_features_per_symbol).reset_index(drop=True)
    else:
        features = _extract_features_per_symbol(data)
    
    return features


def _extract_features_per_symbol(data):
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ —Å –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ–º —Å—Ç–æ–ª–±—Ü–æ–≤ –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É,
    –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏ –∏–∑–º–µ–Ω—ë–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.
    –ó–¥–µ—Å—å —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã rolling‚Äë–æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ—Ö–æ–¥–æ–≤.
    """
    data = data.copy()
    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤ –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤
    for col in ['open', 'high', 'low', 'close', 'volume']:
        data[col] = pd.to_numeric(data[col], errors='coerce').astype(np.float32)
    data = data.replace([np.inf, -np.inf], np.nan).ffill().bfill()
    data = convert_df_dtypes(data)

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–∞–∑–æ–≤—É—é –º–µ—Ç–∫—É —Ä—ã–Ω–∫–∞ (bullish ‚Üí 0)
    data['market_type'] = 0

    # 1. –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –∏ –∏–º–ø—É–ª—å—Å–∞
    data['returns'] = data['close'].pct_change()
    data['log_returns'] = np.log(data['close'] / data['close'].shift(1) + 1e-7)
    data['momentum_1m'] = data['close'].diff(1)
    data['momentum_3m'] = data['close'].diff(3)
    data['acceleration'] = data['momentum_1m'].diff()

    # 2. –†–∞—Å—á–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –ø–æ—Ä–æ–≥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º rolling –¥–ª—è returns —Å –æ–∫–Ω–æ–º 20
    roll_returns_20 = data['returns'].rolling(20)
    volatility = roll_returns_20.std()
    volume_volatility = data['volume'].pct_change().rolling(20).std()  # –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è volume
    base_strong = 0.001
    base_medium = 0.0005
    strong_threshold = base_strong * (1 + volatility / (volatility.mean() + 1e-7))
    medium_threshold = base_medium * (1 + volatility / (volatility.mean() + 1e-7))
    volume_factor = 1 + (volume_volatility / (volume_volatility.mean() + 1e-7))

    # 3. –û–±—ä–µ–º–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–∏–ª—ã –¥–≤–∏–∂–µ–Ω–∏—è
    data['volume_delta'] = data['volume'].diff()
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º rolling –¥–ª—è volume.diff() —Å –æ–∫–Ω–æ–º 3
    roll_vol_diff_3 = data['volume'].diff().rolling(3)
    data['volume_momentum'] = roll_vol_diff_3.sum()
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º rolling –¥–ª—è volume —Å –æ–∫–Ω–æ–º 5
    roll_volume_5 = data['volume'].rolling(5)
    volume_mean_5 = roll_volume_5.mean()
    volume_ratio = data['volume'] / (volume_mean_5 * volume_factor + 1e-7)
    data['volume_ratio'] = volume_ratio  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç

    # 4. –ë—ã—Å—Ç—Ä—ã–µ —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è HFT
    data['sma_2'] = SMAIndicator(data['close'], window=2).sma_indicator()
    data['sma_3'] = SMAIndicator(data['close'], window=3).sma_indicator()
    data['sma_10'] = SMAIndicator(data['close'], window=10).sma_indicator()
    data['ema_3'] = data['close'].ewm(span=3, adjust=False).mean()
    data['ema_5'] = data['close'].ewm(span=5, adjust=False).mean()

    # 5. –ú–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥—ã –¥–ª—è HFT
    data['micro_trend'] = np.where(
        data['close'] > data['close'].shift(1), 1,
        np.where(data['close'] < data['close'].shift(1), -1, 0)
    )
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º rolling –¥–ª—è micro_trend —Å –æ–∫–Ω–æ–º 3
    roll_micro_3 = data['micro_trend'].rolling(3)
    data['micro_trend_strength'] = roll_micro_3.sum()

    # 6. –ë—ã—Å—Ç—Ä—ã–π MACD –¥–ª—è 1-–º–∏–Ω—É—Ç–Ω—ã—Ö —Å–≤–µ—á–µ–π
    macd = MACD(data['close'], window_slow=12, window_fast=6, window_sign=3)
    data['macd'] = macd.macd()
    data['macd_signal'] = macd.macd_signal()
    data['macd_diff'] = data['macd'] - data['macd_signal']
    data['macd_acceleration'] = data['macd_diff'].diff()

    # 7. –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä—ã
    data['rsi_5'] = RSIIndicator(data['close'], window=5).rsi()
    data['rsi_3'] = RSIIndicator(data['close'], window=3).rsi()
    stoch = StochasticOscillator(data['high'], data['low'], data['close'], window=5)
    data['stoch_k'] = stoch.stoch()
    data['stoch_d'] = stoch.stoch_signal()

    # 8. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–æ–≤ –∏ –ø–æ–∑–∏—Ü–∏—è –≤ Bollinger Bands
    bb = BollingerBands(data['close'], window=10)
    data['bb_width'] = bb.bollinger_wband()
    data['bb_position'] = (data['close'] - bb.bollinger_lband()) / (bb.bollinger_hband() - bb.bollinger_lband() + 1e-7)
    data['atr_3'] = AverageTrueRange(data['high'], data['low'], data['close'], window=3).average_true_range()

    # 9. –°–≤–µ—á–Ω–æ–π –∞–Ω–∞–ª–∏–∑
    data['candle_body'] = data['close'] - data['open']
    data['body_ratio'] = np.abs(data['candle_body']) / (data['high'] - data['low'] + 1e-7)
    data['upper_wick'] = data['high'] - np.maximum(data['open'], data['close'])
    data['lower_wick'] = np.minimum(data['open'], data['close']) - data['low']

    # 10. –û–±—ä–µ–º–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    data['obv'] = OnBalanceVolumeIndicator(data['close'], data['volume']).on_balance_volume()
    data['volume_trend'] = data['volume'].diff() / (data['volume'].shift(1) + 1e-7)

    # 11. –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–ª—è HFT
    data['price_acceleration'] = data['returns'].diff()
    data['volume_acceleration'] = data['volume_delta'].diff()

    # 12. –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
    future_returns = data['close'].shift(-1) / data['close'] - 1
    data = data.iloc[:-1]
    future_returns = future_returns.iloc[:-1]
    data['target'] = np.where(
        (future_returns > 0.0001) &
        (data['volume'] > data['volume'].rolling(5).mean()) &
        (data['macd_diff'] > 0) &
        (data['bb_position'] < 0.6),
        1,  # BUY
        np.where(
            (future_returns < -0.0001) &
            (data['rsi_5'] > 60) &
            (data['bb_position'] > 0.4),
            2,  # SELL
            0   # HOLD
        )
    )

    return data.replace([np.inf, -np.inf], np.nan).ffill().bfill()



def remove_outliers(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    return data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]


def add_clustering_feature(data):
    features_for_clustering = [
        'close', 'volume', 'rsi', 'macd', 'atr', 'sma_2', 'sma_3', 'sma_10', 'ema_3', 'ema_5',
        'bb_width', 'macd_diff', 'obv', 'returns', 'log_returns'
    ]
    
    max_for_kmeans = 200_000
    if len(data) > max_for_kmeans:
        sample_df = data.sample(n=max_for_kmeans, random_state=42)
    else:
        sample_df = data

    kmeans = KMeans(n_clusters=5, random_state=42)
    kmeans.fit(sample_df[features_for_clustering])
    data['cluster'] = kmeans.predict(data[features_for_clustering])
    return data

def prepare_data(data):
    logging.info("–ù–∞—á–∞–ª–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ
    if data.empty:
        raise ValueError("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã")

    logging.info(f"–ò—Å—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {data.shape}")

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    if not isinstance(data.index, pd.DatetimeIndex):
        if 'timestamp' in data.columns:
            data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')
            data.set_index('timestamp', inplace=True)
        else:
            raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ 'timestamp'.")

    def process_chunk(df_chunk):
        df_chunk = extract_features(df_chunk)
        df_chunk = remove_outliers(df_chunk)
        df_chunk = add_clustering_feature(df_chunk)
        return df_chunk

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–æ —á–∞–Ω–∫–∞–º, –µ—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π
    data = apply_in_chunks(data, process_chunk, chunk_size=100000)
    logging.info(f"–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è): {data.shape}")


    # –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features = [col for col in data.columns if col != 'target']
    logging.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features)}")
    logging.info(f"–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features}")
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ target:\n{data['target'].value_counts()}")

    return data, features


def clean_data(X, y):
    logging.info("–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ NaN")
    mask = np.isfinite(X).all(axis=1)
    X_clean = X[mask]
    y_clean = y[mask]
    return X_clean, y_clean


def load_last_saved_model(model_filename):
    try:
        models = sorted(
            [f for f in os.listdir() if f.startswith(model_filename)],
            key=lambda x: int(x.split('_')[-1].split('.')[0])
        )
        last_model = models[-1] if models else None
        if last_model:
            logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –º–æ–¥–µ–ª–∏: {last_model}")
            return load_model(last_model)
        else:
            return None
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –º–æ–¥–µ–ª–∏: {e}")
        return None



def balance_classes(X, y, max_for_smote=300_000):
    """
    –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π numpy –∏ pandas.

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    X: numpy.ndarray –∏–ª–∏ pandas.DataFrame ‚Äî –ø—Ä–∏–∑–Ω–∞–∫–∏
    y: numpy.ndarray –∏–ª–∏ pandas.Series ‚Äî –º–µ—Ç–∫–∏
    max_for_smote: int ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è SMOTETomek

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    X_resampled, y_resampled
    """
    logging.info("–ù–∞—á–∞–ª–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤")
    logging.info(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: X={getattr(X, 'shape', None)}, y={getattr(y, 'shape', None)}")
    logging.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã –≤ y: {np.unique(y, return_counts=True)}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏ –¥–µ–ª–∞–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if isinstance(X, pd.DataFrame):
        # –î–ª—è pandas DataFrame
        if len(X) > max_for_smote:
            X_sample = X.sample(n=max_for_smote, random_state=42)
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º pandas.Series –∏–ª–∏ numpy.ndarray –¥–ª—è y
            if isinstance(y, pd.Series):
                y_sample = y.loc[X_sample.index]
            else:
                # y ‚Äî numpy.ndarray, –∏—Å–ø–æ–ª—å–∑—É–µ–º iloc –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º DataFrame
                y_sample = y[X_sample.index.to_numpy()]
        else:
            X_sample = X
            y_sample = y
    else:
        # –î–ª—è numpy.ndarray –∏–ª–∏ –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤
        n_samples = X.shape[0]
        if n_samples > max_for_smote:
            idx = np.random.choice(n_samples, size=max_for_smote, replace=False)
            X_sample = X[idx]
            y_sample = y[idx]
        else:
            X_sample = X
            y_sample = y

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
    if len(X_sample) == 0 or len(y_sample) == 0:
        raise ValueError("–î–∞–Ω–Ω—ã–µ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –ø—É—Å—Ç—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")

    # –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTETomek
    smote_tomek = SMOTETomek(random_state=42)
    X_resampled, y_resampled = smote_tomek.fit_resample(X_sample, y_sample)

    logging.info(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: X={getattr(X_resampled, 'shape', None)}, y={getattr(y_resampled, 'shape', None)}")
    return X_resampled, y_resampled



def ensemble_predict(nn_model, xgb_model, feature_extractor, X_seq, weight_nn=0.5, weight_xgb=0.5):
    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –æ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ ‚Äî –æ–∂–∏–¥–∞–µ–º–∞—è —Ñ–æ—Ä–º–∞ (n_samples, 3)
    nn_probs = nn_model.predict(X_seq)
    logging.info(f"nn_probs.shape = {nn_probs.shape}")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è XGBoost
    embeddings = feature_extractor.predict(X_seq)
    
    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –æ—Ç XGBoost (—Ñ–æ—Ä–º–∞ (n_samples, 3))
    xgb_probs = xgb_model.predict_proba(embeddings)
    logging.info(f"xgb_probs.shape = {xgb_probs.shape}")
    
    # –í–∑–≤–µ—à–∏–≤–∞–µ–º –∏ —Å—É–º–º–∏—Ä—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    final_probs = weight_nn * nn_probs + weight_xgb * xgb_probs
    
    # –í—ã–±–∏—Ä–∞–µ–º –∫–ª–∞—Å—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
    final_pred_class = np.argmax(final_probs, axis=1)
    return final_pred_class



# –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
def build_bullish_neural_network(data):
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    os.makedirs("checkpoints", exist_ok=True)
    
    if os.path.exists("bullish_neural_network.h5"):
        try:
            model = load_model("bullish_neural_network.h5", custom_objects={"custom_profit_loss": custom_profit_loss})
            logging.info("–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ 'bullish_neural_network.h5'. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ.")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–æ–¥–µ–ª—å –≤ –≤–∏–¥–µ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è (–±–µ–∑ XGBoost –∏ feature_extractor, —Ç–∞–∫ –∫–∞–∫ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ)
            return {"ensemble_model": {"nn_model": model}, "scaler": None}
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            logging.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.")
    else:
        logging.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.")
    
    logging.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –±—ã—á—å–µ–≥–æ —Ä—ã–Ω–∫–∞")
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä 'target', 'timestamp', 'symbol'
    features = [col for col in data.columns if col not in ['target', 'timestamp', 'symbol']]
    # –ü—Ä–∏–≤–æ–¥–∏–º –¥–∞–Ω–Ω—ã–µ –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É —Ç–∞–º, –≥–¥–µ —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ
    X = data[features].apply(pd.to_numeric, errors='coerce')
    y = data['target'].values
    
    # –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ü–µ–ª–∏–∫–æ–º —Å–æ—Å—Ç–æ—è—Ç –∏–∑ NaN
    X = X.loc[:, ~X.isna().all()]

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN –º–µ–¥–∏–∞–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    imputer = SimpleImputer(strategy='median')
    X = imputer.fit_transform(X)
    
    # –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –≤—ã—á–∏—Å–ª—è–µ–º SelectKBest –∏ –ª–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    check_feature_quality(X, y)
    
    # 7) –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ DataFrame –¥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
    X_balanced, y_balanced = balance_classes(X, y)

    # 8) –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —É–∂–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    X_scaled = scaler.fit_transform(X_balanced)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_balanced, test_size=0.2, random_state=42)
    
    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
    def create_sequences(X, y, timesteps=10):
        Xs, ys = [], []
        for i in range(len(X) - timesteps):
            Xs.append(X[i:(i + timesteps)])
            ys.append(y[i + timesteps])
        return np.array(Xs), np.array(ys)
    
    timesteps = 10  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤
    time_weights = np.exp(np.linspace(-1, 0, timesteps))  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
    
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º
    X_train_seq, y_train_seq = create_sequences(X_train, y_train, timesteps)
    X_val_seq, y_val_seq = create_sequences(X_val, y_val, timesteps)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º —à–∞–≥–∞–º
    X_train_seq_weighted = X_train_seq * time_weights.reshape(1, -1, 1)
    X_val_seq_weighted = X_val_seq * time_weights.reshape(1, -1, 1)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ tf.data.Dataset
    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_seq_weighted, y_train_seq))
    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.AUTOTUNE)
    
    val_dataset = tf.data.Dataset.from_tensor_slices((X_val_seq_weighted, y_val_seq))
    val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)
    
    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (GPU)
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # –í–∫–ª—é—á–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ GPU
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            strategy = tf.distribute.MirroredStrategy()  # –†–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU
            logging.info("GPU –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º MirroredStrategy")
        except RuntimeError as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU: {e}")
            strategy = tf.distribute.get_strategy()
    else:
        strategy = tf.distribute.get_strategy()
        logging.info("GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    
    logging.info("–ù–∞—á–∏–Ω–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    with strategy.scope():
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        inputs = Input(shape=(timesteps, X_train_seq.shape[2]))
    
        x = LSTM(256, return_sequences=True)(inputs)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
    
        x = LSTM(128, return_sequences=True)(x)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
    
        x = LSTM(64, return_sequences=False)(x)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
    
        x = Dense(128, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
    
        x = Dense(64, activation='relu', name="embedding_layer")(x)
        x = BatchNormalization()(x)

    
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—ã—Ö–æ–¥ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ Dense —Å–ª–æ—è –∫–∞–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–¥–ª—è –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è)
        embedding_layer_output = x  # –ù–ï –ú–ï–ù–Ø–ï–ú –ò–ú–Ø
    
        # –í–º–µ—Å—Ç–æ 'sigmoid' -> 'tanh', —á—Ç–æ–±—ã y_pred ‚àà [-1,+1]
        outputs = Dense(3, activation='softmax')(x)
    
        model = tf.keras.models.Model(inputs, outputs)
        
        logging.info(f"–°–ø–∏—Å–æ–∫ —Å–ª–æ—ë–≤ –º–æ–¥–µ–ª–∏: {[layer.name for layer in model.layers]}")
    
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Å –Ω–∞—à–µ–π –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ—Ç–µ—Ä—å
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss=custom_profit_loss,  # –í—ã–∑–æ–≤ –∫–∞—Å—Ç–æ–º–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
            metrics=[]
        )
        
            
        def bull_profit_metric(y_true, y_pred):
            # –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É–ø—É—â–µ–Ω–Ω–æ–π –ø—Ä–∏–±—ã–ª–∏
            missed_gains = tf.where(y_true > y_pred, y_true - y_pred, 0)
            return tf.reduce_mean(missed_gains)
    
        model.compile(optimizer=Adam(learning_rate=0.001), loss=custom_profit_loss, metrics=[bull_profit_metric])
    
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Ç–æ—á–∫–∏
        try:
            model.load_weights(checkpoint_path_regular.format(epoch=0))  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –ø—É—Ç—å
            logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {checkpoint_path_regular.format(epoch=0)}")
        except FileNotFoundError:
            logging.info("–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è.")
    
    logging.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤...")
    regular_checkpoints = sorted(glob.glob(f"checkpoints/{network_name}_checkpoint_epoch_*.h5"))
    if regular_checkpoints:
        latest_checkpoint = regular_checkpoints[-1]
        try:
            model.load_weights(latest_checkpoint)
            logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –≤–µ—Å–∞ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {latest_checkpoint}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")
    
    if os.path.exists(checkpoint_path_best):
        try:
            model.load_weights(checkpoint_path_best)
            logging.info(f"–õ—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –Ω–∞–π–¥–µ–Ω: {checkpoint_path_best}. –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –±—É–¥—É—Ç —É–¥–∞–ª–µ–Ω—ã.")
        except:
            logging.info("–õ—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –ø–æ–∫–∞ –Ω–µ —Å–æ–∑–¥–∞–Ω. –≠—Ç–æ –æ–∂–∏–¥–∞–µ–º–æ, –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –µ—â—ë –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–ª–ª–±–µ–∫–æ–≤
    checkpoint_every_epoch = ModelCheckpoint(
        filepath=checkpoint_path_regular,
        save_weights_only=True,
        save_best_only=False,
        verbose=1
    )
    
    checkpoint_best_model = ModelCheckpoint(
        filepath=checkpoint_path_best,
        save_weights_only=True,
        save_best_only=True,
        monitor='val_loss',
        verbose=1
    )
    
    tensorboard_callback = TensorBoard(log_dir=f"logs/{time.time()}")
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)
    early_stopping = EarlyStopping(
        monitor='val_bull_profit_metric',  # –ú–æ–Ω–∏—Ç–æ—Ä–∏–º —É–ø—É—â–µ–Ω–Ω—É—é –ø—Ä–∏–±—ã–ª—å
        patience=5,                       # –ú–µ–Ω—å—à–µ —Ç–µ—Ä–ø–µ–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–µ–∞–∫—Ü–∏–∏
        restore_best_weights=True
    )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    class_weights = {
       0: 1.0,  # –í–µ—Å –¥–ª—è –±—ã—á—å–µ–≥–æ —Ä—ã–Ω–∫–∞
       1: 3.0,  # –í–µ—Å –¥–ª—è –º–µ–¥–≤–µ–∂—å–µ–≥–æ —Ä—ã–Ω–∫–∞
       2: 2.5,  # –í–µ—Å –¥–ª—è —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞
    }
    
    history = model.fit(
        train_dataset,
        epochs=200,  # 200
        validation_data=val_dataset,
        class_weight=class_weights,
        callbacks=[
            early_stopping,
            checkpoint_every_epoch,
            checkpoint_best_model,
            tensorboard_callback,
            reduce_lr
        ]
    )
    
    logging.info("–û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤...")
    for checkpoint in glob.glob(f"checkpoints/{network_name}_checkpoint_epoch_*.h5"):
        if checkpoint != checkpoint_path_best:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            os.remove(checkpoint)
            logging.info(f"–£–¥–∞–ª—ë–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {checkpoint}")
    logging.info("–û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å.")
    
    
    try:
        model_save_path = os.path.join("/workspace/saved_models", "bullish_neural_network.h5")
        os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
        model.save(model_save_path)
        logging.info(f"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{model_save_path}'")
        output_dir = os.path.join("/workspace/output", "bullish_neural_network")
        copy_output("Neural_Bullish", output_dir)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")

    
    # –≠—Ç–∞–ø –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –æ–±—É—á–µ–Ω–∏–µ XGBoost
    logging.info("–≠—Ç–∞–ø –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –æ–±—É—á–µ–Ω–∏–µ XGBoost.")
    try:
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å-—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è —Å–ª–æ–π —Å –∏–º–µ–Ω–µ–º 'embedding_layer'
        feature_extractor = Model(inputs=model.input, outputs=model.get_layer("embedding_layer").output)
        embeddings_train = feature_extractor.predict(X_train_seq_weighted)
        embeddings_val = feature_extractor.predict(X_val_seq_weighted)
        logging.info(f"–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ–ª—É—á–µ–Ω—ã: embeddings_train.shape = {embeddings_train.shape}")
        
        # –û–±—É—á–∞–µ–º XGBoost –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö
        xgb_model = train_xgboost_on_embeddings(embeddings_train, y_train_seq)
        logging.info("XGBoost –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö.")
        
        # –ù–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∏ XGBoost —Ä–∞–≤–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        nn_val_pred = model.predict(X_val_seq_weighted)  # (n_samples, 3)
        embeddings_val = feature_extractor.predict(X_val_seq_weighted)
        xgb_val_pred = xgb_model.predict_proba(embeddings_val)  # (n_samples, 3)
        ensemble_val_pred = 0.5 * nn_val_pred + 0.5 * xgb_val_pred
        ensemble_val_pred_class = np.argmax(ensemble_val_pred, axis=1)
        ensemble_f1 = f1_score(y_val_seq, ensemble_val_pred_class, average='weighted')
        logging.info(f"–≠—Ç–∞–ø –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è: F1-score –∞–Ω—Å–∞–º–±–ª—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ = {ensemble_f1:.4f}")

        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—É—é XGBoost –º–æ–¥–µ–ª—å –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        xgb_save_path = os.path.join("/workspace/saved_models", "xgb_model_bullish.pkl")
        joblib.dump(xgb_model, xgb_save_path)
        logging.info(f"XGBoost-–º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{xgb_save_path}'")

        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –∞–Ω—Å–∞–º–±–ª—å –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è
        ensemble_model = {
            "nn_model": model,
            "xgb_model": xgb_model,
            "feature_extractor": feature_extractor,
            "ensemble_weight_nn": 0.5,
            "ensemble_weight_xgb": 0.5
        }
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        ensemble_model = {"nn_model": model}
        feature_extractor = None
    
    return {"ensemble_model": ensemble_model, "scaler": scaler}



# –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è
if __name__ == "__main__":
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (TPU –∏–ª–∏ CPU/GPU)
        strategy = initialize_strategy()

        symbols = ['BTCUSDC', 'ETHUSDC', 'BNBUSDC','XRPUSDC', 'ADAUSDC', 'SOLUSDC', 'DOTUSDC', 'LINKUSDC', 'TONUSDC', 'NEARUSDC']
        
        bullish_periods = [
            {"start": "2017-06-01", "end": "2017-08-31"},
            {"start": "2017-11-01", "end": "2018-01-16"},
            {"start": "2020-11-01", "end": "2021-01-31"},
            {"start": "2021-03-01", "end": "2021-04-30"},
            {"start": "2021-08-15", "end": "2021-10-20"},
            {"start": "2023-02-01", "end": "2023-03-31"},
            {"start": "2023-03-15", "end": "2023-05-15"},
            {"start": "2024-04-01", "end": "2024-06-30"}
        ]



        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—ã—á—å–∏—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
        logging.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—ã—á—å–∏—Ö –ø–µ—Ä–∏–æ–¥–æ–≤")
        data = load_bullish_data(symbols, bullish_periods, interval="1m")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ data ‚Äî —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å, –∏ –æ–Ω –Ω–µ –ø—É—Å—Ç
        if not isinstance(data, dict) or not data:
            raise ValueError("–û—à–∏–±–∫–∞: load_bullish_data() –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å!")

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ DataFrame –∏–∑ —Å–ª–æ–≤–∞—Ä—è `data` –≤ –æ–¥–∏–Ω –æ–±—â–∏–π DataFrame
        data = pd.concat(data.values(), ignore_index=False)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ 'timestamp' –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö
        if 'timestamp' not in data.columns:
            logging.warning("'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–¥–µ–∫—Å.")
            if isinstance(data.index, pd.DatetimeIndex):
                data['timestamp'] = data.index
                logging.info("–ò–Ω–¥–µ–∫—Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –∫–æ–ª–æ–Ω–∫—É 'timestamp'.")
            else:
                raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏ –∏–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex.")


        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–ª–æ–Ω–∫–∏ `timestamp`
        if 'timestamp' not in data.columns and not isinstance(data.index, pd.DatetimeIndex):
            raise ValueError("–ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –∫–æ–ª–æ–Ω–∫—É 'timestamp'. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —ç—Ç–∞–ø –∑–∞–≥—Ä—É–∑–∫–∏.")

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        logging.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö")
        data = extract_features(data)

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        logging.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –æ—á–∏—Å—Ç–∫–æ–π:\n{data.isna().sum()}")
        data.dropna(inplace=True)
        data.reset_index(drop=True, inplace=True)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
        if data.empty:
            logging.error("–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")
            raise ValueError("–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.")

        # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø—É—Ç–∏ –∫ —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã
        checkpoint_path_regular = os.path.join("/workspace/checkpoints", f"{network_name}_checkpoint_epoch_{{epoch:02d}}.h5")
        checkpoint_path_best = os.path.join("/workspace/checkpoints", f"{network_name}_best_model.h5")


        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        logging.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –±—ã—á—å–µ–≥–æ —Ä—ã–Ω–∫–∞")
        build_bullish_neural_network(data)

    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã: {e}")
    finally:
        # –û—á–∏—Å—Ç–∫–∞ —Å–µ—Å—Å–∏–∏ TensorFlow
        logging.info("–û—á–∏—Å—Ç–∫–∞ —Å–µ—Å—Å–∏–∏ TensorFlow...")
        clear_session()  # –ó–∞–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ –≥—Ä–∞—Ñ—ã –∏ —Ñ–æ–Ω–æ–≤—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã TensorFlow


        logging.info("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

