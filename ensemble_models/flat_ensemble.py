import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, log_loss
from imblearn.combine import SMOTETomek
from joblib import Parallel, delayed
from binance.client import Client
from datetime import datetime, timedelta
import logging
import os
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.cluster import KMeans
from concurrent.futures import ThreadPoolExecutor
import joblib
from ta.trend import SMAIndicator, MACD, IchimokuIndicator
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.volatility import BollingerBands, AverageTrueRange
from ta.volume import OnBalanceVolumeIndicator
import sys
from filterpy.kalman import KalmanFilter
from sklearn.metrics import f1_score, precision_score, recall_score
import shutil
import requests
import zipfile
from io import BytesIO
import tensorflow as tf
from threading import Lock
import time
from sklearn.model_selection import GridSearchCV
from utils_output import ensure_directory, copy_output, save_model_output
from sklearn.linear_model import LogisticRegression



# –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
required_dirs = [
    "/workspace/logs",
    "/workspace/saved_models/flat",
    "/workspace/checkpoints/flat",
    "/workspace/data",
    "/workspace/output/flat_ensemble"
]

for directory in required_dirs:
    os.makedirs(directory, exist_ok=True)

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(os.path.join("/workspace/logs", "debug_log_flat_ensemble.log"), encoding="utf-8"),
        logging.StreamHandler(sys.stdout)  # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —é–Ω–∏–∫–æ–¥–∞
    ]
)



# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —á–∞–Ω–∫–æ–≤–∞–Ω–∏—è DataFrame
def apply_in_chunks(df, func, chunk_size=100000):
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é func –∫ DataFrame df –ø–æ —á–∞–Ω–∫–∞–º –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.
    –ï—Å–ª–∏ df –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DataFrame, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç func(df).
    """
    import pandas as pd
    if not isinstance(df, pd.DataFrame):
        return func(df)
    # –ï—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –º–µ–Ω—å—à–µ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞, —Å—Ä–∞–∑—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    if len(df) <= chunk_size:
        return func(df)
    chunks = [df.iloc[i:i+chunk_size] for i in range(0, len(df), chunk_size)]
    processed_chunks = [func(chunk) for chunk in chunks]
    return pd.concat(processed_chunks)


# –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
market_type = "flat"

ensemble_model_filename = os.path.join("/workspace/saved_models/flat", "flat_stacked_ensemble_model.pkl")


checkpoint_base_dir = os.path.join("/workspace/checkpoints/flat", market_type)


ensemble_checkpoint_path = os.path.join(checkpoint_base_dir, f"{market_type}_ensemble_checkpoint.pkl")

def initialize_strategy():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è GPU, –µ—Å–ª–∏ –æ–Ω–∏ –¥–æ—Å—Ç—É–ø–Ω—ã.
    –ï—Å–ª–∏ GPU –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é (CPU –∏–ª–∏ –æ–¥–∏–Ω GPU, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å).
    """
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –≤–∫–ª—é—á–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏,
            # —á—Ç–æ–±—ã TensorFlow –Ω–µ –∑–∞–Ω–∏–º–∞–ª –≤—Å—é –ø–∞–º—è—Ç—å —Å—Ä–∞–∑—É
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            strategy = tf.distribute.MirroredStrategy()  # –†–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö GPU
            print("Running on GPU(s) with strategy:", strategy)
        except RuntimeError as e:
            print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GPU-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:", e)
            strategy = tf.distribute.get_strategy()
    else:
        print("GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.")
        strategy = tf.distribute.get_strategy()
    return strategy


def ensure_directory(path):
    """–°–æ–∑–¥–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
    if not os.path.exists(path):
        os.makedirs(path)
        
        
def calculate_cross_coin_features(data_dict):
    btc_data = data_dict['BTCUSDC']
    for symbol, df in data_dict.items():
        # CHANGED FOR SCALPING
        df['btc_corr'] = df['close'].rolling(15).corr(btc_data['close'])
        df['rel_strength_btc'] = (df['close'].pct_change() - btc_data['close'].pct_change())
        # CHANGED FOR SCALPING
        df['beta_btc'] = (
            df['close'].pct_change().rolling(15).cov(btc_data['close'].pct_change())
            / btc_data['close'].pct_change().rolling(15).var()
        )
        # CHANGED FOR SCALPING
        df['lead_lag_btc'] = df['close'].pct_change().shift(1).rolling(5).corr(btc_data['close'].pct_change())
        data_dict[symbol] = df
    return data_dict


def detect_anomalies(data):
    """
    –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ —Å–≤–µ—á–∏.
    –î–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ –Ω–∞ –∫–æ–ª–µ–±–∞–Ω–∏—è—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–∫–Ω–æ (10 —Å–≤–µ—á–µ–π) –∏ —Å–Ω–∏–∂–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è.
    """
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º z-score –¥–ª—è –æ–±—ä—ë–º–∞ –∏ —Ü–µ–Ω—ã –ø–æ –æ–∫–Ω—É –∏–∑ 10 —Å–≤–µ—á–µ–π
    data['volume_zscore'] = ((data['volume'] - data['volume'].rolling(10).mean()) / 
                             data['volume'].rolling(10).std())
    data['price_zscore'] = ((data['close'] - data['close'].rolling(10).mean()) / 
                            data['close'].rolling(10).std())
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ 2.5 –≤–º–µ—Å—Ç–æ 3 –¥–ª—è –±–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤
    data['is_anomaly'] = ((abs(data['volume_zscore']) > 2.5) & (data['close'] < data['close'].shift(1))) | \
                         (abs(data['price_zscore']) > 2.5)
    return data



def validate_volume_confirmation(data):
    # CHANGED FOR SCALPING
    data['volume_trend_conf'] = np.where(
        (data['close'] > data['close'].shift(1)) &
        (data['volume'] > data['volume'].rolling(5).mean()),
        1,
        np.where(
            (data['close'] < data['close'].shift(1)) &
            (data['volume'] > data['volume'].rolling(5).mean()),
            -1,
            0
        )
    )
    data['volume_strength'] = (data['volume'] / data['volume'].rolling(5).mean()) * data['volume_trend_conf']
    data['volume_accumulation'] = data['volume_trend_conf'].rolling(2).sum()
    return data

def remove_noise(data):
    kf = KalmanFilter(dim_x=2, dim_z=1)
    kf.x = np.array([[data['close'].iloc[0]], [0.]])
    kf.F = np.array([[1., 1.], [0., 1.]])
    kf.H = np.array([[1., 0.]])
    kf.P *= 10
    # CHANGED FOR SCALPING
    kf.R = 2
    kf.Q = np.array([[0.1, 0.1],[0.1, 0.1]])  # –ü–æ–Ω–∏–∂–µ–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–∫–ª–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞
    smoothed_prices = []
    for price in data['close']:
        kf.predict()
        kf.update(price)
        smoothed_prices.append(float(kf.x[0]))
    data['smoothed_close'] = smoothed_prices
    return data



def preprocess_market_data(data_dict):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π.
    """
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    data_dict = calculate_cross_coin_features(data_dict)
    
    for symbol, df in data_dict.items():
        # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –∞–Ω–æ–º–∞–ª–∏–∏
        df = detect_anomalies(df)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ–±—ä–µ–º–æ–º
        df = validate_volume_confirmation(df)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —à—É–º
        df = remove_noise(df)
        
        data_dict[symbol] = df
    
    return data_dict

# GradientBoosting: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
class CheckpointGradientBoosting(GradientBoostingClassifier):
    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, random_state=None, #n_estimators=100
                 subsample=1.0, min_samples_split=2, min_samples_leaf=1):
        super().__init__(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth,
                         random_state=random_state, subsample=subsample, min_samples_split=min_samples_split,
                         min_samples_leaf=min_samples_leaf)
        self.checkpoint_dir = get_checkpoint_path("gradient_boosting", market_type)
        
    def fit(self, X, y):
        logging.info("[GradientBoosting] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        n_features = X.shape[1]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã
        existing_stages = []
        for i in range(self.n_estimators):
            checkpoint_path = os.path.join(self.checkpoint_dir, f"gradient_boosting_checkpoint_{i + 1}.joblib")
            if os.path.exists(checkpoint_path):
                try:
                    stage = joblib.load(checkpoint_path)
                    if stage.n_features_ == n_features:
                        existing_stages.append(stage)
                        logging.info(f"[GradientBoosting] –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏—è {i + 1} –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
                except:
                    logging.warning(f"[GradientBoosting] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç {i + 1}")

        # –ï—Å–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç, –æ—á–∏—â–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        if not existing_stages:
            if os.path.exists(self.checkpoint_dir):
                shutil.rmtree(self.checkpoint_dir)
            ensure_directory(self.checkpoint_dir)
            logging.info("[GradientBoosting] –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")
            super().fit(X, y)
        else:
            self.estimators_ = existing_stages
            remaining_stages = self.n_estimators - len(existing_stages)
            if remaining_stages > 0:
                orig_n_classes = self.n_classes_
                self.n_estimators = remaining_stages
                super().fit(X, y)
                self.n_classes_ = orig_n_classes
                self.estimators_.extend(self.estimators_)
                self.n_estimators = len(self.estimators_)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç—ã
        for i, stage in enumerate(self.estimators_):
            checkpoint_path = os.path.join(self.checkpoint_dir, f"gradient_boosting_checkpoint_{i + 1}.joblib")
            if not os.path.exists(checkpoint_path):
                joblib.dump(stage, checkpoint_path)
                logging.info(f"[GradientBoosting] –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç {i + 1}")

        return self

# XGBoost: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 3 –∏—Ç–µ—Ä–∞—Ü–∏–∏
class CheckpointXGBoost(XGBClassifier):
    def __init__(self, n_estimators=100, max_depth=3, learning_rate=0.1, #n_estimators=100
                 min_child_weight=1, subsample=1.0, colsample_bytree=1.0,
                 random_state=None, objective=None, **kwargs):
        super().__init__(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            min_child_weight=min_child_weight,
            subsample=subsample,
            colsample_bytree=colsample_bytree,
            random_state=random_state,
            objective=objective,
            **kwargs
        )
        self.checkpoint_dir = get_checkpoint_path("xgboost", market_type)

    def fit(self, X, y, **kwargs):
        logging.info("[XGBoost] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏")
        model_path = os.path.join(self.checkpoint_dir, "xgboost_checkpoint")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        final_checkpoint = os.path.join("/workspace/saved_models", "flat_stacked_ensemble_model.pkl")

        if os.path.exists(final_checkpoint):
            try:
                saved_model = joblib.load(final_checkpoint)
                if saved_model.n_features_ == X.shape[1]:
                    logging.info("[XGBoost] –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
                    return saved_model
            except:
                logging.warning("[XGBoost] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç")
        
        # –ï—Å–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, –æ—á–∏—â–∞–µ–º –∏ –Ω–∞—á–∏–Ω–∞–µ–º –∑–∞–Ω–æ–≤–æ
        if os.path.exists(self.checkpoint_dir):
            shutil.rmtree(self.checkpoint_dir)
        ensure_directory(self.checkpoint_dir)
        
        super().fit(X, y)
        joblib.dump(self, final_checkpoint)
        logging.info("[XGBoost] –°–æ—Ö—Ä–∞–Ω–µ–Ω –Ω–æ–≤—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç")
        return self


# LightGBM: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 3 –∏—Ç–µ—Ä–∞—Ü–∏–∏
class CheckpointLightGBM(LGBMClassifier):
    def __init__(self, n_estimators=100, num_leaves=31, learning_rate=0.1, #n_estimators=100
                 min_data_in_leaf=20, max_depth=-1, random_state=None, **kwargs):
        super().__init__(
            n_estimators=n_estimators,
            num_leaves=num_leaves,
            learning_rate=learning_rate,
            min_data_in_leaf=min_data_in_leaf,
            max_depth=max_depth,
            random_state=random_state,
            **kwargs  # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä, objective, num_class –∏ —Ç.–¥.
        )
        self._checkpoint_path = get_checkpoint_path("lightgbm", market_type)


    def fit(self, X, y, **kwargs):
        logging.info("[LightGBM] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏")
        model_path = os.path.join(self._checkpoint_path, "lightgbm_checkpoint")
        final_checkpoint = f"{model_path}_final.joblib"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        if os.path.exists(final_checkpoint):
            try:
                saved_model = joblib.load(final_checkpoint)
                if hasattr(saved_model, '_n_features') and saved_model._n_features == X.shape[1]:
                    logging.info("[LightGBM] –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
                    # –ö–æ–ø–∏—Ä—É–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
                    self.__dict__.update(saved_model.__dict__)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞
                    _ = self.predict(X[:1])
                    return self
            except:
                logging.warning("[LightGBM] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç")
        
        # –ï—Å–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, –æ—á–∏—â–∞–µ–º –∏ –Ω–∞—á–∏–Ω–∞–µ–º –∑–∞–Ω–æ–≤–æ
        if os.path.exists(self._checkpoint_path):
            shutil.rmtree(self._checkpoint_path)
        ensure_directory(self._checkpoint_path)
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        super().fit(X, y, **kwargs)
        self._n_features = X.shape[1]  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        joblib.dump(self, final_checkpoint)
        logging.info("[LightGBM] –°–æ—Ö—Ä–∞–Ω–µ–Ω –Ω–æ–≤—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç")
        return self
    
    
# CatBoost: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 3 –∏—Ç–µ—Ä–∞—Ü–∏–∏
class CheckpointCatBoost(CatBoostClassifier):
    def __init__(self, iterations=1000, depth=6, learning_rate=0.1, #iterations=1000
                 random_state=None, **kwargs):
        # –£–¥–∞–ª—è–µ–º save_snapshot –∏–∑ kwargs –µ—Å–ª–∏ –æ–Ω —Ç–∞–º –µ—Å—Ç—å
        if 'save_snapshot' in kwargs:
            del kwargs['save_snapshot']
            
        super().__init__(
            iterations=iterations, 
            depth=depth, 
            learning_rate=learning_rate, 
            random_state=random_state,
            save_snapshot=False,  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º save_snapshot —Ç–æ–ª—å–∫–æ –∑–¥–µ—Å—å
            **kwargs
        )
        self.checkpoint_dir = get_checkpoint_path("catboost", market_type)

    def fit(self, X, y, **kwargs):
        logging.info("[CatBoost] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏")
        model_path = os.path.join(self.checkpoint_dir, "catboost_checkpoint")
        final_checkpoint = f"{model_path}_final.joblib"
        
        if os.path.exists(final_checkpoint):
            try:
                saved_model = joblib.load(final_checkpoint)
                if hasattr(saved_model, 'feature_count_') and saved_model.feature_count_ == X.shape[1]:
                    logging.info("[CatBoost] –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
                    return saved_model
            except:
                logging.warning("[CatBoost] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç")
        
        if os.path.exists(self.checkpoint_dir):
            shutil.rmtree(self.checkpoint_dir)
        ensure_directory(self.checkpoint_dir)
        
        # –£–¥–∞–ª—è–µ–º save_snapshot –∏–∑ kwargs –ø—Ä–∏ –≤—ã–∑–æ–≤–µ fit –µ—Å–ª–∏ –æ–Ω —Ç–∞–º –µ—Å—Ç—å
        if 'save_snapshot' in kwargs:
            del kwargs['save_snapshot']
        
        super().fit(X, y, **kwargs)
        joblib.dump(self, final_checkpoint)
        logging.info("[CatBoost] –°–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç")
        
        return self
    
    
# RandomForest: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
class CheckpointRandomForest(RandomForestClassifier):
    def __init__(self, n_estimators=100, max_depth=None, #n_estimators=100
                 min_samples_split=2, min_samples_leaf=1, random_state=None):
        super().__init__(n_estimators=n_estimators, max_depth=max_depth, 
                         min_samples_split=min_samples_split,
                         min_samples_leaf=min_samples_leaf, random_state=random_state)
        self.checkpoint_dir = get_checkpoint_path("random_forest", market_type)

    def fit(self, X, y):
        logging.info("[RandomForest] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏")
        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)
        n_features = X.shape[1]
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –¥–µ—Ä–µ–≤—å—è
        existing_trees = []
        for i in range(self.n_estimators):
            checkpoint_path = os.path.join(self.checkpoint_dir, f"random_forest_tree_{i + 1}.joblib")
            if os.path.exists(checkpoint_path):
                try:
                    tree = joblib.load(checkpoint_path)
                    if tree.tree_.n_features == n_features:
                        existing_trees.append(tree)
                        logging.info(f"[RandomForest] –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–µ—Ä–µ–≤–æ {i + 1} –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
                except Exception as e:
                    logging.warning(f"[RandomForest] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç {i + 1}, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–æ –Ω–æ–≤–æ–µ –¥–µ—Ä–µ–≤–æ")
        
        if not existing_trees:
            if os.path.exists(self.checkpoint_dir):
                shutil.rmtree(self.checkpoint_dir)
            ensure_directory(self.checkpoint_dir)
            logging.info("[RandomForest] –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")
            super().fit(X, y)
        else:
            self.estimators_ = existing_trees
            remaining_trees = self.n_estimators - len(existing_trees)
            if remaining_trees > 0:
                logging.info(f"[RandomForest] –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ: –æ—Å—Ç–∞–ª–æ—Å—å {remaining_trees} –¥–µ—Ä–µ–≤—å–µ–≤")
                orig_n_classes = self.n_classes_
                self.n_estimators = remaining_trees
                super().fit(X, y)
                self.n_classes_ = orig_n_classes
                self.estimators_.extend(self.estimators_)
                self.n_estimators = len(self.estimators_)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç—ã –¥–ª—è –≤—Å–µ—Ö –¥–µ—Ä–µ–≤—å–µ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –µ—â–µ –Ω–µ—Ç
        for i, estimator in enumerate(self.estimators_):
            checkpoint_path = os.path.join(self.checkpoint_dir, f"random_forest_tree_{i + 1}.joblib")
            if not os.path.exists(checkpoint_path):
                joblib.dump(estimator, checkpoint_path)
                logging.info(f"[RandomForest] –°–æ–∑–¥–∞–Ω —á–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è –Ω–æ–≤–æ–≥–æ –¥–µ—Ä–µ–≤–∞ {i + 1}")
        
        # –Ø–≤–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º n_outputs_, –µ—Å–ª–∏ –æ–Ω –Ω–µ –±—ã–ª —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–æ–±—ã—á–Ω–æ –¥–µ–ª–∞–µ—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –º–µ—Ç–æ–¥ fit)
        if not hasattr(self, 'n_outputs_'):
            self.n_outputs_ = 1 if y.ndim == 1 else y.shape[1]
        
        return self
    
    
def save_ensemble_checkpoint(ensemble_model, checkpoint_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—â–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –∞–Ω—Å–∞–º–±–ª—è."""
    ensure_directory(os.path.dirname(checkpoint_path))
    joblib.dump(ensemble_model, checkpoint_path)
    logging.info(f"[Ensemble] –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç –∞–Ω—Å–∞–º–±–ª—è: {checkpoint_path}")



def load_ensemble_checkpoint(checkpoint_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—â–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –∞–Ω—Å–∞–º–±–ª—è."""
    if os.path.exists(checkpoint_path):
        logging.info(f"[Ensemble] –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –∞–Ω—Å–∞–º–±–ª—è: {checkpoint_path}")
        return joblib.load(checkpoint_path)
    logging.info(f"[Ensemble] –ß–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {checkpoint_path}")
    return None

        
        
def debug_target_presence(data, stage_name):
    """
    –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ target –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    print(f"\n=== –û—Ç–ª–∞–¥–∫–∞ {stage_name} ===")
    print(f"Shape –¥–∞–Ω–Ω—ã—Ö: {data.shape}")
    print(f"–ö–æ–ª–æ–Ω–∫–∏: {data.columns.tolist()}")
    if 'target' in data.columns:
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ target:\n{data['target'].value_counts()}")
        print(f"–ü–µ—Ä–≤—ã–µ 5 –∑–Ω–∞—á–µ–Ω–∏–π target:\n{data['target'].head()}")
    else:
        print("–í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–ª–æ–Ω–∫–∞ 'target' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç!")
    print("=" * 50)


def save_logs_to_file(message):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–≥–∏ –≤ —Ñ–∞–π–ª –≤–Ω—É—Ç—Ä–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ /workspace/logs.
    """
    log_dir = "/workspace/logs"
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, "trading_logs.txt")
    with open(log_file, "a") as f:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        f.write(f"{timestamp} - {message}\n")


# –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç–∏
def load_all_data(symbols, start_date, end_date, interval):
    all_data = []
    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = {executor.submit(get_historical_data, symbol, interval, start_date, end_date): symbol for symbol in symbols}
        for future in futures:
            symbol = futures[future]
            try:
                symbol_data = future.result()
                if symbol_data is not None:
                    symbol_data['symbol'] = symbol
                    all_data.append(symbol_data)
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
                save_logs_to_file(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")

    if not all_data:
        logging.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞")
        save_logs_to_file("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞")
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞")

    data = pd.concat(all_data)
    logging.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
    save_logs_to_file(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
    return data

# –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö

def get_historical_data(symbols, flat_periods, interval="1m", save_path="/workspace/data/binance_data_flat.csv"):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å Binance (–∞—Ä—Ö–∏–≤) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –æ–¥–∏–Ω CSV-—Ñ–∞–π–ª.

    :param symbols: —Å–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä (–ø—Ä–∏–º–µ—Ä: ['BTCUSDC', 'ETHUSDC'])
    :param flat_periods: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–µ—Ä–∏–æ–¥–∞–º–∏ (–ø—Ä–∏–º–µ—Ä: [{"start": "2019-01-01", "end": "2019-12-31"}])
    :param interval: –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "1m" - 1 –º–∏–Ω—É—Ç–∞)
    :param save_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'binance_data_flat.csv')
    """
    base_url_monthly = "https://data.binance.vision/data/spot/monthly/klines"
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    
    all_data = []
    downloaded_files = set()
    download_lock = Lock()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º threading.Lock

    def download_and_process(symbol, period):
        start_date = datetime.strptime(period["start"], "%Y-%m-%d")
        end_date = datetime.strptime(period["end"], "%Y-%m-%d")
        temp_data = []

        for current_date in pd.date_range(start=start_date, end=end_date, freq='MS'):  # MS = Monthly Start
            year = current_date.year
            month = current_date.month
            month_str = f"{month:02d}"

            file_name = f"{symbol}-{interval}-{year}-{month_str}.zip"
            file_url = f"{base_url_monthly}/{symbol}/{interval}/{file_name}"

            # –ë–ª–æ–∫–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø –∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å–∫–∞—á–∞–Ω
            with download_lock:
                if file_name in downloaded_files:
                    logging.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {file_name}, —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.")
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ

                logging.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞: {file_url}")
                response = requests.head(file_url, timeout=5)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_url}")
                    continue

                logging.info(f"üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ {file_url}...")
                response = requests.get(file_url, timeout=15)
                if response.status_code != 200:
                    logging.warning(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_url}: –ö–æ–¥ {response.status_code}")
                    continue

                logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω {file_name}")
                downloaded_files.add(file_name)  # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

            try:
                zip_file = zipfile.ZipFile(BytesIO(response.content))
                csv_file = file_name.replace('.zip', '.csv')

                with zip_file.open(csv_file) as file:
                    df = pd.read_csv(file, header=None, names=[
                        "timestamp", "open", "high", "low", "close", "volume",
                        "close_time", "quote_asset_volume", "number_of_trades",
                        "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", "ignore"
                    ], dtype={
                        "timestamp": "int64",
                        "open": "float32",
                        "high": "float32",
                        "low": "float32",
                        "close": "float32",
                        "volume": "float32",
                        "quote_asset_volume": "float32",
                        "number_of_trades": "int32",
                        "taker_buy_base_asset_volume": "float32",
                        "taker_buy_quote_asset_volume": "float32"
                    })

                    # üõ† –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ timestamp
                    if "timestamp" not in df.columns:
                        logging.error(f"‚ùå –û—à–∏–±–∫–∞: –ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ df –¥–ª—è {symbol}")
                        return None

                    # üõ† –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime –∏ —Å—Ç–∞–≤–∏–º –∏–Ω–¥–µ–∫—Å
                    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", errors="coerce")
                    df.set_index("timestamp", inplace=True)
                    
                    df["symbol"] = symbol

                    temp_data.append(df)
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {symbol} –∑–∞ {current_date.strftime('%Y-%m')}: {e}")

            time.sleep(0.5)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è–º–∏

        return pd.concat(temp_data) if temp_data else None

    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(download_and_process, symbol, period) for symbol in symbols for period in flat_periods]
        for future in futures:
            result = future.result()
            if result is not None:
                all_data.append(result)

    if not all_data:
        logging.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –º–µ—Å—è—Ü–∞ –¥–∞–Ω–Ω—ã—Ö.")
        return None

    df = pd.concat(all_data, ignore_index=False)  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º ignore_index, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å timestamp  

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å –≤ DataFrame
    logging.info(f"üìä –ö–æ–ª–æ–Ω–∫–∏ –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º df: {df.columns}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å
    if "timestamp" not in df.columns and not isinstance(df.index, pd.DatetimeIndex):
        logging.error(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {df.columns}")
        return None

    # –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å resample
    df = df.resample('1min').ffill()  # –ú–∏–Ω—É—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã, –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º NaN
    num_nans = df.isna().sum().sum()
    if num_nans > 0:
        nan_percentage = num_nans / len(df)
        if nan_percentage > 0.05:  # –ï—Å–ª–∏ –±–æ–ª–µ–µ 5% –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–ø—É—â–µ–Ω—ã
            logging.warning(f"‚ö† –ü—Ä–æ–ø—É—â–µ–Ω–æ {nan_percentage:.2%} –¥–∞–Ω–Ω—ã—Ö! –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏.")
            df.dropna(inplace=True)
        else:
            logging.info(f"üîß –ó–∞–ø–æ–ª–Ω—è–µ–º {nan_percentage:.2%} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ffill.")
            df.fillna(method='ffill', inplace=True)  # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏

    df.to_csv(save_path)
    logging.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}")

    return save_path


def load_flat_data(symbols, flat_periods, interval="1m", save_path="binance_data_flat.csv"):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–¥–∞–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø–µ—Ä–∏–æ–¥–æ–≤.
    –ï—Å–ª–∏ —Ñ–∞–π–ª save_path —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è —Å —É–∂–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å, –≥–¥–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è DataFrame —Å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
    –ß—Ç–µ–Ω–∏–µ CSV –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ —á–∞–Ω–∫–∞–º –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ –ø–∞–º—è—Ç—å.
    """
    CHUNK_SIZE = 200000  # —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è CSV

    # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äì —á–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–æ —á–∞–Ω–∫–∞–º
    if os.path.exists(save_path):
        try:
            chunks = []
            for chunk in pd.read_csv(save_path,
                                     index_col='timestamp',
                                     parse_dates=['timestamp'],
                                     on_bad_lines='skip',
                                     chunksize=CHUNK_SIZE):
                # –°–±—Ä–æ—Å –∏–Ω–¥–µ–∫—Å–∞, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –¥–∞—Ç–∞–º–∏
                chunk = chunk.reset_index(drop=False)
                if 'timestamp' not in chunk.columns:
                    if 'index' in chunk.columns:
                        chunk.rename(columns={'index': 'timestamp'}, inplace=True)
                        logging.info("–°—Ç–æ–ª–±–µ—Ü 'index' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ 'timestamp'.")
                    else:
                        raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏.")
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç–æ–ª–±–µ—Ü 'timestamp' –≤ datetime —Å utc=True
                chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce', utc=True)
                # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏
                chunk = chunk.dropna(subset=['timestamp'])
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 'timestamp' –∫–∞–∫ –∏–Ω–¥–µ–∫—Å
                chunk = chunk.set_index('timestamp')
                chunks.append(chunk)
            existing_data = pd.concat(chunks, ignore_index=False)
            logging.info(f"–°—á–∏—Ç–∞–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {save_path}, —Å—Ç—Ä–æ–∫: {len(existing_data)}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ {save_path}: {e}")
            existing_data = pd.DataFrame()
    else:
        existing_data = pd.DataFrame()

    all_data = {}  # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞–∂–¥–æ–º—É —Å–∏–º–≤–æ–ª—É
    logging.info(f"üöÄ –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–æ–≤: {symbols}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {
            executor.submit(get_historical_data, [symbol], flat_periods, interval, save_path): symbol
            for symbol in symbols
        }
        for future in futures:
            symbol = futures[future]
            try:
                temp_file_path = future.result()
                if temp_file_path is not None:
                    # –ß–∏—Ç–∞–µ–º —Å–∫–∞—á–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–æ —á–∞–Ω–∫–∞–º
                    chunks = []
                    for chunk in pd.read_csv(temp_file_path,
                                             index_col='timestamp',
                                             parse_dates=['timestamp'],
                                             on_bad_lines='skip',
                                             chunksize=CHUNK_SIZE):
                        chunk = chunk.reset_index(drop=False)
                        if 'timestamp' not in chunk.columns:
                            if 'index' in chunk.columns:
                                chunk.rename(columns={'index': 'timestamp'}, inplace=True)
                                logging.info("–°—Ç–æ–ª–±–µ—Ü 'index' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ 'timestamp' –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")
                            else:
                                raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ –≤ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")
                        chunk['timestamp'] = pd.to_datetime(chunk['timestamp'], errors='coerce', utc=True)
                        chunk = chunk.dropna(subset=['timestamp'])
                        chunk = chunk.set_index('timestamp')
                        chunks.append(chunk)
                    if chunks:
                        new_data = pd.concat(chunks, ignore_index=False)
                    else:
                        new_data = pd.DataFrame()
                    
                    if symbol in all_data:
                        all_data[symbol].append(new_data)
                    else:
                        all_data[symbol] = [new_data]
                    logging.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–ª—è {symbol}. –§–∞–π–ª–æ–≤: {len(all_data[symbol])}")
            except Exception as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
    for symbol in list(all_data.keys()):
        if all_data[symbol]:
            all_data[symbol] = pd.concat(all_data[symbol], ignore_index=False)
        else:
            del all_data[symbol]

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ –æ–¥–∏–Ω DataFrame
    if all_data:
        new_combined = pd.concat(all_data.values(), ignore_index=False)
    else:
        new_combined = pd.DataFrame()

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–µ—Å–ª–∏ –∏–º–µ—é—Ç—Å—è)
    if not existing_data.empty:
        combined = pd.concat([existing_data, new_combined], ignore_index=False)
    else:
        combined = new_combined

    # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ ---
    # –°–±—Ä–æ—Å–∏–º –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å –¥–∞—Ç–∞–º–∏
    combined = combined.reset_index(drop=False)
    if 'timestamp' not in combined.columns:
        if 'index' in combined.columns:
            combined.rename(columns={'index': 'timestamp'}, inplace=True)
            logging.info("–°—Ç–æ–ª–±–µ—Ü 'index' –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω –≤ 'timestamp' –ø—Ä–∏ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏.")
        else:
            logging.error("–ù–µ –Ω–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü 'timestamp' –∏–ª–∏ 'index' –≤ –∏—Ç–æ–≥–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
            raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–æ–ª–±–µ—Ü —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏.")
    combined['timestamp'] = pd.to_datetime(combined['timestamp'], errors='coerce', utc=True)
    combined = combined.dropna(subset=['timestamp'])
    combined = combined.set_index('timestamp')

    if not isinstance(combined.index, pd.DatetimeIndex):
        logging.error(f"–ü–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏—Ç–æ–≥–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –∏–º–µ–µ—Ç —Ç–∏–ø: {type(combined.index)}")
        raise ValueError("–ò–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex.")
    else:
        logging.info("–ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ DatetimeIndex.")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π DataFrame —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏–º–µ–Ω–∏ –∫–æ–ª–æ–Ω–∫–∏ –∏–Ω–¥–µ–∫—Å–∞
    combined.to_csv(save_path, index_label='timestamp')
    logging.info(f"üíæ –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path} (–∏—Ç–æ–≥–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫: {len(combined)})")
    return all_data


'''def aggregate_to_2min(data):
    """
    –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ 1 –º–∏–Ω—É—Ç–∞ –¥–æ 2 –º–∏–Ω—É—Ç.
    """
    logging.info("–ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ 1 –º–∏–Ω—É—Ç–∞ –¥–æ 2 –º–∏–Ω—É—Ç")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å
    if not isinstance(data.index, pd.DatetimeIndex):
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        if 'timestamp' in data.columns:
            data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ datetime
            data.set_index('timestamp', inplace=True)
        else:
            raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏ –∏–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex.")

        # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∏–Ω–¥–µ–∫—Å —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex
        if not isinstance(data.index, pd.DatetimeIndex):
            raise ValueError("–ò–Ω–¥–µ–∫—Å –¥–∞–Ω–Ω—ã—Ö –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex –¥–∞–∂–µ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.")

    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    data = data.resample('2T').agg({
        'open': 'first',
        'high': 'max',
        'low': 'min',
        'close': 'last',
        'volume': 'sum'
    }).dropna()

    logging.info(f"–ê–≥—Ä–µ–≥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(data)} —Å—Ç—Ä–æ–∫")
    logging.info(f"–ü–æ—Å–ª–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ 2 –º–∏–Ω—É—Ç—ã: NaN = {data.isna().sum().sum()}")
    return data'''


def diagnose_nan(data, stage):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –∑–∞–ø–∏—Å—å –≤ –ª–æ–≥."""
    if data.isnull().any().any():
        logging.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–∞ —ç—Ç–∞–ø–µ: {stage}")
        nan_summary = data.isnull().sum()
        logging.warning(f"–°—É–º–º–∞—Ä–Ω–æ NaN:\n{nan_summary}")
    else:
        logging.info(f"–ù–∞ —ç—Ç–∞–ø–µ {stage} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.")
        

def log_class_distribution(y, stage):
    """–ó–∞–ø–∏—Å—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –≤ –ª–æ–≥."""
    if y.empty:
        logging.warning(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –ø—É—Å—Ç–∞ –Ω–∞ —ç—Ç–∞–ø–µ {stage}.")
    else:
        class_distribution = y.value_counts()
        logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ —ç—Ç–∞–ø–µ {stage}:\n{class_distribution}")


# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def extract_features(data):
    logging.info("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞")
    data = data.copy()
    
    # 1. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    data['returns'] = data['close'].pct_change()
    data['log_returns'] = np.log(data['close'] / data['close'].shift(1))

    # –í–æ–∑–º–æ–∂–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –∞–º–ø–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ ‚Äì –µ—Å–ª–∏ –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∏–≥–Ω–∞–ª—ã —Å–ª–∏—à–∫–æ–º –º–∞–ª—ã:
    scaling_factor = 1e4  # –ø—Ä–æ–±—É–π—Ç–µ –º–µ–Ω—è—Ç—å —ç—Ç–æ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
    data['returns_scaled'] = data['returns'] * scaling_factor

    window_size = 1000
    data['returns_mean'] = data['returns_scaled'].rolling(window=window_size, min_periods=window_size).mean()
    data['returns_std'] = data['returns_scaled'].rolling(window=window_size, min_periods=window_size).std()
    data['returns_shift'] = data['returns_scaled'].shift(-1)
    data['z_score'] = (data['returns_shift'] - data['returns_mean']) / data['returns_std']

    # –ü–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥–∏ –¥–ª—è z_score
    upper_z = 0.5  # –∏–ª–∏ –ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ –Ω–∏–∂–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, 0.2
    lower_z = -0.5

    data['target'] = np.where(data['z_score'] > upper_z, 2,
                       np.where(data['z_score'] < lower_z, 1, 0))

    
    # 2. –ú–µ—Ç—Ä–∏–∫–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
    data['range_width'] = data['high'] - data['low']
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º rolling –¥–ª—è range_width —Å –æ–∫–Ω–æ–º 10 –∏ 20
    roll_range_10 = data['range_width'].rolling(10)
    data['range_stability'] = roll_range_10.std()
    roll_range_20 = data['range_width'].rolling(20)
    data['range_mean_20'] = roll_range_20.mean()
    data['range_ratio'] = data['range_width'] / data['range_mean_20']
    data['price_in_range'] = (data['close'] - data['low']) / data['range_width']
    
    # 3. –ë—ã—Å—Ç—Ä—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è HFT
    data['sma_3'] = SMAIndicator(data['close'], window=3).sma_indicator()
    data['ema_5'] = data['close'].ewm(span=5, adjust=False).mean()
    data['ema_8'] = data['close'].ewm(span=8, adjust=False).mean()
    if 'clean_returns' in data.columns:
        data['clean_volatility'] = data['clean_returns'].rolling(20).std()
    
    # 4. –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä—ã
    data['rsi_3'] = RSIIndicator(data['close'], window=3).rsi()
    data['rsi_5'] = RSIIndicator(data['close'], window=5).rsi()
    stoch = StochasticOscillator(data['high'], data['low'], data['close'], window=5)
    data['stoch_k'] = stoch.stoch()
    data['stoch_d'] = stoch.stoch_signal()
    
    # 5. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –º–∞–ª—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
    bb = BollingerBands(data['close'], window=10)
    data['bb_width'] = bb.bollinger_wband()
    data['bb_position'] = (data['close'] - bb.bollinger_lband()) / (bb.bollinger_hband() - bb.bollinger_lband())
    data['atr_5'] = AverageTrueRange(data['high'], data['low'], data['close'], window=5).average_true_range()
    
    # 6. –û–±—ä–µ–º–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ ‚Äì –≥—Ä—É–ø–ø–∏—Ä—É–µ–º rolling –ø–æ volume —Å –æ–∫–Ω–æ–º 10
    roll_volume_10 = data['volume'].rolling(10)
    data['volume_ma'] = roll_volume_10.mean()
    data['volume_std'] = roll_volume_10.std()
    data['volume_ratio'] = data['volume'] / data['volume_ma']
    data['volume_stability'] = data['volume_std'] / data['volume_ma']
    
    # 7. –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –ø—Ä–æ–±–æ—è
    data['breakout_intensity'] = abs(data['close'] - data['close'].shift(1)) / data['range_width']
    data['false_breakout'] = (data['high'] > data['high'].shift(1)) & (data['close'] < data['close'].shift(1))
    
    # 8. –ú–∏–∫—Ä–æ-–ø–∞—Ç—Ç–µ—Ä–Ω—ã
    data['micro_trend'] = np.where(
        data['close'] > data['close'].shift(1), 1,
        np.where(data['close'] < data['close'].shift(1), -1, 0)
    )
    data['micro_trend_change'] = (data['micro_trend'] != data['micro_trend'].shift(1)).astype(int)
    
    # 9. –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞
    volatility = data['returns'].rolling(20).std()
    avg_volatility = volatility.rolling(100).mean()
    
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features = {}
    
    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤—Å–µ, —á—Ç–æ —É–∂–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ)
    for col in data.columns:
        if col not in ['target', 'market_type']:
            features[col] = data[col]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ–∂–º–æ–Ω–µ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if 'btc_corr' in data.columns:
        features['btc_corr'] = data['btc_corr']
    if 'rel_strength_btc' in data.columns:
        features['rel_strength_btc'] = data['rel_strength_btc']
    if 'beta_btc' in data.columns:
        features['beta_btc'] = data['beta_btc']
            
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –æ–±—ä–µ–º–æ–º, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if 'volume_strength' in data.columns:
        features['volume_strength'] = data['volume_strength']
    if 'volume_accumulation' in data.columns:
        features['volume_accumulation'] = data['volume_accumulation']
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –æ—Ç —à—É–º–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if 'clean_returns' in data.columns:
        features['clean_returns'] = data['clean_returns']
        
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ DataFrame
    features_df = pd.DataFrame(features)
    
    return data.replace([np.inf, -np.inf], np.nan).ffill().bfill()



def clean_data(X, y):
    """
    –£–¥–∞–ª—è–µ—Ç —Å—Ç—Ä–æ–∫–∏ —Å NaN, –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç—è–º–∏ –∏ –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏.
    """
    mask = X.notnull().all(axis=1)
    X_clean = X.loc[mask]
    y_clean = y.loc[mask]
    duplicated = X_clean.index.duplicated(keep='first')
    X_clean = X_clean.loc[~duplicated]
    y_clean = y_clean.loc[~duplicated]
    if not X_clean.index.equals(y_clean.index):
        raise ValueError("–ò–Ω–¥–µ–∫—Å—ã X –∏ y –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏")
    return X_clean, y_clean


# –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
def remove_outliers(data):
    # –ò—Å–∫–ª—é—á–∞–µ–º 'target' –∏–∑ —Ä–∞—Å—á–µ—Ç–∞ –≤—ã–±—Ä–æ—Å–æ–≤
    numeric_cols = data.select_dtypes(include=['float64', 'float32', 'int64', 'int32']).columns.difference(['target'])
    Q1 = data[numeric_cols].quantile(0.25)
    Q3 = data[numeric_cols].quantile(0.75)
    IQR = Q3 - Q1
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –º–∞—Å–∫–∞ ‚Äì True, –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –≤—ã–±—Ä–æ—Å–æ–º
    mask = ~((data[numeric_cols] < (Q1 - 1.5 * IQR)) | (data[numeric_cols] > (Q3 + 1.5 * IQR))).any(axis=1)
    # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –∏–º–µ–µ—Ç —Å–∏–≥–Ω–∞–ª (target != 0), –µ—ë –æ—Å—Ç–∞–≤–ª—è–µ–º –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –º–∞—Å–∫–∏
    mask = mask | (data['target'] != 0)
    return data[mask]



def add_clustering_feature(data):
    features_for_clustering = [
        'close', 'volume', 'rsi_3', 'rsi_5', 'atr_5', 'sma_3', 'ema_5', 'ema_8',
        'bb_width', 'returns', 'log_returns'
    ]
    max_for_kmeans = 200_000
    if len(data) > max_for_kmeans:
        sample_df = data.sample(n=max_for_kmeans, random_state=42)
    else:
        sample_df = data
    kmeans = KMeans(n_clusters=5, random_state=42)
    kmeans.fit(sample_df[features_for_clustering])
    data['cluster'] = kmeans.fit_predict(data[features_for_clustering])
    return data

def balance_classes(X, y):
    logging.info("–ù–∞—á–∞–ª–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤")
    logging.info(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: X={X.shape}, y={y.shape}")
    logging.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã –≤ y: {np.unique(y, return_counts=True)}")
    
    max_for_smote = 300_000
    if len(X) > max_for_smote:
        X_sample = X.sample(n=max_for_smote, random_state=42)
        y_sample = y.loc[X_sample.index]
    else:
        X_sample = X
        y_sample = y

    if X.shape[0] == 0 or y.shape[0] == 0:
        raise ValueError("–î–∞–Ω–Ω—ã–µ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –ø—É—Å—Ç—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ñ–∏–ª—å—Ç—Ä—ã.")

    smote_tomek = SMOTETomek(random_state=42)
    X_resampled, y_resampled = smote_tomek.fit_resample(X_sample, y_sample)

    logging.info(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: X={X_resampled.shape}, y={y_resampled.shape}")
    return X_resampled, y_resampled
 
 
# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (–¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞)
def augment_data(X):
    """
    –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–±–µ–∑ target)
    Args:
        X: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ (–±–µ–∑ target)
    Returns:
        DataFrame: –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    """
    logging.info(f"–ù–∞—á–∞–ª–æ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. Shape –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º —Ç–æ–ª—å–∫–æ –∫ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
    noise = np.random.normal(0, 0.01, X.shape)
    augmented_features = X + noise
    
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏ –∫–æ–ª–æ–Ω–∫–∏
    augmented_features = pd.DataFrame(augmented_features, 
                                    columns=X.columns, 
                                    index=X.index)
    
    logging.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. Shape –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {augmented_features.shape}")
    return augmented_features


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è SMOTETomek
def smote_process(X_chunk, y_chunk, chunk_id):
    smote_tomek = SMOTETomek(random_state=42)
    X_resampled, y_resampled = smote_tomek.fit_resample(X_chunk, y_chunk)
    
    if 'target' not in data.columns:
        logging.error("–ö–æ–ª–æ–Ω–∫–∞ 'target' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö.")
        raise KeyError("–ö–æ–ª–æ–Ω–∫–∞ 'target' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.")

    return X_resampled, y_resampled


def parallel_smote(X, y, n_chunks=4):
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–∞–∫ –º–∏–Ω–∏–º—É–º –¥–≤—É—Ö –∫–ª–∞—Å—Å–æ–≤
    unique_classes = y.unique()
    logging.info(f"–ö–ª–∞—Å—Å—ã –≤ y: {unique_classes}")
    if len(unique_classes) < 2:
        raise ValueError(f"–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å SMOTETomek, —Ç–∞–∫ –∫–∞–∫ y —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å: {unique_classes}")

    X_chunks = np.array_split(X, n_chunks)
    y_chunks = np.array_split(y, n_chunks)
    results = Parallel(n_jobs=n_chunks)(
        delayed(smote_process)(X_chunk, y_chunk, idx)
        for idx, (X_chunk, y_chunk) in enumerate(zip(X_chunks, y_chunks))
    )
    X_resampled = np.vstack([res[0] for res in results])
    y_resampled = np.hstack([res[1] for res in results])
    
    if 'target' not in data.columns:
        logging.error("–ö–æ–ª–æ–Ω–∫–∞ 'target' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö.")
        raise KeyError("–ö–æ–ª–æ–Ω–∫–∞ 'target' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.")

    return X_resampled, y_resampled

def ensure_datetime_index(data):
    """
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ DataFrame –∏–º–µ–µ—Ç DatetimeIndex –∏ –∫–æ–ª–æ–Ω–∫—É 'timestamp'.
    –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–Ω–¥–µ–∫—Å —É–∂–µ DatetimeIndex.
    –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex, –ø—ã—Ç–∞–µ—Ç—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –µ–≥–æ –≤ datetime.
    –ï—Å–ª–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å ‚Äì –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç—Å—è ValueError.
    """
    if 'timestamp' in data.columns:
        # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ —É–∂–µ –µ—Å—Ç—å, –ø–æ–ø—Ä–æ–±—É–µ–º –µ—ë –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ datetime –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–∞–∫ –∏–Ω–¥–µ–∫—Å.
        try:
            data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')
            data = data.dropna(subset=['timestamp'])
            data = data.set_index('timestamp')
            logging.info("–ö–æ–ª–æ–Ω–∫–∞ 'timestamp' —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –∫ datetime –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∫–∞–∫ –∏–Ω–¥–µ–∫—Å.")
        except Exception as e:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É 'timestamp' –≤ DatetimeIndex.") from e
    else:
        # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–¥–µ–∫—Å
        if not isinstance(data.index, pd.DatetimeIndex):
            try:
                new_index = pd.to_datetime(data.index, errors='coerce')
                if new_index.isnull().all():
                    raise ValueError("–ò–Ω–¥–µ–∫—Å –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ DatetimeIndex.")
                data.index = new_index
                data['timestamp'] = new_index
                logging.info("–ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ DatetimeIndex –∏ –¥–æ–±–∞–≤–ª–µ–Ω –∫–∞–∫ –∫–æ–ª–æ–Ω–∫–∞ 'timestamp'.")
            except Exception as e:
                raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ 'timestamp'.") from e
    return data


# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
def prepare_data(data):
    logging.info("–ù–∞—á–∞–ª–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞")
    
    if data.empty:
        raise ValueError("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã")
    
    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –∫ DatetimeIndex
    if not isinstance(data.index, pd.DatetimeIndex):
        if 'timestamp' in data.columns:
            data['timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')
            data.set_index('timestamp', inplace=True)
        else:
            raise ValueError("–ù–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ 'timestamp'")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞
    def process_chunk(df_chunk):
        df_chunk = detect_anomalies(df_chunk)
        df_chunk = validate_volume_confirmation(df_chunk)
        df_chunk = remove_noise(df_chunk)
        df_chunk = extract_features(df_chunk)
        df_chunk = remove_outliers(df_chunk)
        df_chunk = add_clustering_feature(df_chunk)
        return df_chunk

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–æ —á–∞–Ω–∫–∞–º
    data = apply_in_chunks(data, process_chunk, chunk_size=100000)
    logging.info(f"–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ —á–∞–Ω–∫–∞–º: {data.shape}")
    
    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∏—Å–∫–ª—é—á–∞—è –Ω–µ–Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    features = [col for col in data.columns if col not in ['target', 'symbol', 'close_time', 'ignore']
                and pd.api.types.is_numeric_dtype(data[col])]
    
    logging.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features)}")
    logging.info(f"–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features}")
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ target:\n{data['target'].value_counts()}")
    
    return data, features



def update_model_if_new_data(ensemble_model, selected_features, model_filename, new_data_available, updated_data):
    """
    –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    """
    if new_data_available:
        logging.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        ensemble_model, selected_features = train_ensemble_model(updated_data, selected_features, model_filename)
        logging.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞.")
    return ensemble_model


def get_checkpoint_path(model_name, market_type):
    """
    –°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏.
    
    Args:
        model_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ('rf', 'xgb', 'lgbm', etc.)
        market_type (str): –¢–∏–ø —Ä—ã–Ω–∫–∞ ('bullish', 'bearish', 'flat')
    
    Returns:
        str: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    """
    checkpoint_path = os.path.join("/workspace/checkpoints", market_type, model_name)
    ensure_directory(checkpoint_path)
    return checkpoint_path

def check_class_balance(y):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤"""
    class_counts = pd.Series(y).value_counts()
    total = len(y)
    
    logging.info("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    for class_label, count in class_counts.items():
        percentage = (count / total) * 100
        logging.info(f"–ö–ª–∞—Å—Å {class_label}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({percentage:.2f}%)")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å
    if class_counts.max() / class_counts.min() > 10:
        logging.warning("–°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ (>10:1)")
        

def check_feature_quality(X, y):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏—Å–ø–µ—Ä—Å–∏—é
    zero_var_features = []
    for col in X.columns:
        if X[col].std() == 0:
            zero_var_features.append(col)
    if zero_var_features:
        logging.warning(f"–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π: {zero_var_features}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    corr_matrix = X.corr()
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i,j]) > 0.95:
                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j]))
    if high_corr_pairs:
        logging.warning(f"–°–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {high_corr_pairs}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    selector = SelectKBest(score_func=f_classif, k='all')
    selector.fit(X, y)
    feature_scores = pd.DataFrame({
        'Feature': X.columns,
        'Score': selector.scores_
    }).sort_values('Score', ascending=False)
    logging.info("–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    logging.info(feature_scores.head(10))


# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
def train_model(model, X_train, y_train, name):
    logging.info(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {name}")
    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        early_stopping_rounds=10,
        verbose=True
    )
    return model

def train_models_for_intervals(data, intervals, selected_features=None):
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤.
    """
    models = {}
    for interval in intervals:
        logging.info(f"–ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–æ {interval} –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞")
        interval_data = data.resample(interval).agg({
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        }).dropna()

        logging.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {interval} –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞")
        prepared_data, features = prepare_data(interval_data)
        selected_features = features if selected_features is None else selected_features

        logging.info(f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {interval} –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞")
        X = prepared_data[selected_features]
        y = prepared_data['target']
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        model = RandomForestClassifier(n_estimators=200, max_depth=6, random_state=42) #n_estimators=200
        model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            early_stopping_rounds=10,
            verbose=True
        )
        models[interval] = (model, selected_features)
    return models


def load_progress(base_learners, meta_model, checkpoint_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫.
    """
    for i, (name, model) in enumerate(base_learners):
        intermediate_path = f"{checkpoint_path}_{name}.pkl"
        if os.path.exists(intermediate_path):
            logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –º–æ–¥–µ–ª–∏ {name} –∏–∑ {intermediate_path}")
            base_learners[i] = (name, joblib.load(intermediate_path))
        else:
            logging.info(f"–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ –¥–ª—è {name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è.")
    
    meta_model_path = f"{checkpoint_path}_meta.pkl"
    if not os.path.exists(os.path.dirname(meta_model_path)):
        os.makedirs(os.path.dirname(meta_model_path))
    if os.path.exists(meta_model_path):
        logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –∏–∑ {meta_model_path}")
        meta_model = joblib.load(meta_model_path)
    else:
        logging.info("–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ –¥–ª—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è.")
    
    return base_learners, meta_model

def train_ensemble_model(data, selected_features, model_filename='flat_stacked_ensemble_model.pkl'):
    """
    –û–±—É—á–∞–µ—Ç –∞–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π (3-–∫–ª–∞—Å—Å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: 0=HOLD, 1=SELL, 2=BUY)
    —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞, —Å —É—á—ë—Ç–æ–º SMOTETomek, –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ —Ç.–¥.
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –≤—ã—Å–æ–∫–æ–ø—Ä–∏–±—ã–ª—å–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏ –Ω–∞ —Ñ–ª—ç—Ç–æ–≤–æ–º —Ä—ã–Ω–∫–µ.
    """
    logging.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π (3-–∫–ª–∞—Å—Å–∞) –¥–ª—è —Ñ–ª—ç—Ç–∞")
    
    # 1) –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    if data.empty:
        raise ValueError("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã")
    if not isinstance(selected_features, list):
        raise TypeError("selected_features –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")
    assert all(feat != 'target' for feat in selected_features), "target –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ —Å–ø–∏—Å–∫–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
    
    logging.info(f"–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ shape: {data.shape}")
    logging.info(f"–í—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {data.columns.tolist()}")
    debug_target_presence(data, "–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è (flat)")
    
    if 'target' not in data.columns:
        raise KeyError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'target' –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    
    # 2) –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 2 –∫–ª–∞—Å—Å–∞
    target_dist = data['target'].value_counts()
    if len(target_dist) < 2:
        raise ValueError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –≤ target: {target_dist}")
    
    # 3) X, y + –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    y = data['target'].copy()
    X = data[selected_features].copy()
    
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –¥–æ –æ–±—É—á–µ–Ω–∏—è:\n{y.value_counts()}")
    logging.info(f"–†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö: X = {X.shape}, y = {y.shape}")
    debug_target_presence(pd.concat([X, y], axis=1), "–ü–µ—Ä–µ–¥ –æ—á–∏—Å—Ç–∫–æ–π –¥–∞–Ω–Ω—ã—Ö (flat)")
    
    # 4) –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X_clean, y_clean = clean_data(X, y)
    logging.info(f"–ü–æ—Å–ª–µ clean_data: X = {X_clean.shape}, y = {y_clean.shape}")
    debug_target_presence(pd.concat([X_clean, y_clean], axis=1), "–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö (flat)")
    
    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª—Å—è –æ–¥–∏–Ω –∫–ª–∞—Å—Å, –æ—à–∏–±–∫–∞
    if len(pd.unique(y_clean)) < 2:
        raise ValueError(f"–ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å: {pd.value_counts(y_clean)}")
    
    # 5) –£–¥–∞–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã
    logging.info("–£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤: –ù–∞—á–∞–ª–æ (flat)")
    combined_data = pd.concat([X_clean, y_clean], axis=1)
    combined_data_cleaned = remove_outliers(combined_data)
    removed_count = combined_data.shape[0] - combined_data_cleaned.shape[0]
    logging.info(f"–£–¥–∞–ª–µ–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤: {removed_count} —Å—Ç—Ä–æ–∫")

    # —Å–±—Ä–æ—Å–∏—Ç—å –∏–Ω–¥–µ–∫—Å—ã –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤
    combined_data_cleaned = combined_data_cleaned.reset_index(drop=True)

    X_clean = combined_data_cleaned.drop(columns=['target'])
    y_clean = combined_data_cleaned['target']
    logging.info(f"–ü–æ—Å–ª–µ remove_outliers: X = {X_clean.shape}, y = {y_clean.shape}")
    assert X_clean.shape[0] == y_clean.shape[0], "–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ X –∏ y –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏!"

    X_clean = X_clean.reset_index(drop=True)
    y_clean = y_clean.reset_index(drop=True)

    
    # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
    X_resampled, y_resampled = balance_classes(X_clean, y_clean)
    
    # 6) Train/Test split
    X_train, X_test, y_train, y_test = train_test_split(
        X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
    )

    logging.info(f"Train size: X = {X_train.shape}, y = {y_train.shape}")
    logging.info(f"Test size: X = {X_test.shape}, y = {y_test.shape}")
    debug_target_presence(pd.concat([X_train, y_train], axis=1), "–ü–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ –≤—ã–±–æ—Ä–∫–∏ (flat)")
    
    # 7) –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled  = scaler.transform(X_test)
    
    # 8) –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º)
    X_augmented = augment_data(pd.DataFrame(X_train_scaled, columns=X_train.columns))
    logging.info(f"–ü–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: X = {X_augmented.shape}")
    debug_target_presence(pd.DataFrame(X_augmented, columns=X_train.columns), "–ü–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (flat)")
    
    # 9) –ü—Ä–∏–º–µ–Ω—è–µ–º SMOTETomek (–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤)
    logging.info("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTETomek (flat)")
    smote_tomek = SMOTETomek(random_state=42)
    X_resampled, y_resampled = smote_tomek.fit_resample(X_augmented, y_train)
    logging.info(f"–ü–æ—Å–ª–µ SMOTETomek: X = {X_resampled.shape}, y = {y_resampled.shape}")
    logging.info(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ SMOTETomek:\n{pd.Series(y_resampled).value_counts()}")
    debug_target_presence(pd.DataFrame(X_resampled, columns=X_train.columns), "–ü–æ—Å–ª–µ SMOTETomek (flat)")
    
    check_class_balance(y_resampled)
    check_feature_quality(pd.DataFrame(X_resampled, columns=X_train.columns), y_resampled)
    
    # 10) –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –±—ã–ª –ª–∏ –∞–Ω—Å–∞–º–±–ª—å —É–∂–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω
    if os.path.exists(ensemble_checkpoint_path):
        logging.info(f"[Ensemble] –ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω—Å–∞–º–±–ª—è –∏–∑ {ensemble_checkpoint_path}")
        saved_data = joblib.load(ensemble_checkpoint_path)
        return saved_data["ensemble_model"], saved_data["scaler"], saved_data["features"]
    
    # 11) –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–¥ 3 –∫–ª–∞—Å—Å–∞ (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è —Ñ–ª—ç—Ç–∞)
    rf_model = CheckpointRandomForest(
        n_estimators=100, #n_estimators=100
        max_depth=4,
        min_samples_leaf=5
    )

    gb_model = CheckpointGradientBoosting(
        n_estimators=100, #n_estimators=100
        max_depth=3,
        learning_rate=0.1,
        subsample=0.8
    )

    xgb_model = CheckpointXGBoost(
        n_estimators=100, #n_estimators=100
        max_depth=3,
        subsample=0.8,
        min_child_weight=5,
        learning_rate=0.01,
        objective='multi:softprob',  # –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π XGBoost
        num_class=3                  # 3 –∫–ª–∞—Å—Å–∞
    )

    lgbm_model = CheckpointLightGBM(
        n_estimators=100, #n_estimators=100
        num_leaves=16,
        learning_rate=0.1,
        min_data_in_leaf=5,
        random_state=42,
        **{"objective": "multiclass", "num_class": 3}
    )

    catboost_model = CheckpointCatBoost(
        iterations=200, #iterations=200
        depth=4,
        learning_rate=0.1,
        min_data_in_leaf=5,
        save_snapshot=False,
        random_state=42,
        loss_function='MultiClass'
    )

    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
    meta_weights = {
        'xgb': 0.3,
        'lgbm': 0.3,
        'catboost': 0.2,
        'gb': 0.1,
        'rf': 0.1
    }

    base_learners = [
        ('rf', rf_model),
        ('gb', gb_model),
        ('xgb', xgb_model),
        ('lgbm', lgbm_model),
        ('catboost', catboost_model)
    ]

    # 12) –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ X_resampled –∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    X_resampled_scaled = scaler.fit_transform(X_resampled)
    X_test_scaled = scaler.transform(X_test)

    # 13) –û–±—É—á–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º early stopping –¥–ª—è —Ç–µ—Ö, –∫–æ—Ç–æ—Ä—ã–µ –µ–≥–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç
    for name, model in base_learners:
        checkpoint_path = get_checkpoint_path(name, market_type)
        final_checkpoint = os.path.join(checkpoint_path, f"{name}_final.joblib")
        logging.info(f"[Ensemble] –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {name}")
        if name in ['xgb', 'catboost']:
            model.fit(X_resampled_scaled, y_resampled, eval_set=[(X_test_scaled, y_test)], early_stopping_rounds=20)
        elif name == 'lgbm':
            model.fit(X_resampled_scaled, y_resampled, eval_set=[(X_test_scaled, y_test)])
        else:
            model.fit(X_resampled_scaled, y_resampled)

        joblib.dump(model, final_checkpoint)
        logging.info(f"[Ensemble] –ú–æ–¥–µ–ª—å {name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {final_checkpoint}")

    # 14) –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ GridSearchCV (–±–ª–æ–∫ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    meta_model_candidate = XGBClassifier(random_state=42)
    param_grid = {
        'n_estimators': [50, 100, 150], #'n_estimators': [50, 100, 150],
        'max_depth': [3, 4, 5],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0]
    }

    # –ü–µ—Ä–µ–¥–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –º–µ—Ç–æ–¥ fit —á–µ—Ä–µ–∑ fit_params:
    grid_search = GridSearchCV(
        estimator=meta_model_candidate,
        param_grid=param_grid,
        cv=3,
        scoring='f1_macro',
        n_jobs=-1
    )

    grid_search.fit(
        X_resampled_scaled,
        y_resampled,
        eval_set=[(X_test_scaled, y_test)],
    )

    meta_model = grid_search.best_estimator_
    logging.info(f"–õ—É—á—à–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏: {grid_search.best_params_}")
    # ------------------------------------------------------------------
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
    for name, _ in base_learners:
        checkpoint_dir = os.path.join(checkpoint_base_dir, name)
        if os.path.exists(checkpoint_dir):
            shutil.rmtree(checkpoint_dir)
            ensure_directory(checkpoint_dir)
    
    # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è
    X_resampled_scaled = RobustScaler().fit_transform(X_resampled)
    
    ensemble_model = StackingClassifier(
        estimators=base_learners,
        final_estimator=meta_model,
        passthrough=True,
        cv=5,
        n_jobs=1
    )
    
    ensemble_model.fit(X_resampled_scaled, y_resampled)
    
    # 15) –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    y_pred = ensemble_model.predict(X_test_scaled)
    f1_macro = f1_score(y_test, y_pred, average='macro')
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    logging.info(f"F1-Score (macro, 3 –∫–ª–∞—Å—Å–∞, flat): {f1_macro:.4f}")
    logging.info(f"Precision (macro, 3 –∫–ª–∞—Å—Å–∞, flat): {precision:.4f}")
    logging.info(f"Recall (macro, 3 –∫–ª–∞—Å—Å–∞, flat): {recall:.4f}")
    logging.info(f"[Ensemble] Weighted F1-score (3-–∫–ª–∞—Å—Å–∞, flat): {f1_macro:.4f}")
    
    # 16) –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥
    save_data = {
        "ensemble_model": ensemble_model,
        "scaler": scaler,
        "features": selected_features
    }
    ensure_directory(os.path.dirname(ensemble_checkpoint_path))
    joblib.dump(save_data, ensemble_checkpoint_path)
    logging.info(f"[Ensemble] –ê–Ω—Å–∞–º–±–ª—å (3-–∫–ª–∞—Å—Å–∞, flat) —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {ensemble_checkpoint_path}")
    
    output_dir = os.path.join("/workspace/output", "flat_ensemble")
    copy_output("Ensemble_Flat", output_dir)
    
    return {"ensemble_model": ensemble_model, "scaler": scaler, "features": selected_features}


# –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ –¥–ª—è —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è
if __name__ == "__main__":
    
    strategy = initialize_strategy()
    
    symbols = ['BTCUSDC', 'ETHUSDC', 'BNBUSDC','XRPUSDC', 'ADAUSDC', 'SOLUSDC', 'DOTUSDC', 'LINKUSDC', 'TONUSDC', 'NEARUSDC']

    # –ü–µ—Ä–∏–æ–¥—ã —Ñ–ª—ç—Ç–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞
    flat_periods = [
        {"start": "2019-02-01", "end": "2019-04-30"},
        {"start": "2019-06-01", "end": "2019-08-31"},
        {"start": "2020-01-01", "end": "2020-02-29"},
        {"start": "2020-07-01", "end": "2020-08-31"},
        {"start": "2020-09-01", "end": "2020-10-31"},
        {"start": "2021-09-01", "end": "2021-10-31"},
        {"start": "2023-04-01", "end": "2023-05-31"}
    ]
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–ª—ç—Ç–∞ (—Ñ—É–Ω–∫—Ü–∏—è load_flat_data –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å DataFrame, –∫–∞–∫ –≤ –±—ã—á—å–µ–º –≤–∞—Ä–∏–∞–Ω—Ç–µ)
    data_dict = load_flat_data(symbols, flat_periods, interval="1m", save_path="/workspace/data/binance_data_flat.csv")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ª–æ–≤–∞—Ä—å –Ω–µ –ø—É—Å—Ç–æ–π
    if not data_dict:
        raise ValueError("–û—à–∏–±–∫–∞: load_flat_data() –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å!")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ DataFrame –≤ –æ–¥–∏–Ω
    data = pd.concat(data_dict.values(), ignore_index=False)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ 'timestamp'
    if 'timestamp' not in data.columns:
        logging.warning("'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–¥–µ–∫—Å.")
        if isinstance(data.index, pd.DatetimeIndex):
            data['timestamp'] = data.index
            logging.info("–ò–Ω–¥–µ–∫—Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –∫–æ–ª–æ–Ω–∫—É 'timestamp'.")
        else:
            raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'timestamp' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏ –∏–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex.")
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    data, selected_features = prepare_data(data)
    logging.debug(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –ø–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {data.columns.tolist()}")
    logging.debug(f"–í—ã–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {selected_features}")
    
    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        logging.info("–ù–∞—á–∞–ª–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö (flat)...")
        prepared_data, selected_features = prepare_data(data)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        if prepared_data.empty:
            raise ValueError("–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã")
        if 'target' not in prepared_data.columns:
            raise KeyError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'target' –≤ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        if not selected_features:
            raise ValueError("–°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—É—Å—Ç")
        
        logging.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {prepared_data.shape}")
        logging.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(selected_features)}")
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        sys.exit(1)
    
    try:
        # –û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ñ–ª—ç—Ç–∞
        logging.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π (flat)...")
        ensemble_model, scaler, features = train_ensemble_model(prepared_data, selected_features, model_filename='flat_stacked_ensemble_model.pkl')
        logging.info("–û–±—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π: {e}")
        sys.exit(1)
    
    try:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        if not os.path.exists('models'):
            os.makedirs('models')
            
        model_path = os.path.join('models', 'flat_stacked_ensemble_model.pkl')
        joblib.dump((ensemble_model, features), model_path)
        logging.info(f"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        sys.exit(1)
    
    logging.info("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    sys.exit(0)
